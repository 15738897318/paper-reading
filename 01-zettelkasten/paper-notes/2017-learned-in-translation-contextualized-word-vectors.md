---
title: Learned in Translation - Contextualized Word Vectors
authors:
- Bryan McCann
- James Bradbury
- Caiming Xiong
- R. Socher
fieldsOfStudy:
- Computer Science
meta_key: 2017-learned-in-translation-contextualized-word-vectors
numCitedBy: 712
reading_status: TBD
ref_count: 74
tags:
- gen-from-ref
- other-default
- paper
venue: NIPS
year: 2017
---

# Learned in Translation - Contextualized Word Vectors

## Abstract

Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.

## Paper References

1. Towards Universal Paraphrastic Sentence Embeddings
2. [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](2017-supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data)
3. [Skip-Thought Vectors](2015-skip-thought-vectors)
4. Neural Semantic Encoders
5. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
6. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
7. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
8. [Learning Word Vectors for Sentiment Analysis](2011-learning-word-vectors-for-sentiment-analysis)
9. Encoding Syntactic Knowledge in Neural Networks for Sentiment Classification
10. Unsupervised Pretraining for Sequence to Sequence Learning
11. The representational geometry of word meanings acquired by neural machine translation models
12. Dependency Sensitive Convolutional Neural Networks for Modeling Sentences and Documents
13. [Effective Approaches to Attention-based Neural Machine Translation](2015-effective-approaches-to-attention-based-neural-machine-translation)
14. Language to Logical Form with Neural Attention
15. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
16. Learning to Generate Reviews and Discovering Sentiment
17. Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings
18. Reading and Thinking - Re-read LSTM Unit for Textual Entailment Recognition
19. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference)
20. Semi-supervised Sequence Learning
21. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
22. Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling
23. TopicRNN - A Recurrent Neural Network with Long-Range Semantic Dependency
24. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
25. [Framewise phoneme classification with bidirectional LSTM and other neural network architectures](2005-framewise-phoneme-classification-with-bidirectional-lstm-and-other-neural-network-architectures)
26. [Learning Distributed Representations of Sentences from Unlabelled Data](2016-learning-distributed-representations-of-sentences-from-unlabelled-data)
27. Question Answering through Transfer Learning from Large Fine-grained Supervision Data
28. A Decomposable Attention Model for Natural Language Inference
29. Grounded Compositional Semantics for Finding and Describing Images with Sentences
30. Machine Comprehension Using Match-LSTM and Answer Pointer
31. SemEval-2014 Task 10 - Multilingual Semantic Textual Similarity
32. A Joint Many-Task Model - Growing a Neural Network for Multiple NLP Tasks
33. The IWSLT 2015 Evaluation Campaign
34. Discriminative Neural Sentence Modeling by Tree-Based Convolution
35. Adversarial Training Methods for Semi-Supervised Text Classification
36. Neural Tree Indexers for Text Understanding
37. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
38. Enhancing and Combining Sequential and Tree LSTM for Natural Language Inference
39. [Bidirectional Attention Flow for Machine Comprehension](2017-bidirectional-attention-flow-for-machine-comprehension)
40. [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](2016-multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding)
41. [Knowing When to Look - Adaptive Attention via a Visual Sentinel for Image Captioning](2017-knowing-when-to-look-adaptive-attention-via-a-visual-sentinel-for-image-captioning)
42. End-to-End Multi-View Networks for Text Classification
43. [Dynamic Memory Networks for Visual and Textual Question Answering](2016-dynamic-memory-networks-for-visual-and-textual-question-answering)
44. Recursive Neural Networks for Learning Logical Semantics
45. From symbolic to sub-symbolic information in question classification
46. [Moses - Open Source Toolkit for Statistical Machine Translation](2007-moses-open-source-toolkit-for-statistical-machine-translation)
47. Learning question classifiers - the role of semantic information
48. A Shared Task on Multimodal Machine Translation and Crosslingual Image Description
49. Gated Self-Matching Networks for Reading Comprehension and Question Answering
50. Question Classification by Weighted Combination of Lexical, Syntactic and Semantic Features
51. OpenNMT - Open-Source Toolkit for Neural Machine Translation
52. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift)
53. [Deep Residual Learning for Image Recognition](2015-resnet.md)
54. High Accuracy Rule-based Question Classification using Question Syntax and Semantics
55. Heterogeneous Transfer Learning for Image Classification
56. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2014-vggnet.md)
57. [ImageNet classification with deep convolutional neural networks](2012-alexnet.md)
58. [Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation](2014-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation)
59. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database)
60. [Maxout Networks](2013-maxout-networks)
61. Deep Learning with Dynamic Computation Graphs
62. The TREC-8 Question Answering Track Evaluation
63. Adapting Visual Category Models to New Domains
64. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines)
65. Hedged Deep Tracking
66. Dynamic Coattention Networks For Question Answering
67. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond
68. Improving Question Classification by Feature Extraction and Selection
69. Open Source Toolkit for Statistical Machine Translation - Factored Translation Models and Lattice Decoding
70. End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking
71. A Neural Architecture Mimicking Humans End-to-End for Natural Language Inference
72. [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](2016-ask-me-anything-dynamic-memory-networks-for-natural-language-processing)
