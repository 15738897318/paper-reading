---
title: Fusion of Detected Objects in Text for Visual Question Answering
authors:
- Chris Alberti
- Jeffrey Ling
- Michael Collins
- D. Reitter
fieldsOfStudy:
- Computer Science
meta_key: 2019-fusion-of-detected-objects-in-text-for-visual-question-answering
numCitedBy: 114
reading_status: TBD
ref_count: 32
tags:
- gen-from-ref
- paper
venue: EMNLP
year: 2019
---

# Fusion of Detected Objects in Text for Visual Question Answering

## Abstract

To advance models of multimodal context, we introduce a simple yet powerful neural architecture for data that combines vision and natural language. The “Bounding Boxes in Text Transformer” (B2T2) also leverages referential information binding words to portions of the image in a single unified architecture. B2T2 is highly effective on the Visual Commonsense Reasoning benchmark, achieving a new state-of-the-art with a 25% relative reduction in error rate compared to published baselines and obtaining the best performance to date on the public leaderboard (as of May 22, 2019). A detailed ablation analysis shows that the early integration of the visual features into the text analysis is key to the effectiveness of the new architecture. A reference implementation of our models is provided.

## Paper References

1. [Yin and Yang - Balancing and Answering Binary Visual Questions](2016-yin-and-yang-balancing-and-answering-binary-visual-questions)
2. [VisualBERT - A Simple and Performant Baseline for Vision and Language](2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language)
3. [CLEVR - A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning](2017-clevr-a-diagnostic-dataset-for-compositional-language-and-elementary-visual-reasoning)
4. [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks)
5. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
6. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
7. [VL-BERT - Pre-training of Generic Visual-Linguistic Representations](2020-vl-bert-pre-training-of-generic-visual-linguistic-representations)
8. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
9. [GQA - A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](2019-gqa-a-new-dataset-for-real-world-visual-reasoning-and-compositional-question-answering)
10. [From Recognition to Cognition - Visual Commonsense Reasoning](2019-from-recognition-to-cognition-visual-commonsense-reasoning)
11. GQA - a new dataset for compositional question answering over real-world images
12. [VideoBERT - A Joint Model for Video and Language Representation Learning](2019-videobert-a-joint-model-for-video-and-language-representation-learning)
13. [Compositional Attention Networks for Machine Reasoning](2018-compositional-attention-networks-for-machine-reasoning)
14. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
15. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding)
16. Like a Baby - Visually Situated Neural Language Acquisition
17. The Neuro-Symbolic Concept Learner - Interpreting Scenes Words and Sentences from Natural Supervision
18. [Attention is All you Need](2017-attention-is-all-you-need)
19. [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning)
20. Multimodal Distributional Semantics
21. [Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training](2020-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training)
22. End-to-End Retrieval in Continuous Space
23. Learning Visually Grounded Sentence Representations
24. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
25. StarSpace - Embed All The Things!
26. WSABIE - Scaling Up to Large Vocabulary Image Annotation
27. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks)
28. Distributional Structure
29. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
30. Indexing by Latent Semantic Analysis
31. Information science as Little Science - The implications of a bibliometric analysis of theJournal of the American Society for Information Science
32. A Synopsis of Linguistic Theory, 1930-1955
