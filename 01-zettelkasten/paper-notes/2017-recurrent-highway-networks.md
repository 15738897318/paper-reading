---
title: Recurrent Highway Networks
authors:
- Julian G. Zilly
- R. Srivastava
- J. Koutn√≠k
- J. Schmidhuber
fieldsOfStudy:
- Computer Science
meta_key: 2017-recurrent-highway-networks
numCitedBy: 351
reading_status: TBD
ref_count: 74
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Recurrent-Highway-Networks-Zilly-Srivastava/7dba53e72c182e25e98e8f73a99d75ff69dda0c2?sort=total-citations
venue: ICML
year: 2017
---

[semanticscholar url](https://www.semanticscholar.org/paper/Recurrent-Highway-Networks-Zilly-Srivastava/7dba53e72c182e25e98e8f73a99d75ff69dda0c2?sort=total-citations)

# Recurrent Highway Networks

## Abstract

Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character.

## Paper References

1. Multiplicative LSTM for sequence modelling
2. Improved Learning through Augmenting the Loss
3. Highway long short-term memory RNNS for distant speech recognition
4. Context dependent recurrent neural network language model
5. [Exploring the Limits of Language Modeling](2016-exploring-the-limits-of-language-modeling.md)
6. [An Empirical Exploration of Recurrent Network Architectures](2015-an-empirical-exploration-of-recurrent-network-architectures.md)
7. [Grid Long Short-Term Memory](2016-grid-long-short-term-memory.md)
8. Adaptive Computation Time for Recurrent Neural Networks
9. [LSTM - A Search Space Odyssey](2017-lstm-a-search-space-odyssey.md)
10. Gated Feedback Recurrent Neural Networks
11. How to Construct Deep Recurrent Neural Networks
12. Learning to Forget - Continual Prediction with LSTM
13. [Character-Aware Neural Language Models](2016-character-aware-neural-language-models.md)
14. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
15. A Simple Way to Initialize Recurrent Networks of Rectified Linear Units
16. [Long Short-Term Memory](1997-long-short-term-memory.md)
17. Learning Complex, Extended Sequences Using the Principle of History Compression
18. [Pointer Sentinel Mixture Models](2017-pointer-sentinel-mixture-models.md)
19. [Tying Word Vectors and Word Classifiers - A Loss Framework for Language Modeling](2017-tying-word-vectors-and-word-classifiers-a-loss-framework-for-language-modeling.md)
20. [On Multiplicative Integration with Recurrent Neural Networks](2016-on-multiplicative-integration-with-recurrent-neural-networks.md)
21. [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](2016-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.md)
22. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model.md)
23. Highway and Residual Networks learn Unrolled Iterative Estimation
24. [Hierarchical Multiscale Recurrent Neural Networks](2017-hierarchical-multiscale-recurrent-neural-networks.md)
25. [Recurrent Batch Normalization](2017-recurrent-batch-normalization.md)
26. Sequence Labelling in Structured Domains with Hierarchical Recurrent Neural Networks
27. Learning long-term dependencies with gradient descent is difficult
28. On the Complexity of Neural Network Classifiers - A Comparison Between Shallow and Deep Architectures
29. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
30. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization.md)
31. [Training Very Deep Networks](2015-training-very-deep-networks.md)
32. Self-Delimiting Neural Networks
33. [Using the Output Embedding to Improve Language Models](2017-using-the-output-embedding-to-improve-language-models.md)
34. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks.md)
35. [HyperNetworks](2017-hypernetworks.md)
36. [Deep learning in neural networks - An overview](2015-deep-learning-in-neural-networks-an-overview.md)
37. Scaling learning algorithms towards AI
38. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
39. [Practical Variational Inference for Neural Networks](2011-practical-variational-inference-for-neural-networks.md)
40. [On the difficulty of training recurrent neural networks](2013-on-the-difficulty-of-training-recurrent-neural-networks.md)
41. Generalization of backpropagation with application to a recurrent gas market model
42. Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
43. Modeling Temporal Dependencies in High-Dimensional Sequences - Application to Polyphonic Music Generation and Transcription
44. First Experiments with PowerPlay
45. Building a Large Annotated Corpus of English - The Penn Treebank
46. [Torch7 - A Matlab-like Environment for Machine Learning](2011-torch7-a-matlab-like-environment-for-machine-learning.md)
47. Reinforcement Learning in Markovian and Non-Markovian Environments
48. Applications of advances in nonlinear sensitivity analysis
49. Taylor expansion of the accumulated rounding error
50. System modeling and optimization - proceedings of the 10th IFIP Conference, New York City, USA, August 31-September 4, 1981
51. System Modeling and Optimization
52. Untersuchungen zu dynamischen neuronalen Netzen
