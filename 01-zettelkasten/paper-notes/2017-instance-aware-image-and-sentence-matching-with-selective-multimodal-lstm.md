---
title: Instance-Aware Image and Sentence Matching with Selective Multimodal LSTM
authors:
- Yan Huang
- Wei Wang
- Liang Wang
fieldsOfStudy:
- Computer Science
meta_key: 2017-instance-aware-image-and-sentence-matching-with-selective-multimodal-lstm
numCitedBy: 160
reading_status: TBD
ref_count: 41
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Instance-Aware-Image-and-Sentence-Matching-with-Huang-Wang/e1b6735f6ecb09e1d83b0aa9d2cde42993ee2eb0?sort=total-citations
venue: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2017
---

[semanticscholar url](https://www.semanticscholar.org/paper/Instance-Aware-Image-and-Sentence-Matching-with-Huang-Wang/e1b6735f6ecb09e1d83b0aa9d2cde42993ee2eb0?sort=total-citations)

# Instance-Aware Image and Sentence Matching with Selective Multimodal LSTM

## Abstract

Effective image and sentence matching depends on how to well measure their global visual-semantic similarity. Based on the observation that such a global similarity arises from a complex aggregation of multiple local similarities between pairwise instances of image (objects) and sentence (words), we propose a selective multimodal Long Short-Term Memory network (sm-LSTM) for instance-aware image and sentence matching. The sm-LSTM includes a multimodal context-modulated attention scheme at each timestep that can selectively attend to a pair of instances of image and sentence, by predicting pairwise instance-aware saliency maps for image and sentence. For selected pairwise instances, their representations are obtained based on the predicted saliency maps, and then compared to measure their local similarity. By similarly measuring multiple local similarities within a few timesteps, the sm-LSTM sequentially aggregates them with hidden states to obtain a final matching score as the desired global similarity. Extensive experiments show that our model can well match image and sentence with complex content, and achieve the state-of-the-art results on two public benchmark datasets.

## Paper References

1. Deep Fragment Embeddings for Bidirectional Image Sentence Mapping
2. Multimodal Convolutional Neural Networks for Matching Image and Sentence
3. [Deep Visual-Semantic Alignments for Generating Image Descriptions](2017-deep-visual-semantic-alignments-for-generating-image-descriptions)
4. Flickr30k Entities - Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models
5. [Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](2014-unifying-visual-semantic-embeddings-with-multimodal-neural-language-models)
6. Explain Images with Multimodal Recurrent Neural Networks
7. [From captions to visual concepts and back](2015-from-captions-to-visual-concepts-and-back)
8. Grounded Compositional Semantics for Finding and Describing Images with Sentences
9. Associating neural word embeddings with deep image representations using Fisher Vectors
10. Mind's eye - A recurrent visual representation for image caption generation
11. [DeViSE - A Deep Visual-Semantic Embedding Model](2013-devise-a-deep-visual-semantic-embedding-model)
12. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator)
13. Contextual LSTM (CLSTM) models for Large scale NLP tasks
14. [Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation](2014-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation)
15. [Long-term recurrent convolutional networks for visual recognition and description](2015-long-term-recurrent-convolutional-networks-for-visual-recognition-and-description)
16. [Learning Deep Structure-Preserving Image-Text Embeddings](2016-learning-deep-structure-preserving-image-text-embeddings)
17. [Skip-Thought Vectors](2015-skip-thought-vectors)
18. An effective regional saliency model based on extended site entropy rate
19. Unconstrained Multimodal Multi-Label Learning
20. Measuring visual saliency by Site Entropy Rate
21. [Order-Embeddings of Images and Language](2016-order-embeddings-of-images-and-language)
22. Multiple Object Recognition with Visual Attention
23. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
24. Fisher Kernels on Visual Vocabularies for Image Categorization
25. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
26. Deep correlation for matching images and text
27. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2014-vggnet.md)
28. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
29. Simulating human saccadic scanpaths on natural images
30. [ImageNet classification with deep convolutional neural networks](2012-alexnet.md)
31. RNN Fisher Vectors for Action Recognition and Image Annotation
32. [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](2014-from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions)
33. [Long Short-Term Memory](1997-long-short-term-memory)
34. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
35. Top-Down Attentional Guidance Based on Implicit Learning of Visual Covariation
36. Scene and screen center bias early eye movements in scene viewing
37. Bidirectional recurrent neural networks
38. Quantifying center bias of observers in free viewing of dynamic natural scenes.
39. Generating Typed Dependency Parses from Phrase Structure Parses
40. Bidirectional Recurrent Convolutional Networks for Multi-Frame Super-Resolution
41. The role of context in object recognition
