---
title: Deep Sparse Rectifier Neural Networks
authors:
- Xavier Glorot
- Antoine Bordes
- Yoshua Bengio
fieldsOfStudy:
- Computer Science
meta_key: 2011-deep-sparse-rectifier-neural-networks
numCitedBy: 5954
reading_status: TBD
ref_count: 40
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Deep-Sparse-Rectifier-Neural-Networks-Glorot-Bordes/67107f78a84bdb2411053cb54e94fa226eea6d8e?sort=total-citations
venue: AISTATS
year: 2011
---

[semanticscholar url](https://www.semanticscholar.org/paper/Deep-Sparse-Rectifier-Neural-Networks-Glorot-Bordes/67107f78a84bdb2411053cb54e94fa226eea6d8e?sort=total-citations)

# Deep Sparse Rectifier Neural Networks

## Abstract

While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-dierentiabil ity

## Paper References

1. [Greedy Layer-Wise Training of Deep Networks](2006-greedy-layer-wise-training-of-deep-networks)
2. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks)
3. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines)
4. On the piecewise analysis of networks of linear threshold neurons
5. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
6. Sparse Feature Learning for Deep Belief Networks
7. [Learning Deep Architectures for AI](2007-learning-deep-architectures-for-ai)
8. Efficient Learning of Sparse Representations with an Energy-Based Model
9. Recurrent excitation in neocortical circuits
10. A model of multiplicative neural responses in parietal cortex.
11. Sparse deep belief net model for visual area V2
12. The Cost of Cortical Computation
13. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images)
14. Sparse coding with an overcomplete basis set - A strategy employed by V1?
15. Efficient sparse coding algorithms
16. Measuring Invariances in Deep Networks
17. [Extracting and composing robust features with denoising autoencoders](2008-extracting-and-composing-robust-features-with-denoising-autoencoders)
18. Why Does Unsupervised Pre-training Help Deep Learning?
19. Incorporating Second-Order Functional Knowledge for Better Option Pricing
20. [Gradient-based learning applied to document recognition](1998-lenet5.md)
21. Active Deep Networks for Semi-Supervised Sentiment Classification
22. Deep Self-Taught Learning for Handwritten Character Recognition
23. The Cortical Neuron
24. What is the best multi-stage architecture for object recognition?
25. A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels
26. Learning methods for generic object recognition with invariance to pose and lighting
27. Biographies, Bollywood, Boom-boxes and Blenders - Domain Adaptation for Sentiment Classification
28. Decoding by linear programming
29. Multiple Aspect Ranking Using the Good Grief Algorithm
30. The Cortical Neuron
31. An Energy Budget for Signaling in the Grey Matter of the Brain
32. [Efficient BackProp](2012-efficient-backprop)
33. A quantitative theory of immediate visual recognition.
34. Supervised Dictionary Learning
