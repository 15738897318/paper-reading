---
title: GloVe - Global Vectors for Word Representation
authors:
- Jeffrey Pennington
- R. Socher
- Christopher D. Manning
fieldsOfStudy:
- Computer Science
meta_key: 2014-glove-global-vectors-for-word-representation
numCitedBy: 22729
reading_status: TBD
ref_count: 40
tags:
- gen-from-ref
- other-default
- paper
venue: EMNLP
year: 2014
---

# GloVe - Global Vectors for Word Representation

## Abstract

Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.

## Paper References

1. [Linguistic Regularities in Continuous Space Word Representations](2013-linguistic-regularities-in-continuous-space-word-representations)
2. Linguistic Regularities in Sparse and Explicit Word Representations
3. Better Word Representations with Recursive Neural Networks for Morphology
4. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
5. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
6. Learning word embeddings efficiently with noise-contrastive estimation
7. Word Representations - A Simple and General Method for Semi-Supervised Learning
8. A Neural Probabilistic Language Model
9. Word Embeddings through Hellinger PCA
10. Producing high-dimensional semantic spaces from lexical co-occurrence
11. An Improved Model of Semantic Similarity Based on Lexical Co-Occurrence
12. Extracting semantic representations from word co-occurrence statistics - A computational study
13. Parsing with Compositional Vector Grammars
14. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
15. Don't count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors
16. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
17. Effect of Non-linear Deep Architecture in Sequence Labeling
18. Indexing by Latent Semantic Analysis
19. [Learning Deep Architectures for AI](2007-learning-deep-architectures-for-ai)
20. Contextual correlates of semantic similarity
21. Placing search in context - the concept revisited
22. Introduction to the CoNLL-2003 Shared Task - Language-Independent Named Entity Recognition
23. Quantitative evaluation of passage retrieval algorithms for question answering
24. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization)
25. [Deep Neural Networks Segment Neuronal Membranes in Electron Microscopy Images](2012-deep-neural-networks-segment-neuronal-membranes-in-electron-microscopy-images)
26. Introduction to analytic number theory
27. Contextual correlates of synonymy
28. [Improving Word Representations via Global Context and Multiple Word Prototypes](2012-improving-word-representations-via-global-context-and-multiple-word-prototypes)
29. Machine learning in automated text categorization
