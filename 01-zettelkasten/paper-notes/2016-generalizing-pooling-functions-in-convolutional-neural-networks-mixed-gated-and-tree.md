---
title: Generalizing Pooling Functions in Convolutional Neural Networks - Mixed, Gated, and Tree
authors:
- Chen-Yu Lee
- Patrick W. Gallagher
- Z. Tu
fieldsOfStudy:
- Computer Science
meta_key: 2016-generalizing-pooling-functions-in-convolutional-neural-networks-mixed-gated-and-tree
numCitedBy: 391
reading_status: TBD
ref_count: 50
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Generalizing-Pooling-Functions-in-Convolutional-and-Lee-Gallagher/4b88e948121a87f00fb5aa0081d2044dde51ee36?sort=total-citations
venue: AISTATS
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/Generalizing-Pooling-Functions-in-Convolutional-and-Lee-Gallagher/4b88e948121a87f00fb5aa0081d2044dde51ee36?sort=total-citations)

# Generalizing Pooling Functions in Convolutional Neural Networks - Mixed, Gated, and Tree

## Abstract

We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.

## Paper References

1. [Network In Network](2014-network-in-network.md)
2. Fractional Max-Pooling
3. Stochastic Pooling for Regularization of Deep Convolutional Neural Networks
4. Improving Deep Neural Networks with Probabilistic Maxout Units
5. Selecting Receptive Fields in Deep Networks
6. Deep Networks with Internal Selective Attention through Feedback Connections
7. [Deeply-Supervised Nets](2015-deeply-supervised-nets.md)
8. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
9. Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition
10. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
11. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
12. [Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition](2015-spatial-pyramid-pooling-in-deep-convolutional-networks-for-visual-recognition.md)
13. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
14. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
15. [FitNets - Hints for Thin Deep Nets](2015-fitnets-hints-for-thin-deep-nets.md)
16. Discriminative Transfer Learning with Tree-based Priors
17. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
18. Neural Decision Forests for Semantic Image Labelling
19. Ask the locals - Multi-way local pooling for image recognition
20. A Theoretical Analysis of Feature Pooling in Visual Recognition
21. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets.md)
22. Autoencoder Trees
23. [Maxout Networks](2013-maxout-networks.md)
24. Learning Activation Functions to Improve Deep Neural Networks
25. Sparse Feature Learning for Deep Belief Networks
26. What is the best multi-stage architecture for object recognition?
27. Robust Object Recognition with Cortex-Like Mechanisms
28. [GradientBased Learning Applied to Document Recognition](2001-gradientbased-learning-applied-to-document-recognition.md)
29. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition.md)
30. [Reading Digits in Natural Images with Unsupervised Feature Learning](2011-reading-digits-in-natural-images-with-unsupervised-feature-learning.md)
31. Receptive fields, binocular interaction and functional architecture in the cat's visual cortex
32. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge.md)
33. Backpropagation Applied to Handwritten Zip Code Recognition
34. [Visualizing Data using t-SNE](2008-visualizing-data-using-t-sne.md)
35. [The mnist database of handwritten digits](2005-the-mnist-database-of-handwritten-digits.md)
36. Logic-Based Artificial Intelligence
37. Receptive fields
