---
title: ALBERT - A Lite BERT for Self-supervised Learning of Language Representations
authors:
- Zhenzhong Lan
- Mingda Chen
- Sebastian Goodman
- Kevin Gimpel
- Piyush Sharma
- Radu Soricut
fieldsOfStudy:
- Computer Science
meta_key: 2020-albert-a-lite-bert-for-self-supervised-learning-of-language-representations
numCitedBy: 2773
reading_status: TBD
ref_count: 83
tags:
- gen-from-ref
- other-default
- paper
venue: ICLR
year: 2020
---

# ALBERT - A Lite BERT for Self-supervised Learning of Language Representations

## Abstract

Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL.

## Paper References

1. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
2. [RoBERTa - A Robustly Optimized BERT Pretraining Approach](2019-roberta-a-robustly-optimized-bert-pretraining-approach)
3. Well-Read Students Learn Better - On the Importance of Pre-training Compact Models
4. Efficient Training of BERT by Progressively Stacking
5. StructBERT - Incorporating Language Structures into Pre-training for Deep Language Understanding
6. Well-Read Students Learn Better - The Impact of Student Initialization on Knowledge Distillation
7. [XLNet - Generalized Autoregressive Pretraining for Language Understanding](2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding)
8. [Language Models are Unsupervised Multitask Learners](2019-language-models-are-unsupervised-multitask-learners)
9. BAM! Born-Again Multi-Task Networks for Natural Language Understanding
10. Adaptive Input Representations for Neural Language Modeling
11. Patient Knowledge Distillation for BERT Model Compression
12. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](2020-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer)
13. [Attention is All you Need](2017-attention-is-all-you-need)
14. [Transformer-XL - Attentive Language Models beyond a Fixed-Length Context](2019-transformer-xl-attentive-language-models-beyond-a-fixed-length-context)
15. Megatron-LM - Training Multi-Billion Parameter Language Models Using Model Parallelism
16. [SpanBERT - Improving Pre-training by Representing and Predicting Spans](2020-spanbert-improving-pre-training-by-representing-and-predicting-spans)
17. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
18. Generating Long Sequences with Sparse Transformers
19. Reducing BERT Pre-Training Time from 3 Days to 76 Minutes
20. A Simple Method for Commonsense Reasoning
21. [Skip-Thought Vectors](2015-skip-thought-vectors)
22. DisSent - Learning Sentence Representations from Explicit Discourse Relations
23. Semi-supervised Sequence Learning
24. [Universal Language Model Fine-tuning for Text Classification](2018-universal-language-model-fine-tuning-for-text-classification)
25. Deep Equilibrium Models
26. Training Deep Nets with Sublinear Memory Cost
27. Efficient softmax approximation for GPUs
28. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
29. Understanding the Disharmony Between Dropout and Batch Normalization by Variance Shift
30. [Learning Distributed Representations of Sentences from Unlabelled Data](2016-learning-distributed-representations-of-sentences-from-unlabelled-data)
31. Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling
32. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
33. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
34. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](2017-inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning)
35. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
36. [Learned in Translation - Contextualized Word Vectors](2017-learned-in-translation-contextualized-word-vectors)
37. Learning Generic Sentence Representations Using Convolutional Neural Networks
38. [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](2018-a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference)
39. [Distributed Representations of Sentences and Documents](2014-distributed-representations-of-sentences-and-documents)
40. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
41. [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](2017-semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation)
42. The Reversible Residual Network - Backpropagation Without Storing Activations
43. [RACE - Large-scale ReAding Comprehension Dataset From Examinations](2017-race-large-scale-reading-comprehension-dataset-from-examinations)
44. SentencePiece - A simple and language independent subword tokenizer and detokenizer for Neural Text Processing
45. [Know What You Don't Know - Unanswerable Questions for SQuAD](2018-know-what-you-don-t-know-unanswerable-questions-for-squad)
46. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
47. [Neural Network Acceptability Judgments](2019-neural-network-acceptability-judgments)
48. The Sixth PASCAL Recognizing Textual Entailment Challenge
49. Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning
50. Dual Co-Matching Network for Multi-choice Reading Comprehension
51. The Seventh PASCAL Recognizing Textual Entailment Challenge
52. The Second PASCAL Recognising Textual Entailment Challenge
53. Automatically Constructing a Corpus of Sentential Paraphrases
54. The PASCAL Recognising Textual Entailment Challenge
55. The Winograd Schema Challenge
56. Gaussian Error Linear Units (GELUs)
57. Centering - A Framework for Modeling the Local Coherence of Discourse
58. Coherence and Coreference
59. Association for Computational Linguistics
60. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
61. Modeling Recurrence for Transformer
62. Mesh-TensorFlow - Deep Learning for Supercomputers
63. The Third PASCAL Recognizing Textual Entailment Challenge
