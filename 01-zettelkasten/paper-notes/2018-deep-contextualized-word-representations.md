---
title: Deep Contextualized Word Representations
authors:
- Matthew E. Peters
- Mark Neumann
- Mohit Iyyer
- Matt Gardner
- Christopher Clark
- Kenton Lee
- Luke Zettlemoyer
fieldsOfStudy:
- Computer Science
meta_key: 2018-deep-contextualized-word-representations
numCitedBy: 8062
reading_status: TBD
ref_count: 65
tags:
- gen-from-ref
- other-default
- paper
venue: NAACL
year: 2018
---

# Deep Contextualized Word Representations

## Abstract

We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.

## Paper References

1. [Learned in Translation - Contextualized Word Vectors](2017-learned-in-translation-contextualized-word-vectors)
2. Semi-supervised sequence tagging with bidirectional language models
3. context2vec - Learning Generic Context Embedding with Bidirectional LSTM
4. Word Representations - A Simple and General Method for Semi-Supervised Learning
5. Embeddings for Word Sense Disambiguation - An Evaluation Study
6. Finding Function in Form - Compositional Character Models for Open Vocabulary Word Representation
7. [Character-Aware Neural Language Models](2016-character-aware-neural-language-models)
8. Neural Sequence Learning Models for Word Sense Disambiguation
9. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
10. [Enriching Word Vectors with Subword Information](2017-enriching-word-vectors-with-subword-information)
11. Regularizing and Optimizing LSTM Language Models
12. Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space
13. Charagram - Embedding Words and Sentences via Character n-grams
14. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
15. [Exploring the Limits of Language Modeling](2016-exploring-the-limits-of-language-modeling)
16. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference)
17. What do Neural Machine Translation Models Learn about Morphology?
18. End-to-end Neural Coreference Resolution
19. End-to-end learning of semantic role labeling using recurrent neural networks
20. Word Sense Disambiguation - A Unified Evaluation Framework and Empirical Comparison
21. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
22. Enhanced LSTM for Natural Language Inference
23. Towards Robust Linguistic Analysis using OntoNotes
24. On the State of the Art of Evaluation in Neural Language Models
25. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
26. A Joint Many-Task Model - Growing a Neural Network for Multiple NLP Tasks
27. One billion word benchmark for measuring progress in statistical language modeling
28. [End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF](2016-end-to-end-sequence-labeling-via-bi-directional-lstm-cnns-crf)
29. Neural Tree Indexers for Text Understanding
30. Learning Global Features for Coreference Resolution
31. [Bidirectional Attention Flow for Machine Comprehension](2017-bidirectional-attention-flow-for-machine-comprehension)
32. CoNLL-2012 Shared Task - Modeling Multilingual Unrestricted Coreference in OntoNotes
33. [Named Entity Recognition with Bidirectional LSTM-CNNs](2016-named-entity-recognition-with-bidirectional-lstm-cnns)
34. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
35. Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling
36. The Proposition Bank - An Annotated Corpus of Semantic Roles
37. Simple and Effective Multi-Paragraph Reading Comprehension
38. An Empirical Exploration of Recurrent Network Architectures
39. Building a Large Annotated Corpus of English - The Penn Treebank
40. Deep multi-task learning with low level tasks supervised at lower layers
41. A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
42. [On the Properties of Neural Machine Translation - Encoder-Decoder Approaches](2014-on-the-properties-of-neural-machine-translation-encoder-decoder-approaches)
43. Deep Reinforcement Learning for Mention-Ranking Coreference Models
44. Gated Self-Matching Networks for Reading Comprehension and Question Answering
45. [Long Short-Term Memory](1997-long-short-term-memory)
46. Stochastic Answer Networks for Machine Reading Comprehension
47. Using a Semantic Concordance for Sense Identification
48. [Conditional Random Fields - Probabilistic Models for Segmenting and Labeling Sequence Data](2001-conditional-random-fields-probabilistic-models-for-segmenting-and-labeling-sequence-data)
49. [Training Very Deep Networks](2015-training-very-deep-networks)
50. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting)
51. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
52. [Layer Normalization](2016-layer-normalization)
53. Introduction to the CoNLL-2003 Shared Task - Language-Independent Named Entity Recognition
54. [Neural Architectures for Named Entity Recognition](2016-neural-architectures-for-named-entity-recognition)
55. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method)
56. Natural Language Inference over Interaction Space
57. Deep Semantic Role Labeling - What Works and What's Next
58. [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](2016-ask-me-anything-dynamic-memory-networks-for-natural-language-processing)
59. Easy Victories and Uphill Battles in Coreference Resolution
