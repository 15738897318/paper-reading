---
title: A Neural Attention Model for Abstractive Sentence Summarization
authors:
- Alexander M. Rush
- S. Chopra
- J. Weston
fieldsOfStudy:
- Computer Science
meta_key: 2015-a-neural-attention-model-for-abstractive-sentence-summarization
numCitedBy: 2122
reading_status: TBD
ref_count: 45
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/A-Neural-Attention-Model-for-Abstractive-Sentence-Rush-Chopra/5082a1a13daea5c7026706738f8528391a1e6d59?sort=total-citations
venue: EMNLP
year: 2015
---

# A Neural Attention Model for Abstractive Sentence Summarization

## Abstract

Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.

## Paper References

1. Summarization beyond sentence extraction - A probabilistic approach to sentence compression
2. Headline Generation Based on Statistical Translation
3. Using Hidden Markov Modeling to Decompose Human-Written Summaries
4. Sentence Compression by Deletion with LSTMs
5. Overcoming the Lack of Parallel Data in Sentence Compression
6. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
7. Sentence Compression Beyond Word Deletion
8. Sentence Simplification by Monolingual Machine Translation
9. [Recurrent Continuous Translation Models](2013-recurrent-continuous-translation-models)
10. Global inference for sentence compression - an integer linear programming approach
11. DUC in context
12. BBN/UMD at DUC-2004 - Topiary
13. Addressing the Rare Word Problem in Neural Machine Translation
14. Annotated Gigaword
15. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
16. Hedge Trimmer - A Parse-and-Trim Approach to Headline Generation
17. [The Stanford CoreNLP Natural Language Processing Toolkit](2014-the-stanford-corenlp-natural-language-processing-toolkit)
18. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
19. [ROUGE - A Package for Automatic Evaluation of Summaries](2004-rouge-a-package-for-automatic-evaluation-of-summaries)
20. Hierarchical Probabilistic Neural Network Language Model
21. [Minimum Error Rate Training in Statistical Machine Translation](2003-minimum-error-rate-training-in-statistical-machine-translation)
22. A Neural Probabilistic Language Model
23. Empirical Evaluation and Combination of Advanced Language Modeling Techniques
24. [Moses - Open Source Toolkit for Statistical Machine Translation](2007-moses-open-source-toolkit-for-statistical-machine-translation)
25. Z-MERT - A Fully Configurable Open Source Tool for Minimum Error Rate Training of Machine Translation Systems
26. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors)
27. Introduction to information retrieval
28. [Introduction to Information Retrieval](2010-introduction-to-information-retrieval)
29. A Noisy-Channel Model for Document Compression
