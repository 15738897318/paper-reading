---
title: On the difficulty of training recurrent neural networks
authors:
- Razvan Pascanu
- Tomas Mikolov
- Yoshua Bengio
fieldsOfStudy:
- Computer Science
meta_key: 2013-on-the-difficulty-of-training-recurrent-neural-networks
numCitedBy: 3820
reading_status: TBD
ref_count: 37
tags:
- gen-from-ref
- paper
venue: ICML
year: 2013
---

# On the difficulty of training recurrent neural networks

## Abstract

There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.

## Paper References

1. Learning Recurrent Neural Networks with Hessian-Free Optimization
2. The problem of learning long-term dependencies in recurrent networks
3. Learning long-term dependencies with gradient descent is difficult
4. Advances in optimizing recurrent networks
5. New results on recurrent network training - unifying the algorithms and accelerating convergence
6. A Learning Algorithm for Continually Running Fully Recurrent Neural Networks
7. [Generating Text with Recurrent Neural Networks](2011-generating-text-with-recurrent-neural-networks)
8. Learning representations by back-propagating errors
9. Training recurrent neural networks
10. On the training of recurrent neural networks
11. Bifurcations of Recurrent Neural Networks in Gradient Descent Learning
12. Reservoir computing approaches to recurrent neural network training
13. Optimization and applications of echo state networks with leaky- integrator neurons
14. Generalization of backpropagation with application to a recurrent gas market model
15. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization)
16. SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
17. Long Short-Term Memory in Echo State Networks - Details of a Simulation Study
18. [Long Short-Term Memory](1997-long-short-term-memory)
19. [Theano - new features and speed improvements](2012-theano-new-features-and-speed-improvements)
20. A neurodynamical model for working memory
21. On the Computational Power of Neural Nets
22. Harnessing Nonlinearity - Predicting Chaotic Systems and Saving Energy in Wireless Communication
23. Neural Networks with Adaptive Learning Rate and Momentum Terms
24. Finding Structure in Time
25. Statistical Language Models Based on Neural Networks
26. Modeling Temporal Dependencies in High-Dimensional Sequences - Application to Polyphonic Music Generation and Transcription
27. Learning representations by backpropagating errors
28. Empirical Evaluation and Combination of Advanced Language Modeling Techniques
29. Adaptive Synchronization of Neural and Physical Oscillators
30. Nonlinear Dynamics and Chaos - With Applications to Physics, Biology, Chemistry and Engineering
31. A Novel Connectionist System for Unconstrained Handwriting Recognition
