---
title: Neural Turing Machines
authors:
- A. Graves
- Greg Wayne
- Ivo Danihelka
fieldsOfStudy:
- Computer Science
meta_key: 2014-neural-turing-machines
numCitedBy: 1643
reading_status: TBD
ref_count: 44
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Neural-Turing-Machines-Graves-Wayne/c3823aacea60bc1f2cabb9283144690a3d015db5?sort=total-citations
venue: ArXiv
year: 2014
---

# Neural Turing Machines

## Abstract

We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.

## Paper References

1. On the Computational Power of Neural Nets
2. Neural networks and physical systems with emergent collective computational abilities.
3. Learning Context-free Grammars - Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory
4. Recursive Distributed Representations
5. Simple Substrates for Complex Cognition
6. The Algebraic Mind - Integrating Connectionism and Cognitive Science
7. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
8. Continuous attractors and oculomotor control
9. BoltzCONS - Dynamic Symbol Structures in a Connectionist Network
10. A general framework for adaptive processing of data structures
11. Cellular basis of working memory
12. Banishing the homunculus - Making working memory work
13. [Generating Text with Recurrent Neural Networks](2011-generating-text-with-recurrent-neural-networks)
14. Learning to Learn Using Gradient Descent
15. Computation Finite And Infinite Machines
16. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
17. Synaptic Basis of Cortical Persistent Activity - the Importance of NMDA Receptors to Working Memory
18. [Speech recognition with deep recurrent neural networks](2013-speech-recognition-with-deep-recurrent-neural-networks)
19. Holographic Reduced Representation - Distributed Representation for Cognitive Structures
20. Connectionism and cognitive architecture - A critical analysis
21. Memory and the Computational Brain - Why Cognitive Science will Transform Neuroscience
22. The magical number seven plus or minus two - some limits on our capacity for processing information.
23. [Long Short-Term Memory](1997-long-short-term-memory)
24. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
25. [Towards End-To-End Speech Recognition with Recurrent Neural Networks](2014-towards-end-to-end-speech-recognition-with-recurrent-neural-networks)
26. The importance of mixed selectivity in complex cognitive tasks
27. Hyperdimensional Computing - An Introduction to Computing in Distributed Representation with High-Dimensional Random Vectors
28. The Problem of Rapid Variable Creation
29. Time constraints and resource sharing in adults' working memory spans.
30. Three Models for the Description of Language
31. Machine learning - a probabilistic perspective
32. [Semantic Compositionality through Recursive Matrix-Vector Spaces](2012-semantic-compositionality-through-recursive-matrix-vector-spaces)
33. Memory
34. How to Build a Brain - A Neural Architecture for Biological Cognition
35. The nature of the language faculty and its implications for evolution of language (Reply to Fitch, Hauser, and Chomsky)
36. Tensor Product Variable Binding and the Representation of Symbolic Structures in Connectionist Systems
37. First draft of a report on the EDVAC
38. Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
39. The evolution of the language faculty - Clarifications and implications
40. Learning distributed representations of concepts.
41. [Parallel & distributed processing](2005-parallel-distributed-processing)
42. The cognitive revolution - a historical perspective
