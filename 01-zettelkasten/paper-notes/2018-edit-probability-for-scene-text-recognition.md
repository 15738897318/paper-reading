---
title: Edit Probability for Scene Text Recognition
authors:
- Fan Bai
- Zhanzhan Cheng
- Yi Niu
- Shiliang Pu
- Shuigeng Zhou
fieldsOfStudy:
- Computer Science
meta_key: 2018-edit-probability-for-scene-text-recognition
numCitedBy: 127
reading_status: TBD
ref_count: 41
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Edit-Probability-for-Scene-Text-Recognition-Bai-Cheng/2a6a4cd7623d12b467571461e8c19a3138474908?sort=total-citations
venue: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition
year: 2018
---

[semanticscholar url](https://www.semanticscholar.org/paper/Edit-Probability-for-Scene-Text-Recognition-Bai-Cheng/2a6a4cd7623d12b467571461e8c19a3138474908?sort=total-citations)

# Edit Probability for Scene Text Recognition

## Abstract

We consider the scene text recognition problem under the attention-based encoder-decoder framework, which is the state of the art. The existing methods usually employ a frame-wise maximal likelihood loss to optimize the models. When we train the model, the misalignment between the ground truth strings and the attention's output sequences of probability distribution, which is caused by missing or superfluous characters, will confuse and mislead the training process, and consequently make the training costly and degrade the recognition accuracy. To handle this problem, we propose a novel method called edit probability (EP) for scene text recognition. EP tries to effectively estimate the probability of generating a string from the output sequence of probability distribution conditioned on the input image, while considering the possible occurrences of missing/superfluous characters. The advantage lies in that the training process can focus on the missing, superfluous and unrecognized characters, and thus the impact of the misalignment problem can be alleviated or even overcome. We conduct extensive experiments on standard benchmarks, including the IIIT-5K, Street View Text and ICDAR datasets. Experimental results show that the EP can substantially boost scene text recognition performance.

## Paper References

1. Scene Text Recognition using Higher Order Language Priors
2. [Focusing Attention - Towards Accurate Text Recognition in Natural Images](2017-focusing-attention-towards-accurate-text-recognition-in-natural-images)
3. End-to-end scene text recognition
4. [Robust Scene Text Recognition with Automatic Rectification](2016-robust-scene-text-recognition-with-automatic-rectification)
5. Label Embedding - A Frugal Baseline for Text Recognition
6. Deep Structured Output Learning for Unconstrained Text Recognition
7. Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition
8. Whole is Greater than Sum of Parts - Recognizing Scene Text Words
9. Reading Text in the Wild with Convolutional Neural Networks
10. Accurate Scene Text Recognition Based on Recurrent Neural Network
11. Real-time scene text localization and recognition
12. [An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition](2017-an-end-to-end-trainable-neural-network-for-image-based-sequence-recognition-and-its-application-to-scene-text-recognition)
13. Supervised mid-level features for word image representation
14. Strokelets - A Learned Multi-scale Representation for Scene Text Recognition
15. Word Spotting in the Wild
16. Word Spotting and Recognition with Embedded Attributes
17. End-to-end text recognition with convolutional neural networks
18. [Synthetic Data for Text Localisation in Natural Images](2016-synthetic-data-for-text-localisation-in-natural-images)
19. PhotoOCR - Reading Text in Uncontrolled Conditions
20. End-to-End Text Recognition with Hybrid HMM Maxout Models
21. End-to-end attention-based large vocabulary speech recognition
22. Joint CTC-attention based end-to-end speech recognition using multi-task learning
23. [Deep Residual Learning for Image Recognition](2015-resnet.md)
24. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
25. ICDAR 2003 robust reading competitions
26. [Recursive Recurrent Nets with Attention Modeling for OCR in the Wild](2016-recursive-recurrent-nets-with-attention-modeling-for-ocr-in-the-wild)
27. ICDAR 2015 competition on Robust Reading
28. [Speech recognition with deep recurrent neural networks](2013-speech-recognition-with-deep-recurrent-neural-networks)
29. ICDAR 2013 Robust Reading Competition
30. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
31. [On the Properties of Neural Machine Translation - Encoder-Decoder Approaches](2014-on-the-properties-of-neural-machine-translation-encoder-decoder-approaches)
32. [Connectionist temporal classification - labelling unsegmented sequence data with recurrent neural networks](2006-connectionist-temporal-classification-labelling-unsegmented-sequence-data-with-recurrent-neural-networks)
33. [Caffe - Convolutional Architecture for Fast Feature Embedding](2014-caffe-convolutional-architecture-for-fast-feature-embedding)
34. [Long Short-Term Memory](1997-long-short-term-memory)
35. Learning representations by back-propagating errors
36. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method)
37. File searching using variable length keys
38. Et al
39. (b)
