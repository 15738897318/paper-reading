---
title: Deeply-Supervised Nets
authors:
- Chen-Yu Lee
- Saining Xie
- Patrick W. Gallagher
- Zhengyou Zhang
- Z. Tu
fieldsOfStudy:
- Computer Science
meta_key: 2015-deeply-supervised-nets
numCitedBy: 1449
reading_status: TBD
ref_count: 48
tags:
- gen-from-ref
- other-default
- paper
venue: AISTATS
year: 2015
---

# Deeply-Supervised Nets

## Abstract

Our proposed deeply-supervised nets (DSN) method simultaneously minimizes classification error while making the learning process of hidden layers direct and transparent. We make an attempt to boost the classification performance by studying a new formulation in deep networks. Three aspects in convolutional neural networks (CNN) style architectures are being looked at: (1) transparency of the intermediate layers to the overall classification; (2) discriminativeness and robustness of learned features, especially in the early layers; (3) effectiveness in training due to the presence of the exploding and vanishing gradients. We introduce "companion objective" to the individual hidden layers, in addition to the overall objective at the output layer (a different strategy to layer-wise pre-training). We extend techniques from stochastic gradient methods to analyze our algorithm. The advantage of our method is evident and our experimental result on benchmark datasets shows significant performance gain over existing methods (e.g. all state-of-the-art results on MNIST, CIFAR-10, CIFAR-100, and SVHN).

## Paper References

1. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks)
2. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition)
3. [Going deeper with convolutions](2015-going-deeper-with-convolutions)
4. [Network In Network](2014-network-in-network)
5. [DeCAF - A Deep Convolutional Activation Feature for Generic Visual Recognition](2014-decaf-a-deep-convolutional-activation-feature-for-generic-visual-recognition)
6. Deep Learning using Linear Support Vector Machines
7. [Visualizing and Understanding Convolutional Networks](2014-visualizing-and-understanding-convolutional-networks)
8. Discriminative Transfer Learning with Tree-based Priors
9. Understanding Deep Architectures using a Recursive Convolutional Network
10. Traffic sign recognition with multi-scale Convolutional Networks
11. [Greedy Layer-Wise Training of Deep Networks](2006-greedy-layer-wise-training-of-deep-networks)
12. Stochastic Pooling for Regularization of Deep Convolutional Neural Networks
13. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks)
14. Deep learning via semi-supervised embedding
15. Tiled convolutional neural networks
16. [Multi-column deep neural networks for image classification](2012-multi-column-deep-neural-networks-for-image-classification)
17. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
18. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations
19. [On the difficulty of training recurrent neural networks](2013-on-the-difficulty-of-training-recurrent-neural-networks)
20. [Maxout Networks](2013-maxout-networks)
21. [Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition](2012-context-dependent-pre-trained-deep-neural-networks-for-large-vocabulary-speech-recognition)
22. [Gradient-based learning applied to document recognition](1998-lenet5.md)
23. Nonparametric guidance of autoencoder representations using label information
24. [Regularization of Neural Networks using DropConnect](2013-regularization-of-neural-networks-using-dropconnect)
25. Large-scale Learning with SVM and Convolutional for Generic Object Categorization
26. What is the best multi-stage architecture for object recognition?
27. Distributed optimization of deeply nested systems
28. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors)
29. Making Gradient Descent Optimal for Strongly Convex Stochastic Optimization
30. Stochastic Gradient Descent for Non-smooth Optimization - Convergence Results and Optimal Averaging Schemes
31. Backpropagation Applied to Handwritten Zip Code Recognition
32. Regularized M-estimators with nonconvexity - statistical and algorithmic theory for local optima
33. Distributed representations, simple recurrent networks, and grammatical structure
34. Distributed Representations
35. Adaptive Algorithms and Stochastic Approximations
36. [The Nature of Statistical Learning Theory](2000-the-nature-of-statistical-learning-theory)
