---
title: A training algorithm for optimal margin classifiers
authors:
- B. Boser
- I. Guyon
- V. Vapnik
fieldsOfStudy:
- Computer Science
meta_key: 1992-a-training-algorithm-for-optimal-margin-classifiers
numCitedBy: 10868
reading_status: TBD
ref_count: 40
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon/2599131a4bc2fa957338732a37c744cfe3e17b24?sort=total-citations
venue: COLT '92
year: 1992
---

[semanticscholar url](https://www.semanticscholar.org/paper/A-training-algorithm-for-optimal-margin-classifiers-Boser-Guyon/2599131a4bc2fa957338732a37c744cfe3e17b24?sort=total-citations)

# A training algorithm for optimal margin classifiers

## Abstract

A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of the classification functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.

## Paper References

1. Structural Risk Minimization for Character Recognition
2. Computer aided cleaning of large databases for character recognition
3. Comparing different neural network architectures for classifying handwritten digits
4. Regularization Algorithms for Learning That Are Equivalent to Multilayer Networks
5. Consistent inference of probabilities in layered networks - predictions and generalizations
6. Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network
7. What Size Net Gives Valid Generalization?
8. Predicting {0,1}-functions on randomly drawn points
9. Neural Networks and the Bias/Variance Dilemma
10. Fast Learning in Networks of Locally-Tuned Processing Units
11. Bumptrees for Efficient Function, Constraint and Classification Learning
12. Handwritten Digit Recognition with a Back-Propagation Network
13. A Practical Bayesian Framework for Backprop Networks
14. Pattern classification and scene analysis
15. Learning algorithms with optimal stability in neural networks
16. Methods of Mathematical Physics
17. Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning
18. Methods of Mathematical Physics
19. Numerical methods for non-linear optimization
20. Principles of neurodynamics
21. Numerical Methods for Non-Linear Optimization
22. Problem Complexity and Method Efficiency in Optimization
23. Multivariable Functional Interpolation and Adaptive Networks
