---
title: Extensions of recurrent neural network language model
authors:
- Tomas Mikolov
- Stefan Kombrink
- L. Burget
- "J. Cernock\xFD"
- S. Khudanpur
fieldsOfStudy:
- Computer Science
meta_key: 2011-extensions-of-recurrent-neural-network-language-model
numCitedBy: 1428
reading_status: TBD
ref_count: 23
tags:
- gen-from-ref
- other-default
- paper
venue: 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)
year: 2011
---

# Extensions of recurrent neural network language model

## Abstract

We present several modifications of the original recurrent neural network language model (RNN LM).While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.

## Paper References

1. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model)
2. Hierarchical Probabilistic Neural Network Language Model
3. Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model
4. Training Neural Network Language Models on Very Large Corpora
5. Learning representations by back-propagating errors
6. Learning long-term dependencies with gradient descent is difficult
7. A guide to recurrent neural networks and backpropagation
8. Classes for fast maximum entropy training
9. Exact training of a neural syntactic language model
10. Factored Neural Language Models
11. A Neural Syntactic Language Model
12. A Neural Probabilistic Language Model
13. Finding Structure in Time
14. A bit of progress in language modeling
15. Neural network based language models for highly inflective languages
16. Random forests and the data sparseness problem in language modeling
17. Scaling learning algorithms towards AI
18. A Joint Language Model With Fine-grain Syntactic Tags
19. Learning internal representations by back-propagating errors
