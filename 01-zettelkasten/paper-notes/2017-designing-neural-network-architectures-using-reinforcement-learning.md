---
title: Designing Neural Network Architectures using Reinforcement Learning
authors:
- Bowen Baker
- Otkrist Gupta
- Nikhil Naik
- R. Raskar
fieldsOfStudy:
- Computer Science
meta_key: 2017-designing-neural-network-architectures-using-reinforcement-learning
numCitedBy: 1064
reading_status: TBD
ref_count: 47
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Designing-Neural-Network-Architectures-using-Baker-Gupta/6cd5dfccd9f52538b19a415e00031d0ee4e5b181?sort=total-citations
venue: ICLR
year: 2017
---

# Designing Neural Network Architectures using Reinforcement Learning

## Abstract

At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using $Q$-learning with an $\epsilon$-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.

## Paper References

1. Generative NeuroEvolution for Deep Learning
2. [Network In Network](2014-network-in-network.md)
3. Recurrent convolutional neural network for object recognition
4. [Generalizing Pooling Functions in Convolutional Neural Networks - Mixed, Gated, and Tree](2016-generalizing-pooling-functions-in-convolutional-neural-networks-mixed-gated-and-tree.md)
5. [Striving for Simplicity - The All Convolutional Net](2015-striving-for-simplicity-the-all-convolutional-net.md)
6. [Human-level control through deep reinforcement learning](2015-human-level-control-through-deep-reinforcement-learning.md)
7. Reinforcement learning for robots using neural networks
8. [FitNets - Hints for Thin Deep Nets](2015-fitnets-hints-for-thin-deep-nets.md)
9. End-to-End Training of Deep Visuomotor Policies
10. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks.md)
11. Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves
12. [Caffe - Convolutional Architecture for Fast Feature Embedding](2014-caffe-convolutional-architecture-for-fast-feature-embedding.md)
13. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
14. [Mastering the game of Go with deep neural networks and tree search](2016-mastering-the-game-of-go-with-deep-neural-networks-and-tree-search.md)
15. Continuous control with deep reinforcement learning
16. [Algorithms for Hyper-Parameter Optimization](2011-algorithms-for-hyper-parameter-optimization.md)
17. [Convolutional Neural Fabrics](2016-convolutional-neural-fabrics.md)
18. Convolutional neural networks applied to house numbers digit classification
19. Evolving Neural Networks through Augmenting Topologies
20. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks.md)
21. [Practical Bayesian Optimization of Machine Learning Algorithms](2012-practical-bayesian-optimization-of-machine-learning-algorithms.md)
22. [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](2016-fast-and-accurate-deep-network-learning-by-exponential-linear-units-elus.md)
23. Self-improving reactive agents based on reinforcement learning, planning and teaching
24. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
25. Multi-Task Bayesian Optimization
26. [Regularization of Neural Networks using DropConnect](2013-regularization-of-neural-networks-using-dropconnect.md)
27. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
28. [Maxout Networks](2013-maxout-networks.md)
29. A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation
30. Reinforcement Learning - A Survey
31. Experience Replay for Real-Time Reinforcement Learning Control
32. [Making a Science of Model Search - Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures](2013-making-a-science-of-model-search-hyperparameter-optimization-in-hundreds-of-dimensions-for-vision-architectures.md)
33. Under review as a conference paper at ICLR 2017
34. [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](2016-under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
35. Pedestrian Detection with Unsupervised Multi-stage Feature Learning
36. Multi-armed Bandit Algorithms and Empirical Evaluation
37. Combinations of genetic algorithms and neural networks - a survey of the state of the art
38. [Taking the Human Out of the Loop - A Review of Bayesian Optimization](2016-taking-the-human-out-of-the-loop-a-review-of-bayesian-optimization.md)
39. [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](2018-published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks.md)
40. Learning from delayed rewards
41. Convex Optimization Algorithms
42. [Deep Learning](2016-deep-learning.md)
