---
title: VideoBERT - A Joint Model for Video and Language Representation Learning
authors:
- Chen Sun
- Austin Myers
- Carl Vondrick
- K. Murphy
- C. Schmid
fieldsOfStudy:
- Computer Science
meta_key: 2019-videobert-a-joint-model-for-video-and-language-representation-learning
numCitedBy: 584
reading_status: TBD
ref_count: 40
tags:
- gen-from-ref
- other-default
- paper
venue: 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
year: 2019
---

# VideoBERT - A Joint Model for Video and Language Representation Learning

## Abstract

Self-supervised learning has become increasingly important to leverage the abundance of unlabeled data available on platforms like YouTube. Whereas most existing approaches learn low-level representations, we propose a joint visual-linguistic model to learn high-level features without any explicit supervision. In particular, inspired by its recent success in language modeling, we build upon the BERT model to learn bidirectional joint distributions over sequences of visual and linguistic tokens, derived from vector quantization of video data and off-the-shelf speech recognition outputs, respectively. We use VideoBERT in numerous tasks, including action classification and video captioning. We show that it can be applied directly to open-vocabulary classification, and confirm that large amounts of training data and cross-modal information are critical to performance. Furthermore, we outperform the state-of-the-art on video captioning, and quantitative results verify that the model learns high-level semantic features.

## Paper References

1. Shuffle and Learn - Unsupervised Learning Using Temporal Order Verification
2. SoundNet - Learning Sound Representations from Unlabeled Video
3. Anticipating Visual Representations from Unlabeled Video
4. Generating Videos with Scene Dynamics
5. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
6. Deep multi-scale video prediction beyond mean square error
7. [Neural Baby Talk](2018-neural-baby-talk)
8. Weakly-Supervised Video Object Grounding from Text by Loss Weighting and Object Interaction
9. [Attention is All you Need](2017-transformer.md)
10. Stochastic Variational Video Prediction
11. SLAC - A Sparsely Labeled Dataset for Action Classification and Localization
12. Visual Dynamics - Probabilistic Future Frame Synthesis via Cross Convolutional Networks
13. End-to-End Dense Video Captioning with Masked Transformer
14. Revisiting Unreasonable Effectiveness of Data in Deep Learning Era
15. COIN - A Large-Scale Dataset for Comprehensive Instructional Video Analysis
16. An Uncertain Future - Forecasting from Static Images Using Variational Autoencoders
17. [Cross-lingual Language Model Pretraining](2019-cross-lingual-language-model-pretraining)
18. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
19. Stochastic Video Generation with a Learned Prior
20. Moments in Time Dataset - One Million Videos for Event Understanding
21. Towards Automatic Learning of Procedures From Web Instructional Videos
22. Rethinking Spatiotemporal Feature Learning For Video Understanding
23. Stochastic Adversarial Video Prediction
24. MoCoGAN - Decomposing Motion and Content for Video Generation
25. AVA - A Video Dataset of Spatio-Temporally Localized Atomic Visual Actions
26. [CIDEr - Consensus-based image description evaluation](2015-cider-consensus-based-image-description-evaluation)
27. [Going deeper with convolutions](2015-going-deeper-with-convolutions)
28. Ambient Sound Provides Supervision for Visual Learning
29. The Kinetics Human Action Video Dataset
30. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation)
31. What's Cookin'? Interpreting Cooking Videos using Text, Speech and Vision
32. Baby Talk - Understanding and Generating Image Descriptions
33. BERT has a Mouth, and It Must Speak - BERT as a Markov Random Field Language Model
34. [ROUGE - A Package for Automatic Evaluation of Summaries](2004-rouge-a-package-for-automatic-evaluation-of-summaries)
35. Visually Indicated Sounds
36. Dense-Captioning Events in Videos
37. Unsupervised Learning from Narrated Instruction Videos
