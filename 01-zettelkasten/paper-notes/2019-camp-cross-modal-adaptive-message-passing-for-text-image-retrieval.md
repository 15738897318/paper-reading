---
title: CAMP - Cross-Modal Adaptive Message Passing for Text-Image Retrieval
authors:
- Zihao Wang
- Xihui Liu
- Hongsheng Li
- Lu Sheng
- Junjie Yan
- Xiaogang Wang
- Jing Shao
fieldsOfStudy:
- Computer Science
meta_key: 2019-camp-cross-modal-adaptive-message-passing-for-text-image-retrieval
numCitedBy: 104
reading_status: TBD
ref_count: 36
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/CAMP:-Cross-Modal-Adaptive-Message-Passing-for-Wang-Liu/19c630ad5a9de227f6357479fc95c62667be17f6?sort=total-citations
venue: 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
year: 2019
---

[semanticscholar url](https://www.semanticscholar.org/paper/CAMP:-Cross-Modal-Adaptive-Message-Passing-for-Wang-Liu/19c630ad5a9de227f6357479fc95c62667be17f6?sort=total-citations)

# CAMP - Cross-Modal Adaptive Message Passing for Text-Image Retrieval

## Abstract

Text-image cross-modal retrieval is a challenging task in the field of language and vision. Most previous approaches independently embed images and sentences into a joint embedding space and compare their similarities. However, previous approaches rarely explore the interactions between images and sentences before calculating similarities in the joint space. Intuitively, when matching between images and sentences, human beings would alternatively attend to regions in images and words in sentences, and select the most salient information considering the interaction between both modalities. In this paper, we propose Cross-modal Adaptive Message Passing (CAMP), which adaptively controls the information flow for message passing across modalities. Our approach not only takes comprehensive and fine-grained cross-modal interactions into account, but also properly handles negative pairs and irrelevant information with an adaptive gating scheme. Moreover, instead of conventional joint embedding approaches for text-image matching, we infer the matching score based on the fused features, and propose a hardest negative binary cross-entropy loss for training. Results on COCO and Flickr30k significantly surpass state-of-the-art methods, demonstrating the effectiveness of our approach.

## Paper References

1. [Stacked Cross Attention for Image-Text Matching](2018-stacked-cross-attention-for-image-text-matching)
2. Look, Imagine and Match - Improving Textual-Visual Cross-Modal Retrieval with Generative Models
3. Dual-Path Convolutional Image-Text Embedding
4. Deep Cross-Modal Projection Learning for Image-Text Matching
5. Hierarchical Multimodal LSTM for Dense Visual-Semantic Embedding
6. [Learning Semantic Concepts and Order for Image and Sentence Matching](2018-learning-semantic-concepts-and-order-for-image-and-sentence-matching)
7. Conditional Image-Text Embedding Networks
8. [Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](2014-unifying-visual-semantic-embeddings-with-multimodal-neural-language-models)
9. Improved Fusion of Visual and Language Representations by Dense Symmetric Co-attention for Visual Question Answering
10. [Learning Deep Structure-Preserving Image-Text Embeddings](2016-learning-deep-structure-preserving-image-text-embeddings)
11. [Dual Attention Networks for Multimodal Reasoning and Matching](2017-dual-attention-networks-for-multimodal-reasoning-and-matching)
12. [Knowing When to Look - Adaptive Attention via a Visual Sentinel for Image Captioning](2017-knowing-when-to-look-adaptive-attention-via-a-visual-sentinel-for-image-captioning)
13. [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](2016-multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding)
14. Show, Tell and Discriminate - Image Captioning by Self-retrieval with Partially Labeled Data
15. Multi-level Attention Networks for Visual Question Answering
16. [From captions to visual concepts and back](2015-from-captions-to-visual-concepts-and-back)
17. [Linking Image and Text with 2-Way Nets](2017-linking-image-and-text-with-2-way-nets)
18. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
19. Structured Attentions for Visual Question Answering
20. [Bottom-Up and Top-Down Attention for Image Captioning and VQA](2017-bottom-up-and-top-down-attention-for-image-captioning-and-vqa)
21. [Hierarchical Question-Image Co-Attention for Visual Question Answering](2016-hierarchical-question-image-co-attention-for-visual-question-answering)
22. SCA-CNN - Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning
23. Associating neural word embeddings with deep image representations using Fisher Vectors
24. [Ask, Attend and Answer - Exploring Question-Guided Spatial Attention for Visual Question Answering](2016-ask-attend-and-answer-exploring-question-guided-spatial-attention-for-visual-question-answering)
25. [VSE++ - Improved Visual-Semantic Embeddings](2017-vse-improved-visual-semantic-embeddings)
26. [Bilinear Attention Networks](2018-bilinear-attention-networks)
27. [VSE++ - Improving Visual-Semantic Embeddings with Hard Negatives](2018-vse-improving-visual-semantic-embeddings-with-hard-negatives)
28. [Stacked Attention Networks for Image Question Answering](2016-stacked-attention-networks-for-image-question-answering)
29. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
30. [Order-Embeddings of Images and Language](2016-order-embeddings-of-images-and-language)
31. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn.md)
32. [Compact Bilinear Pooling](2016-compact-bilinear-pooling)
33. [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](2014-from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions)
34. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](2014-empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling)
35. Improving Referring Expression Grounding With Cross-Modal Attention-Guided Erasing
