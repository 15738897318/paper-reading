---
title: Multitask Learning
authors:
- R. Caruana
fieldsOfStudy:
- Computer Science
meta_key: 2004-multitask-learning
numCitedBy: 3378
reading_status: TBD
ref_count: 99
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Multitask-Learning-Caruana/161ffb54a3fdf0715b198bb57bd22f910242eb49?sort=total-citations
venue: Machine Learning
year: 2004
---

[semanticscholar url](https://www.semanticscholar.org/paper/Multitask-Learning-Caruana/161ffb54a3fdf0715b198bb57bd22f910242eb49?sort=total-citations)

# Multitask Learning

## Abstract

Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.

## Paper References

1. Multitask Learning - A Knowledge-Based Source of Inductive Bias
2. Discovering Structure in Multiple Learning Tasks - The TC Algorithm
3. Learning Many Related Tasks at the Same Time with Backpropagation
4. Solving Multiclass Learning Problems via Error-Correcting Output Codes
5. Learning internal representations
6. Goal-directed clustering
7. Lifelong Learning - A Case Study.
8. Explanation-based neural network learning
9. Learning One More Thing
10. A Bayesian/information theoretic model of bias learning
11. Is Learning The n-th Thing Any Easier Than Learning The First?
12. Explanation-based neural network learning a lifelong learning approach
13. Hints and the VC Dimension
14. What Size Net Gives Valid Generalization?
15. The Strength of Weak Learnability
16. Using the Future to Sort Out the Present - Rankprop and Multitask Learning for Medical Risk Evaluation
17. Rule-Injection Hints as a Means of Improving Network Performance and Learning Time
18. Self-organizing neural network that discovers surfaces in random-dot stereograms
19. Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network
20. A Comparative Study of ID3 and Backpropagation for English Text-to-Speech Mapping
21. Promoting Poor Features to Supervisors - Some Inputs Work Better as Outputs
22. Learning representations by back-propagating errors
23. Competition Among Networks Improves Committee Performance
24. [C4.5 - Programs for Machine Learning](1992-c4-5-programs-for-machine-learning)
25. Using the Representation in a Neural Network's Hidden Layer for Task-Specific Focus of Attention
26. Using Sampling and Queries to Extract Rules from Trained Neural Networks
27. A Probabilistic Approach to Feature Selection - A Filter Solution
28. Hints
29. Modularity and scaling in large phonemic neural networks
30. Symbolic-Neural Systems and the Use of Hints for Developing Complex Systems
31. A Comparison of ID3 and Backpropagation for English Text-To-Speech Mapping
32. Multi-Task Learning for Stock Selection
33. Hierarchical Mixtures of Experts and the EM Algorithm
34. Learning from hints in neural networks
35. Neural Network Perception for Mobile Robot Guidance
36. Programs for Machine Learning
37. Non-literal Transfer Among Neural Network Learners
38. [Bagging predictors](2004-bagging-predictors)
39. Backpropagation Applied to Handwritten Zip Code Recognition
40. Learning representations by backpropagating errors
41. [Induction of Decision Trees](2004-induction-of-decision-trees)
42. Causation, prediction, and search
43. Conceptual Clustering, Learning from Examples, and Inference
44. Experience with a learning personal assistant
45. A Bayesian method for the induction of probabilistic networks from data
46. Training Neural Networks with Deficient Data
47. NETtalk - a parallel network that learns to read aloud
48. A Personal Learning Apprentice
49. Family Discovery
50. Generalization by Weight-Elimination with Application to Forecasting
51. Monotonicity Hints
52. Predicting Multivariate Responses in Multiple Linear Regression
53. Acquiring and Combining Overlapping Concepts
54. An evaluation of machine-learning methods for predicting pneumonia mortality
55. A Powerful Heuristic for the Discovery of Complex Patterned Behaviour
56. Learning a Preference Predicate
57. Sensor fusion for autonomous outdoor navigation using neural networks
58. Validation of a pneumonia prognostic index using the MedisGroups Comparative Hospital Database.
59. Handwritten ZIP code recognition
60. Learning representations by back-propagation errors, nature
61. 19. Statistical Analysis with Missing Data
62. Mixture models for learning from incomplete data
63. Learning distributed representations of concepts.
64. Neural networks
65. Using additive noise in back-propagation training
66. Learning from time.
67. Direct Transfer of Learned Information Among Neural Networks
68. Statistical Analysis with Missing Data
69. Expectation-based selective attention
70. Toward Optimal Feature Selection
71. Supervised learning from incomplete data via an EM approach
