---
title: A Corpus for Reasoning about Natural Language Grounded in Photographs
authors:
- Alane Suhr
- Stephanie Zhou
- Iris Zhang
- Huajun Bai
- Yoav Artzi
fieldsOfStudy:
- Computer Science
meta_key: 2019-a-corpus-for-reasoning-about-natural-language-grounded-in-photographs
numCitedBy: 214
reading_status: TBD
ref_count: 78
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/A-Corpus-for-Reasoning-about-Natural-Language-in-Suhr-Zhou/cf336d272a30d6ad6141db67faa64deb8791cd61?sort=total-citations
venue: ACL
year: 2019
---

[semanticscholar url](https://www.semanticscholar.org/paper/A-Corpus-for-Reasoning-about-Natural-Language-in-Suhr-Zhou/cf336d272a30d6ad6141db67faa64deb8791cd61?sort=total-citations)

# A Corpus for Reasoning about Natural Language Grounded in Photographs

## Abstract

We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.

## Paper References

1. A Corpus of Natural Language for Visual Reasoning
2. Visual Entailment - A Novel Task for Fine-Grained Image Understanding
3. Object Ordering with Bidirectional Matchings for Visual Reasoning
4. [CLEVR - A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning](2017-clevr-a-diagnostic-dataset-for-compositional-language-and-elementary-visual-reasoning)
5. TOUCHDOWN - Natural Language Navigation and Spatial Reasoning in Visual Street Environments
6. FigureQA - An Annotated Figure Dataset for Visual Reasoning
7. Bringing Semantics into Focus Using Visual Abstraction
8. A Joint Model of Language and Perception for Grounded Attribute Learning
9. Towards a Dataset for Human Computer Communication via Grounded Language Acquisition
10. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
11. Learning to interpret natural language navigation instructions from observations
12. Using Syntax to Ground Referring Expressions in Natural Images
13. Visual Dialog
14. ShapeWorld - A new test methodology for multimodal language understanding
15. Binary Image Selection (BISON) - Interpretable Evaluation of Visual Grounding
16. Cascaded Mutual Modulation for Visual Reasoning
17. [Yin and Yang - Balancing and Answering Binary Visual Questions](2016-yin-and-yang-balancing-and-answering-binary-visual-questions)
18. [From Recognition to Cognition - Visual Commonsense Reasoning](2019-from-recognition-to-cognition-visual-commonsense-reasoning)
19. A Survey of Current Datasets for Vision and Language Research
20. Weakly Supervised Semantic Parsing with Abstract Examples
21. Learning Distributions over Logical Forms for Referring Expression Generation
22. GQA - a new dataset for compositional question answering over real-world images
23. Transparency by Design - Closing the Gap Between Performance and Interpretability in Visual Reasoning
24. A dataset and architecture for visual reasoning with a working memory
25. Vision-and-Language Navigation - Interpreting Visually-Grounded Navigation Instructions in Real Environments
26. Learning to Disambiguate by Asking Discriminative Questions
27. [Neural Module Networks](2016-neural-module-networks)
28. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
29. [Learning to Compose Neural Networks for Question Answering](2016-learning-to-compose-neural-networks-for-question-answering)
30. Talk the Walk - Navigating New York City through Grounded Dialogue
31. Natural Reference to Objects in a Visual Domain
32. Bootstrap, Review, Decode - Using Out-of-Domain Textual Data to Improve Image Captioning
33. TallyQA - Answering Complex Counting Questions
34. Don't Just Assume; Look and Answer - Overcoming Priors for Visual Question Answering
35. [A simple neural network module for relational reasoning](2017-a-simple-neural-network-module-for-relational-reasoning)
36. Structured Attentions for Visual Question Answering
37. [Inferring and Executing Programs for Visual Reasoning](2017-inferring-and-executing-programs-for-visual-reasoning)
38. [Compositional Attention Networks for Machine Reasoning](2018-compositional-attention-networks-for-machine-reasoning)
39. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
40. C-VQA - A Compositional Split of the Visual Question Answering (VQA) v1.0 Dataset
41. [FiLM - Visual Reasoning with a General Conditioning Layer](2018-film-visual-reasoning-with-a-general-conditioning-layer)
42. An Analysis of Visual Question Answering Algorithms
43. [Learning to Reason - End-to-End Module Networks for Visual Question Answering](2017-learning-to-reason-end-to-end-module-networks-for-visual-question-answering)
44. Explainable Neural Computation via Stack Neural Module Networks
45. TVQA - Localized, Compositional Video Question Answering
46. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
47. Working Memory Networks - Augmenting Memory Networks with a Relational Reasoning Module
48. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
49. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
50. Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction
51. Learning Visual Question Answering by Bootstrapping Hard Attention
52. Scalable multi-label annotation
53. Finding Structure in Time
54. Effectively Crowdsourcing Radiology Report Annotations
55. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge)
56. DDRprog - A CLEVR Differentiable Dynamic Reasoning Programmer
57. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2014-vggnet.md)
58. [Deep Residual Learning for Image Recognition](2015-resnet.md)
59. [Microsoft COCO Captions - Data Collection and Evaluation Server](2015-microsoft-coco-captions-data-collection-and-evaluation-server)
60. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
61. [Long Short-Term Memory](1997-long-short-term-memory)
62. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](2017-inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning)
63. Mapping Navigation Instructions to Continuous Control Actions with Position-Visitation Prediction
64. “Caption” as a Coherence Relation - Evidence and Implications
65. Neural-Symbolic VQA - Disentangling Reasoning from Vision and Language Understanding
66. [Mask R-CNN](2017-mask-r-cnn.md)
67. The measurement of observer agreement for categorical data.
68. [Generation and Comprehension of Unambiguous Object Descriptions](2016-generation-and-comprehension-of-unambiguous-object-descriptions)
69. ReferItGame - Referring to Objects in Photographs of Natural Scenes
70. Walk the Talk - Connecting Language, Knowledge, and Action in Route Instructions
71. [WordNet - A Lexical Database for English](1992-wordnet-a-lexical-database-for-english)
