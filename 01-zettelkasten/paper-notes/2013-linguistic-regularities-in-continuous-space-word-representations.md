---
title: Linguistic Regularities in Continuous Space Word Representations
authors:
- Tomas Mikolov
- Wen-tau Yih
- G. Zweig
fieldsOfStudy:
- Computer Science
meta_key: 2013-linguistic-regularities-in-continuous-space-word-representations
numCitedBy: 3064
reading_status: TBD
ref_count: 24
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Linguistic-Regularities-in-Continuous-Space-Word-Mikolov-Yih/c4fd9c86b2b41df51a6fe212406dda81b1997fd4?sort=total-citations
venue: NAACL
year: 2013
---

# Linguistic Regularities in Continuous Space Word Representations

## Abstract

Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.

## Paper References

1. A Neural Probabilistic Language Model
2. Neural Probabilistic Language Models
3. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
4. Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing
5. Continuous space language models
6. Word Representations - A Simple and General Method for Semi-Supervised Learning
7. UTD - Determining Relational Similarity Using Lexical Patterns
8. Distributed representations, simple recurrent networks, and grammatical structure
9. Discovering Binary Codes for Documents by Learning Deep Generative Models
10. Structured Output Layer neural network language model
11. Hierarchical Probabilistic Neural Network Language Model
12. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
13. Domain and Function - A Dual-Space Model of Semantic Relations and Compositions
14. A Scalable Hierarchical Distributed Language Model
15. Recursive Distributed Representations
16. SemEval-2012 Task 2 - Measuring Degrees of Relational Similarity
17. Building a Large Annotated Corpus of English - The Penn Treebank
18. Strategies for training large scale neural network language models
19. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model)
20. Indexing by Latent Semantic Analysis
21. [Reducing the Dimensionality of Data with Neural Networks](2006-reducing-the-dimensionality-of-data-with-neural-networks)
22. Learning distributed representations of concepts.
