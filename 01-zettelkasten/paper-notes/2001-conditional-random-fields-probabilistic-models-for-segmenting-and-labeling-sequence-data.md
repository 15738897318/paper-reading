---
title: Conditional Random Fields - Probabilistic Models for Segmenting and Labeling Sequence Data
authors:
- J. Lafferty
- A. McCallum
- Fernando Pereira
fieldsOfStudy:
- Computer Science
meta_key: 2001-conditional-random-fields-probabilistic-models-for-segmenting-and-labeling-sequence-data
numCitedBy: 13449
reading_status: TBD
ref_count: 75
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Conditional-Random-Fields:-Probabilistic-Models-for-Lafferty-McCallum/f4ba954b0412773d047dc41231c733de0c1f4926?sort=total-citations
venue: ICML
year: 2001
---

# Conditional Random Fields - Probabilistic Models for Segmenting and Labeling Sequence Data

## Abstract

We present conditional random fields , a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.

## Paper References

1. Efficient Training of Conditional Random Fields
2. Maximum Entropy Markov Models for Information Extraction and Segmentation
3. Information Extraction with HMM Structures Learned by Stochastic Optimization
4. Inducing Features of Random Fields
5. A whole sentence maximum entropy language model
6. Boltzmann Chains and Hidden Markov Models
7. The Use of Classifiers in Sequential Inference
8. Equivalence of Linear Boltzmann Chains and Hidden Markov Models
9. A comparison of several approximate algorithms for finding multiple (N-best) sentence hypotheses
10. Biological Sequence Analysis - Probabilistic Models of Proteins and Nucleic Acids
11. Learning to Resolve Natural Language Ambiguities - A Unified Approach
12. A Maximum Entropy Approach to Natural Language Processing
13. A Maximum Entropy Model for Part-Of-Speech Tagging
14. Minimization algorithms for sequential transducers
15. An Introduction to Probabilistic Automata
16. [Foundations of statistical natural language processing](2002-foundations-of-statistical-natural-language-processing)
17. Finite-State Transducers in Language and Speech Processing
18. [A decision-theoretic generalization of on-line learning and an application to boosting](1995-a-decision-theoretic-generalization-of-on-line-learning-and-an-application-to-boosting)
19. [Gradient-based learning applied to document recognition](1998-lenet5.md)
20. [GradientBased Learning Applied to Document Recognition](2001-gradientbased-learning-applied-to-document-recognition)
21. Boosting Applied to Tagging and PP Attachment
22. Generalized Iterative Scaling for Log-Linear Models
23. Markov fields on finite graphs and lattices
24. Introduction to Probabilistic Automata
25. Transformation-Based Error-Driven Learning and Natural Language Processing - A Case Study in Part-of-Speech Tagging
