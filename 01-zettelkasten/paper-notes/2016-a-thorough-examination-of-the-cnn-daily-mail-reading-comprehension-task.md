---
title: A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task
authors:
- Danqi Chen
- Jason Bolton
- Christopher D. Manning
fieldsOfStudy:
- Computer Science
meta_key: 2016-a-thorough-examination-of-the-cnn-daily-mail-reading-comprehension-task
numCitedBy: 500
reading_status: TBD
ref_count: 22
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/A-Thorough-Examination-of-the-CNN/Daily-Mail-Task-Chen-Bolton/b1e20420982a4f923c08652941666b189b11b7fe?sort=total-citations
venue: ACL
year: 2016
---

# A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task

## Abstract

Enabling a computer to understand a document so that it can answer comprehension questions is a central, yet unsolved goal of NLP. A key factor impeding its solution by machine learned systems is the limited availability of human-annotated data. Hermann et al. (2015) seek to solve this problem by creating over a million training examples by pairing CNN and Daily Mail news articles with their summarized bullet points, and show that a neural network can then be trained to give good performance on this task. In this paper, we conduct a thorough examination of this new reading comprehension task. Our primary aim is to understand what depth of language understanding is required to do well on this task. We approach this from one side by doing a careful hand-analysis of a small subset of the problems and from the other by showing that simple, carefully designed systems can obtain accuracies of 73.6% and 76.6% on these two datasets, exceeding current state-of-the-art results by 7-10% and approaching what we believe is the ceiling for performance on this task.

## Paper References

1. Learning Answer-Entailing Structures for Machine Comprehension
2. [Towards AI-Complete Question Answering - A Set of Prerequisite Toy Tasks](2016-towards-ai-complete-question-answering-a-set-of-prerequisite-toy-tasks.md)
3. [Text Understanding with the Attention Sum Reader Network](2016-text-understanding-with-the-attention-sum-reader-network.md)
4. MCTest - A Challenge Dataset for the Open-Domain Machine Comprehension of Text
5. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend.md)
6. Reasoning in Vector Space - An Exploratory Study of Question Answering
7. [Effective Approaches to Attention-based Neural Machine Translation](2015-effective-approaches-to-attention-based-neural-machine-translation.md)
8. [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](2016-ask-me-anything-dynamic-memory-networks-for-natural-language-processing.md)
9. [The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations](2016-the-goldilocks-principle-reading-children-s-books-with-explicit-memory-representations.md)
10. [End-To-End Memory Networks](2015-end-to-end-memory-networks.md)
11. Modeling Biological Processes for Reading Comprehension
12. Dynamic Entity Representation with Max-pooling Improves Machine Reading
13. [Memory Networks](2015-memory-networks.md)
14. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation.md)
15. Machine Comprehension with Syntax, Frames, and Semantics
16. [A Fast and Accurate Dependency Parser using Neural Networks](2014-a-fast-and-accurate-dependency-parser-using-neural-networks.md)
17. Adapting boosting for information retrieval measures
18. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
19. Towards the Machine Comprehension of Text - An Essay
20. A Unified Theory of Inference for Text Understanding
