---
title: Pointer Sentinel Mixture Models
authors:
- Stephen Merity
- Caiming Xiong
- James Bradbury
- R. Socher
fieldsOfStudy:
- Computer Science
meta_key: 2017-pointer-sentinel-mixture-models
numCitedBy: 1047
reading_status: TBD
ref_count: 38
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Pointer-Sentinel-Mixture-Models-Merity-Xiong/efbd381493bb9636f489b965a2034d529cd56bcd?sort=total-citations
venue: ICLR
year: 2017
---

# Pointer Sentinel Mixture Models

## Abstract

Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.

## Paper References

1. Context dependent recurrent neural network language model
2. One billion word benchmark for measuring progress in statistical language modeling
3. Pointing the Unknown Words
4. Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks
5. [Character-Aware Neural Language Models](2016-character-aware-neural-language-models.md)
6. [Recurrent Highway Networks](2017-recurrent-highway-networks.md)
7. Latent Predictor Networks for Code Generation
8. [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](2016-ask-me-anything-dynamic-memory-networks-for-natural-language-processing.md)
9. [End-To-End Memory Networks](2015-end-to-end-memory-networks.md)
10. Language modeling with sum-product networks
11. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model.md)
12. [Text Understanding with the Attention Sum Reader Network](2016-text-understanding-with-the-attention-sum-reader-network.md)
13. Incorporating Copying Mechanism in Sequence-to-Sequence Learning
14. [Zoneout - Regularizing RNNs by Randomly Preserving Hidden Activations](2017-zoneout-regularizing-rnns-by-randomly-preserving-hidden-activations.md)
15. [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](2016-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.md)
16. [Dynamic Memory Networks for Visual and Textual Question Answering](2016-dynamic-memory-networks-for-visual-and-textual-question-answering.md)
17. [Pointer Networks](2015-pointer-networks.md)
18. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization.md)
19. A Maximum Entropy Approach to Adaptive Statistical Language Modeling
20. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
21. A maximum entropy approach to adaptive statistical language modelling
22. [Long Short-Term Memory](1997-long-short-term-memory.md)
23. Building a Large Annotated Corpus of English - The Penn Treebank
24. How to Construct Deep Recurrent Neural Networks
25. [On the difficulty of training recurrent neural networks](2013-on-the-difficulty-of-training-recurrent-neural-networks.md)
26. [Moses - Open Source Toolkit for Statistical Machine Translation](2007-moses-open-source-toolkit-for-statistical-machine-translation.md)
27. Open Source Toolkit for Statistical Machine Translation - Factored Translation Models and Lattice Decoding
