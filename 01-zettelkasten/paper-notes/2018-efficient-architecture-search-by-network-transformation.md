---
title: Efficient Architecture Search by Network Transformation
authors:
- Han Cai
- Tianyao Chen
- Weinan Zhang
- Yong Yu
- Jun Wang
fieldsOfStudy:
- Computer Science
meta_key: 2018-efficient-architecture-search-by-network-transformation
numCitedBy: 422
reading_status: TBD
ref_count: 61
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Efficient-Architecture-Search-by-Network-Cai-Chen/84e65a5bdb735d62eef4f72c2f01af354b2285ba?sort=total-citations
venue: AAAI
year: 2018
---

# Efficient Architecture Search by Network Transformation

## Abstract

Techniques for automatically designing deep neural network architectures such as reinforcement learning based approaches have recently shown promising results. However, their success is based on vast computational resources (e.g. hundreds of GPUs), making them difficult to be widely used. A noticeable limitation is that they still design and train each network from scratch during the exploration of the architecture space, which is highly inefficient. In this paper, we propose a new framework toward efficient architecture search by exploring the architecture space based on the current network and reusing its weights. We employ a reinforcement learning agent as the meta-controller, whose action is to grow the network depth or layer width with function-preserving transformations. As such, the previously validated networks can be reused for further exploration, thus saves a large amount of computational cost. We apply our method to explore the architecture space of the plain convolutional neural networks (no skip-connections, branching etc.) on image benchmark datasets (CIFAR-10, SVHN) with restricted computational resources (5 GPUs). Our method can design highly competitive networks that outperform existing networks using the same design scheme. On CIFAR-10, our model without skip-connections achieves 4.23% test error rate, exceeding a vast majority of modern architectures and approaching DenseNet. Furthermore, by applying our method to explore the DenseNet architecture space, we are able to achieve more accurate networks with fewer parameters.

## Paper References

1. [Designing Neural Network Architectures using Reinforcement Learning](2017-designing-neural-network-architectures-using-reinforcement-learning.md)
2. [DeepArchitect - Automatically Designing and Training Deep Architectures](2017-deeparchitect-automatically-designing-and-training-deep-architectures.md)
3. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
4. [Learning both Weights and Connections for Efficient Neural Network](2015-learning-both-weights-and-connections-for-efficient-neural-network.md)
5. [Wide Residual Networks](2016-wide-residual-networks.md)
6. [Generalizing Pooling Functions in Convolutional Neural Networks - Mixed, Gated, and Tree](2016-generalizing-pooling-functions-in-convolutional-neural-networks-mixed-gated-and-tree.md)
7. [Large-Scale Evolution of Image Classifiers](2017-large-scale-evolution-of-image-classifiers.md)
8. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks.md)
9. Net2Net - Accelerating Learning via Knowledge Transfer
10. [Network In Network](2014-network-in-network.md)
11. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks.md)
12. Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves
13. [Algorithms for Hyper-Parameter Optimization](2011-algorithms-for-hyper-parameter-optimization.md)
14. [Striving for Simplicity - The All Convolutional Net](2015-striving-for-simplicity-the-all-convolutional-net.md)
15. Scalable Bayesian Optimization Using Deep Neural Networks
16. Learning to reinforcement learn
17. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
18. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks.md)
19. [On the importance of initialization and momentum in deep learning](2013-on-the-importance-of-initialization-and-momentum-in-deep-learning.md)
20. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
21. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
22. [Mastering the game of Go with deep neural networks and tree search](2016-mastering-the-game-of-go-with-deep-neural-networks-and-tree-search.md)
23. [Training Very Deep Networks](2015-training-very-deep-networks.md)
24. Shake-Shake regularization
25. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
26. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
27. [Maxout Networks](2013-maxout-networks.md)
28. Evolving Neural Networks through Augmenting Topologies
29. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
30. [Learning to learn by gradient descent by gradient descent](2016-learning-to-learn-by-gradient-descent-by-gradient-descent.md)
31. [Practical Bayesian Optimization of Machine Learning Algorithms](2012-practical-bayesian-optimization-of-machine-learning-algorithms.md)
32. Towards Automatically-Tuned Neural Networks
33. [TensorFlow - Large-Scale Machine Learning on Heterogeneous Distributed Systems](2016-tensorflow-large-scale-machine-learning-on-heterogeneous-distributed-systems.md)
34. Designing Neural Networks using Genetic Algorithms
35. Bidirectional recurrent neural networks
36. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
37. [Simple statistical gradient-following algorithms for connectionist reinforcement learning](2004-simple-statistical-gradient-following-algorithms-for-connectionist-reinforcement-learning.md)
38. [Reinforcement Learning - An Introduction](2005-reinforcement-learning-an-introduction.md)
39. A Natural Policy Gradient
40. Learning Curve Prediction with Bayesian Neural Networks
41. [Reading Digits in Natural Images with Unsupervised Feature Learning](2011-reading-digits-in-natural-images-with-unsupervised-feature-learning.md)
42. [Random Search for Hyper-Parameter Optimization](2012-random-search-for-hyper-parameter-optimization.md)
43. Trust Region Policy Optimization
44. A Perspective View and Survey of Meta-Learning
45. Real-Time Bidding by Reinforcement Learning in Display Advertising
46. C
47. {m
48. 'I.
49. Q.
50. Y.
