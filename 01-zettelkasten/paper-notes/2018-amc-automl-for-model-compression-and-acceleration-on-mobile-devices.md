---
title: AMC - AutoML for Model Compression and Acceleration on Mobile Devices
authors:
- Yihui He
- Ji Lin
- Zhijian Liu
- Hanrui Wang
- Li-Jia Li
- Song Han
fieldsOfStudy:
- Computer Science
meta_key: 2018-amc-automl-for-model-compression-and-acceleration-on-mobile-devices
numCitedBy: 843
reading_status: TBD
ref_count: 74
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/AMC:-AutoML-for-Model-Compression-and-Acceleration-He-Lin/1717255b6aea01fe956cef998abbc3c399b5d7cf?sort=total-citations
venue: ECCV
year: 2018
---

[semanticscholar url](https://www.semanticscholar.org/paper/AMC:-AutoML-for-Model-Compression-and-Acceleration-He-Lin/1717255b6aea01fe956cef998abbc3c399b5d7cf?sort=total-citations)

# AMC - AutoML for Model Compression and Acceleration on Mobile Devices

## Abstract

Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4\(\times \) FLOPs reduction, we achieved 2.7% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53\(\times \) on the GPU (Titan Xp) and 1.95\(\times \) on an Android phone (Google Pixel 1), with negligible loss of accuracy.

## Paper References

1. [Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications](2016-compression-of-deep-convolutional-neural-networks-for-fast-and-low-power-mobile-applications.md)
2. ESE - Efficient Speech Recognition Engine with Sparse LSTM on FPGA
3. [NetAdapt - Platform-Aware Neural Network Adaptation for Mobile Applications](2018-netadapt-platform-aware-neural-network-adaptation-for-mobile-applications.md)
4. [EIE - Efficient Inference Engine on Compressed Deep Neural Network](2016-eie-efficient-inference-engine-on-compressed-deep-neural-network.md)
5. [Speeding up Convolutional Neural Networks with Low Rank Expansions](2014-speeding-up-convolutional-neural-networks-with-low-rank-expansions.md)
6. [Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding](2016-deep-compression-compressing-deep-neural-network-with-pruning-trained-quantization-and-huffman-coding.md)
7. Domain-Adaptive Deep Network Compression
8. [SqueezeNet - AlexNet-level accuracy with 50x fewer parameters and <1MB model size](2016-squeezenet-alexnet-level-accuracy-with-50x-fewer-parameters-and-1mb-model-size.md)
9. [Trained Ternary Quantization](2017-trained-ternary-quantization.md)
10. ThiNet - A Filter Level Pruning Method for Deep Neural Network Compression
11. Hierarchical Compression of Deep Convolutional Neural Networks on Large Scale Visual Recognition for Mobile Applications
12. [ShuffleNet - An Extremely Efficient Convolutional Neural Network for Mobile Devices](2018-shufflenet-an-extremely-efficient-convolutional-neural-network-for-mobile-devices.md)
13. LCNN - Lookup-Based Convolutional Neural Network
14. [Practical Block-Wise Neural Network Architecture Generation](2018-practical-block-wise-neural-network-architecture-generation.md)
15. SCNN - An accelerator for compressed-sparse convolutional neural networks
16. Compressing Deep Convolutional Networks using Vector Quantization
17. [Efficient Architecture Search by Network Transformation](2018-efficient-architecture-search-by-network-transformation.md)
18. [MobileNetV2 - Inverted Residuals and Linear Bottlenecks](2018-mobilenetv2-inverted-residuals-and-linear-bottlenecks.md)
19. [Caffe - Convolutional Architecture for Fast Feature Embedding](2014-caffe-convolutional-architecture-for-fast-feature-embedding.md)
20. [An Analysis of Deep Neural Network Models for Practical Applications](2016-an-analysis-of-deep-neural-network-models-for-practical-applications.md)
21. [Pruning Filters for Efficient ConvNets](2017-pruning-filters-for-efficient-convnets.md)
22. [MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications](2017-mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications.md)
23. [Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation](2014-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.md)
24. Channel-Level Acceleration of Deep Face Representations
25. [Runtime Neural Pruning](2017-runtime-neural-pruning.md)
26. [Binarized Neural Networks - Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](2016-binarized-neural-networks-training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or-1.md)
27. Network Trimming - A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures
28. Practical Network Blocks Design with Q-Learning
29. [XNOR-Net - ImageNet Classification Using Binary Convolutional Neural Networks](2016-xnor-net-imagenet-classification-using-binary-convolutional-neural-networks.md)
30. [SMASH - One-Shot Model Architecture Search through HyperNetworks](2018-smash-one-shot-model-architecture-search-through-hypernetworks.md)
31. Fast Training of Convolutional Networks through FFTs
32. Fast Convolutional Nets With fbfft - A GPU Performance Evaluation
33. [Accelerating Very Deep Convolutional Networks for Classification and Detection](2016-accelerating-very-deep-convolutional-networks-for-classification-and-detection.md)
34. [Channel Pruning for Accelerating Very Deep Neural Networks](2017-channel-pruning-for-accelerating-very-deep-neural-networks.md)
35. [Learning both Weights and Connections for Efficient Neural Network](2015-learning-both-weights-and-connections-for-efficient-neural-network.md)
36. Compact Deep Convolutional Neural Networks With Coarse Pruning
37. [Learning Transferable Architectures for Scalable Image Recognition](2018-learning-transferable-architectures-for-scalable-image-recognition.md)
38. N2N Learning - Network to Network Compression via Policy Gradient Reinforcement Learning
39. Structured Probabilistic Pruning for Convolutional Neural Network Acceleration
40. [Fast Algorithms for Convolutional Neural Networks](2016-fast-algorithms-for-convolutional-neural-networks.md)
41. Restructuring of deep neural network acoustic models with singular value decomposition
42. Structured Probabilistic Pruning for Deep Convolutional Neural Network Acceleration
43. Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition
44. Reinforcement Learning for Architecture Search by Network Transformation
45. [BinaryNet - Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](2016-binarynet-training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or-1.md)
46. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
47. [Large-Scale Evolution of Image Classifiers](2017-large-scale-evolution-of-image-classifiers.md)
48. [Evolving Deep Neural Networks](2019-evolving-deep-neural-networks.md)
49. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
50. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
51. Data-free Parameter Pruning for Deep Neural Networks
52. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
53. [Designing Neural Network Architectures using Reinforcement Learning](2017-designing-neural-network-architectures-using-reinforcement-learning.md)
54. Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning
55. Net2Net - Accelerating Learning via Knowledge Transfer
56. [Xception - Deep Learning with Depthwise Separable Convolutions](2017-xception-deep-learning-with-depthwise-separable-convolutions.md)
57. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
58. More is Less - A More Complicated Network with Less Inference Complexity
59. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.md)
60. [Fast R-CNN](2015-fast-r-cnn.md)
61. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
62. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database.md)
63. Second Order Derivatives for Network Pruning - Optimal Brain Surgeon
64. Continuous control with deep reinforcement learning
65. [Human-level control through deep reinforcement learning](2015-human-level-control-through-deep-reinforcement-learning.md)
66. Evolving Neural Networks through Augmenting Topologies
67. Optimal Brain Damage
68. [Simple statistical gradient-following algorithms for connectionist reinforcement learning](2004-simple-statistical-gradient-following-algorithms-for-connectionist-reinforcement-learning.md)
69. Scalable parallel programming with CUDA
70. Learning from delayed rewards
71. The PASCAL visual object classes challenge 2006 (VOC2006) results
72. [The PASCAL Visual Object Classes Challenge](2006-the-pascal-visual-object-classes-challenge.md)
73. [Inverted Residuals and Linear Bottlenecks - Mobile Networks for Classification, Detection and Segmentation](2018-inverted-residuals-and-linear-bottlenecks-mobile-networks-for-classification-detection-and-segmentation.md)
74. Efficient methods and hardware for deep learning
