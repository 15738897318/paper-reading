---
title: SMASH - One-Shot Model Architecture Search through HyperNetworks
authors:
- Andrew Brock
- Theodore Lim
- J. Ritchie
- Nick Weston
fieldsOfStudy:
- Computer Science
meta_key: 2018-smash-one-shot-model-architecture-search-through-hypernetworks
numCitedBy: 525
reading_status: TBD
ref_count: 41
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/SMASH:-One-Shot-Model-Architecture-Search-through-Brock-Lim/e56b10f7cd4bf037beac84da5925dc4544fab974?sort=total-citations
venue: ICLR
year: 2018
---

[semanticscholar url](https://www.semanticscholar.org/paper/SMASH:-One-Shot-Model-Architecture-Search-through-Brock-Lim/e56b10f7cd4bf037beac84da5925dc4544fab974?sort=total-citations)

# SMASH - One-Shot Model Architecture Search through HyperNetworks

## Abstract

Designing architectures for deep neural networks requires expert knowledge and substantial computation time. We propose a technique to accelerate architecture selection by learning an auxiliary HyperNet that generates the weights of a main model conditioned on that model's architecture. By comparing the relative validation performance of networks with HyperNet-generated weights, we can effectively search over a wide range of architectures at the cost of a single training run. To facilitate this search, we develop a flexible mechanism based on memory read-writes that allows us to define a wide range of network connectivity patterns, with ResNet, DenseNet, and FractalNet blocks as special cases. We validate our method (SMASH) on CIFAR-10 and CIFAR-100, STL-10, ModelNet10, and Imagenet32x32, achieving competitive performance with similarly-sized hand-designed networks. Our code is available at this https URL

## Paper References

1. Swapout - Learning an ensemble of deep architectures
2. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
3. [HyperNetworks](2017-hypernetworks.md)
4. [Designing Neural Network Architectures using Reinforcement Learning](2017-designing-neural-network-architectures-using-reinforcement-learning.md)
5. [Wide Residual Networks](2016-wide-residual-networks.md)
6. Learning multiple visual domains with residual adapters
7. Dynamic Filter Networks
8. [An Analysis of Single-Layer Networks in Unsupervised Feature Learning](2011-an-analysis-of-single-layer-networks-in-unsupervised-feature-learning.md)
9. [Learning Transferable Architectures for Scalable Image Recognition](2018-learning-transferable-architectures-for-scalable-image-recognition.md)
10. Net2Net - Accelerating Learning via Knowledge Transfer
11. [Large-Scale Evolution of Image Classifiers](2017-large-scale-evolution-of-image-classifiers.md)
12. [Deep Networks with Stochastic Depth](2016-deep-networks-with-stochastic-depth.md)
13. [Convolutional Neural Fabrics](2016-convolutional-neural-fabrics.md)
14. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](2017-inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning.md)
15. Hyperband - Bandit-Based Configuration Evaluation for Hyperparameter Optimization
16. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
17. [Weight Normalization - A Simple Reparameterization to Accelerate Training of Deep Neural Networks](2016-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks.md)
18. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks.md)
19. [FractalNet - Ultra-Deep Neural Networks without Residuals](2017-fractalnet-ultra-deep-neural-networks-without-residuals.md)
20. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
21. [Training Very Deep Networks](2015-training-very-deep-networks.md)
22. A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks
23. [Predicting Parameters in Deep Learning](2013-predicting-parameters-in-deep-learning.md)
24. [How transferable are features in deep neural networks?](2014-how-transferable-are-features-in-deep-neural-networks.md)
25. Learning to Control Fast-weight Memories - an Alternative to Dynamic Recurrent Networks
26. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
27. On Random Weights and Unsupervised Feature Learning
28. Normalization Propagation - A Parametric Technique for Removing Internal Covariate Shift in Deep Networks
29. Generative and Discriminative Voxel Modeling with Convolutional Neural Networks
30. [SGDR - Stochastic Gradient Descent with Warm Restarts](2017-sgdr-stochastic-gradient-descent-with-warm-restarts.md)
31. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
32. [Practical Bayesian Optimization of Machine Learning Algorithms](2012-practical-bayesian-optimization-of-machine-learning-algorithms.md)
33. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
34. Shake-Shake regularization of 3-branch residual networks
35. [Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](2016-unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks.md)
36. A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets
37. [Layer Normalization](2016-layer-normalization.md)
38. Modeling systems with internal state using evolino
39. [Random Search for Hyper-Parameter Optimization](2012-random-search-for-hyper-parameter-optimization.md)
40. Neuroevolution - from architectures to learning
41. 3D ShapeNets - A deep representation for volumetric shapes
