---
title: DeepArchitect - Automatically Designing and Training Deep Architectures
authors:
- Renato Negrinho
- Geoffrey J. Gordon
fieldsOfStudy:
- Computer Science
meta_key: 2017-deeparchitect-automatically-designing-and-training-deep-architectures
numCitedBy: 153
reading_status: TBD
ref_count: 31
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/DeepArchitect:-Automatically-Designing-and-Training-Negrinho-Gordon/71a80e7342e56f33fd120246e907151a0cf1b4d0?sort=total-citations
venue: ArXiv
year: 2017
---

[semanticscholar url](https://www.semanticscholar.org/paper/DeepArchitect:-Automatically-Designing-and-Training-Negrinho-Gordon/71a80e7342e56f33fd120246e907151a0cf1b4d0?sort=total-citations)

# DeepArchitect - Automatically Designing and Training Deep Architectures

## Abstract

In deep learning, performance is strongly affected by the choice of architecture and hyperparameters. While there has been extensive work on automatic hyperpa- rameter optimization for simple spaces, complex spaces such as the space of deep architectures remain largely unexplored. As a result, the choice of architecture is done manually by the human expert through a slow trial and error process guided mainly by intuition. In this paper we describe a framework for automatically designing and training deep models. We propose an extensible and modular lan- guage that allows the human expert to compactly represent complex search spaces over architectures and their hyperparameters. The resulting search spaces are tree- structured and therefore easy to traverse. Models can be automatically compiled to computational graphs once values for all hyperparameters have been chosen. We can leverage the structure of the search space to introduce different model search algorithms, such as random search, Monte Carlo tree search (MCTS), and sequen- tial model-based optimization (SMBO). We present experiments comparing the different algorithms on CIFAR-10 and show that MCTS and SMBO outperform random search. We also present experiments on MNIST, showing that the same search space achieves near state-of-the-art performance with a few samples. These experiments show that our framework can be used effectively for model discov- ery, as it is possible to describe expressive search spaces and discover competitive models without much effort from the human expert. Code for our framework and experiments has been made publicly available

## Paper References

1. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
2. [Algorithms for Hyper-Parameter Optimization](2011-algorithms-for-hyper-parameter-optimization.md)
3. [Practical Bayesian Optimization of Machine Learning Algorithms](2012-practical-bayesian-optimization-of-machine-learning-algorithms.md)
4. [Designing Neural Network Architectures using Reinforcement Learning](2017-designing-neural-network-architectures-using-reinforcement-learning.md)
5. [Making a Science of Model Search - Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures](2013-making-a-science-of-model-search-hyperparameter-optimization-in-hundreds-of-dimensions-for-vision-architectures.md)
6. [Large-Scale Evolution of Image Classifiers](2017-large-scale-evolution-of-image-classifiers.md)
7. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
8. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks.md)
9. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
10. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
11. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
12. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
13. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
14. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
15. Selecting Near-Optimal Learners via Incremental Data Allocation
16. [TensorFlow - Large-Scale Machine Learning on Heterogeneous Distributed Systems](2016-tensorflow-large-scale-machine-learning-on-heterogeneous-distributed-systems.md)
17. [Mastering the game of Go with deep neural networks and tree search](2016-mastering-the-game-of-go-with-deep-neural-networks-and-tree-search.md)
18. [Sequential Model-Based Optimization for General Algorithm Configuration](2011-sequential-model-based-optimization-for-general-algorithm-configuration.md)
19. [Deep Visual-Semantic Alignments for Generating Image Descriptions](2017-deep-visual-semantic-alignments-for-generating-image-descriptions.md)
20. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
21. High Dimensional Bayesian Optimisation and Bandits via Additive Models
22. [Torch7 - A Matlab-like Environment for Machine Learning](2011-torch7-a-matlab-like-environment-for-machine-learning.md)
23. Playing Atari with Deep Reinforcement Learning
24. Evolving Neural Networks through Augmenting Topologies
25. Neuroevolution - from architectures to learning
26. Oracle inequalities for computationally budgeted model selection
27. A Survey of Monte Carlo Tree Search Methods
28. Modeling systems with internal state using evolino
29. A Framework for the Cooperation of Learning Algorithms
30. Bandit Based Monte-Carlo Planning
