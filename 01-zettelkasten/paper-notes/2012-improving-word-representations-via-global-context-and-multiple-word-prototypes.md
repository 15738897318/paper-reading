---
title: Improving Word Representations via Global Context and Multiple Word Prototypes
authors:
- E. Huang
- R. Socher
- Christopher D. Manning
- A. Ng
fieldsOfStudy:
- Computer Science
meta_key: 2012-improving-word-representations-via-global-context-and-multiple-word-prototypes
numCitedBy: 1185
reading_status: TBD
ref_count: 42
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Improving-Word-Representations-via-Global-Context-Huang-Socher/2b669398c4cf2ebe04375c8b1beae20f4ac802fa?sort=total-citations
venue: ACL
year: 2012
---

[semanticscholar url](https://www.semanticscholar.org/paper/Improving-Word-Representations-via-Global-Context-Huang-Socher/2b669398c4cf2ebe04375c8b1beae20f4ac802fa?sort=total-citations)

# Improving Word Representations via Global Context and Multiple Word Prototypes

## Abstract

Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems. However, most of these models are built with only local context and one representation per word. This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings. We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word. We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models.

## Paper References

1. Word Representations - A Simple and General Method for Semi-Supervised Learning
2. Multi-View Learning of Word Embeddings via CCA
3. Word Meaning in Context - A Simple and Effective Vector Model
4. A Structured Vector Space Model for Word Meaning in Context
5. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
6. [WordNet - A Lexical Database for English](1992-wordnet-a-lexical-database-for-english)
7. A Neural Probabilistic Language Model
8. Multi-Prototype Vector-Space Models of Word Meaning
9. Measuring Distributional Similarity in Context
10. Automatic Word Sense Discrimination
11. A Mixture Model with Sharing for Lexical Semantics
12. Using a connectionist model in a syntactical based language model
13. From distributional to semantic similarity
14. Dynamic and Static Prototype Vectors for Semantic Composition
15. Corpus-Based Approaches to Semantic Interpretation in Natural Language Processing
16. A Scalable Hierarchical Distributed Language Model
17. Connectionist language modeling for large vocabulary continuous speech recognition
18. [Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions](2011-semi-supervised-recursive-autoencoders-for-predicting-sentiment-distributions)
19. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
20. Three new graphical models for statistical language modelling
21. Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis
22. Cheap and Fast - But is it Good? Evaluating Non-Expert Annotations for Natural Language Tasks
23. The Acquisition of Word Meaning through Global Lexical Co-occurrences
24. SemEval-2010 Task 14 - Word Sense Induction &Disambiguation
25. [Parsing Natural Scenes and Natural Language with Recursive Neural Networks](2011-parsing-natural-scenes-and-natural-language-with-recursive-neural-networks)
26. Contextual correlates of semantic similarity
27. Vector-based Models of Semantic Composition
28. Contextual correlates of synonymy
29. Quantitative evaluation of passage retrieval algorithms for question answering
30. Placing search in context - the concept revisited
31. Concept Decompositions for Large Sparse Text Data Using Clustering
32. Unifying rational models of categorization via the hierarchical Dirichlet process
33. Machine learning in automated text categorization
34. Mixture models of categorization
35. [Introduction to Information Retrieval](2010-introduction-to-information-retrieval)
36. On the limited memory BFGS method for large scale optimization
