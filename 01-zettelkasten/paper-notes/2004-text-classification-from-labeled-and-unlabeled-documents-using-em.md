---
title: Text Classification from Labeled and Unlabeled Documents using EM
authors:
- K. Nigam
- A. McCallum
- S. Thrun
- Tom Michael Mitchell
fieldsOfStudy:
- Computer Science
meta_key: 2004-text-classification-from-labeled-and-unlabeled-documents-using-em
numCitedBy: 3089
reading_status: TBD
ref_count: 79
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Text-Classification-from-Labeled-and-Unlabeled-EM-Nigam-McCallum/e2de29049d62de925cf709024b92774cd82b0a5a?sort=total-citations
venue: Machine Learning
year: 2004
---

[semanticscholar url](https://www.semanticscholar.org/paper/Text-Classification-from-Labeled-and-Unlabeled-EM-Nigam-McCallum/e2de29049d62de925cf709024b92774cd82b0a5a?sort=total-citations)

# Text Classification from Labeled and Unlabeled Documents using EM

## Abstract

This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available.We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve classification accuracy under these conditions: (1) a weighting factor to modulate the contribution of the unlabeled data, and (2) the use of multiple mixture components per class. Experimental results, obtained using text from three different real-world tasks, show that the use of unlabeled data reduces classification error by up to 30%.

## Paper References

1. Employing EM and Pool-Based Active Learning for Text Classification
2. Employing Em in Pool-based Active Learning for Text Classiication
3. Combining labeled and unlabeled data with co-training
4. Expert network - effective and efficient learning from human decisions in text categorization and retrieval
5. Improving Text Classification by Shrinkage in a Hierarchy of Classes
6. Committee-Based Sampling For Training Probabilistic Classifiers
7. Active Learning with Committees for Text Categorization
8. A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data
9. A comparison of event models for naive bayes text classification
10. Context-sensitive learning methods for text categorization
11. An Evaluation of Statistical Approaches to Text Categorization
12. Hierarchically Classifying Documents Using Very Few Words
13. A sequential algorithm for training text classifiers
14. A comparison of two learning algorithms for text categorization
15. A sequential algorithm for training text classifiers - corrigendum and additional data
16. A Comparative Study on Feature Selection in Text Categorization
17. Combining Classiiers in Text Categorization
18. An evaluation of phrasal and clustered representations on a text categorization task
19. On the exponential value of labeled samples
20. Document Classification Using a Finite Mixture Model
21. Text Categorization with Support Vector Machines - Learning with Many Relevant Features
22. [On the Optimality of the Simple Bayesian Classifier under Zero-One Loss](2004-on-the-optimality-of-the-simple-bayesian-classifier-under-zero-one-loss.md)
23. Best-first Model Merging for Hidden Markov Model Induction
24. NewsWeeder - Learning to Filter Netnews
25. A New Metric-Based Approach to Model Selection
26. On Bias, Variance, 0/1-Loss, and the Curse-of-Dimensionality
27. The EM algorithm and extensions
28. Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms
29. A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization
30. A Bayesian Approach to Filtering Junk E-Mail
31. Naive (Bayes) at Forty - The Independence Assumption in Information Retrieval
32. [Machine learning](1996-machine-learning.md)
33. Learning to Extract Symbolic Knowledge from the World Wide Web
34. Learning in Graphical Models
35. The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon
36. Combining classifiers in text categorization
37. Relevance weighting of search terms
38. Supervised learning from incomplete data via an EM approach
39. Preventing Overfitting of Cross-Validation Data
40. Syskill & Webert - Identifying Interesting Web Sites
41. Intelligent Agents for Web-based Tasks - An Advice-Taking Approach
42. A UNIVERSAL PRIOR FOR INTEGERS AND ESTIMATION BY MINIMUM DESCRIPTION LENGTH
43. Bayesian Classification (AutoClass) - Theory and Results
44. Relevance feedback in information retrieval
45. Threading Electronic Mail - A Preliminary Study
46. Improving the Mean Field Approximation Via the Use of Mixture Distributions
47. Developments in Automatic Text Retrieval
48. [Elements of Information Theory](1991-elements-of-information-theory.md)
49. [Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper](1977-maximum-likelihood-from-incomplete-data-via-the-em-algorithm-plus-discussions-on-the-paper.md)
50. Data mining and knowledge discovery - making sense out of data
51. The SMART Retrieval System-Experiments in Automatic Document Processing
52. Improving the Mean Field Approximation via the Use of Mixture Distributions
