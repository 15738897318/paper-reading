---
title: Zero-Shot Learning Through Cross-Modal Transfer
authors:
- R. Socher
- M. Ganjoo
- Christopher D. Manning
- A. Ng
fieldsOfStudy:
- Computer Science
meta_key: 2013-zero-shot-learning-through-cross-modal-transfer
numCitedBy: 1179
reading_status: TBD
ref_count: 40
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Zero-Shot-Learning-Through-Cross-Modal-Transfer-Socher-Ganjoo/755e9f43ce398ae8737366720c5f82685b0c253e?sort=total-citations
venue: NIPS
year: 2013
---

# Zero-Shot Learning Through Cross-Modal Transfer

## Abstract

This work introduces a model that can recognize objects in images even if no training data is available for the object class. The only necessary knowledge about unseen visual categories comes from unsupervised text corpora. Unlike previous zero-shot learning models, which can only differentiate between unseen classes, our model can operate on a mixture of seen and unseen classes, simultaneously obtaining state of the art performance on classes with thousands of training images and reasonable performance on unseen classes. This is achieved by seeing the distributions of words in texts as a semantic space for understanding what objects look like. Our deep learning model does not require any manually defined semantic or visual features for either words or images. Images are mapped to be close to semantic word vectors corresponding to their classes, and the resulting image embeddings can be used to distinguish whether an image is of a seen or unseen class. We then use novelty detection methods to differentiate unseen classes from seen classes. We demonstrate two novelty detection strategies; the first gives high accuracy on unseen classes, while the second is conservative in its prediction of novelty and keeps the seen classes' accuracy high.

## Paper References

1. Learning to detect unseen object classes by between-class attribute transfer
2. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images)
3. One-shot learning of object categories
4. [Multimodal learning with deep Boltzmann machines](2012-multimodal-learning-with-deep-boltzmann-machines)
5. Connecting modalities - Semi-supervised segmentation and annotation of images using unaligned text corpora
6. One shot learning of simple visual concepts
7. [Multimodal Deep Learning](2011-multimodal-deep-learning)
8. Towards cross-category knowledge propagation for learning visual concepts
9. Object Classication from a Single Example Utilizing Class Relevance Pseudo-Metrics
10. Cross-generalization - learning novel classes from a single example by feature replacement
11. Object Classification from a Single Example Utilizing Class Relevance Metrics
12. The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization
13. Beyond spatial pyramids - Receptive field learning for pooled image features
14. Zero-data Learning of New Tasks
15. Describing objects by their attributes
16. Learning to Learn with Compound HD Models
17. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
18. A Neural Probabilistic Language Model
19. Domain Adaptation for Large-Scale Sentiment Classification - A Deep Learning Approach
20. Geometric context from a single image
21. Biographies, Bollywood, Boom-boxes and Blenders - Domain Adaptation for Sentiment Classification
22. Going Beyond Text - A Hybrid Image-Text Approach for Measuring Word Relatedness
23. [Visualizing Data using t-SNE](2008-visualizing-data-using-t-sne)
24. Distributional Memory - A General Framework for Corpus-Based Semantics
25. Visual Information in Semantic Representation
26. A Solution to Plato's Problem - The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.
27. From Frequency to Meaning - Vector Space Models of Semantics
28. Automatic Word Sense Discrimination
29. Dependency-Based Construction of Semantic Space Models
30. A Structured Vector Space Model for Word Meaning in Context
31. LoOP - local outlier probabilities
32. Automatic Retrieval and Clustering of Similar Words
33. Zero-shot Learning with Semantic Output Codes
34. From distributional to semantic similarity
35. [Improving Word Representations via Global Context and Multiple Word Prototypes](2012-improving-word-representations-via-global-context-and-multiple-word-prototypes)
36. Distributional Semantics in Technicolor
