---
title: An Empirical Exploration of Recurrent Network Architectures
authors:
- R. Józefowicz
- Wojciech Zaremba
- Ilya Sutskever
fieldsOfStudy:
- Computer Science
meta_key: 2015-an-empirical-exploration-of-recurrent-network-architectures
numCitedBy: 1395
reading_status: TBD
ref_count: 32
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/An-Empirical-Exploration-of-Recurrent-Network-Józefowicz-Zaremba/5b8364c21155d3d2cd38ea4c8b8580beba9a3250?sort=total-citations
venue: ICML
year: 2015
---

# An Empirical Exploration of Recurrent Network Architectures

## Abstract

The Recurrent Neural Network (RNN) is an extremely powerful sequence model that is often difficult to train. The Long Short-Term Memory (LSTM) is a specific RNN architecture whose design makes it much easier to train. While wildly successful in practice, the LSTM's architecture appears to be ad-hoc so it is not clear if it is optimal, and the significance of its individual components is unclear. 
 
In this work, we aim to determine whether the LSTM architecture is optimal or whether much better architectures exist. We conducted a thorough architecture search where we evaluated over ten thousand different RNN architectures, and identified an architecture that outperforms both the LSTM and the recently-introduced Gated Recurrent Unit (GRU) on some but not all tasks. We found that adding a bias of 1 to the LSTM's forget gate closes the gap between the LSTM and the GRU.

## Paper References

1. Learning to Execute
2. Learning to Forget - Continual Prediction with LSTM
3. Learning Longer Memory in Recurrent Neural Networks
4. [LSTM - A Search Space Odyssey](2017-lstm-a-search-space-odyssey.md)
5. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](2014-empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling.md)
6. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks.md)
7. Learning Recurrent Neural Networks with Hessian-Free Optimization
8. [On the importance of initialization and momentum in deep learning](2013-on-the-importance-of-initialization-and-momentum-in-deep-learning.md)
9. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization.md)
10. [Long Short-Term Memory](1997-long-short-term-memory.md)
11. Learning long-term dependencies with gradient descent is difficult
12. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model.md)
13. [On the difficulty of training recurrent neural networks](2013-on-the-difficulty-of-training-recurrent-neural-networks.md)
14. Evolving Memory Cell Structures for Sequence Learning
15. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
16. Statistical Language Models Based on Neural Networks
17. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks.md)
18. The''echo state''approach to analysing and training recurrent neural networks
19. Lesioning an attractor network - investigations of acquired dyslexia.
20. Modeling Temporal Dependencies in High-Dimensional Sequences - Application to Polyphonic Music Generation and Transcription
21. Deep learning via Hessian-free optimization
22. Semantic and Associative Priming in a Distributed Attractor Network
23. Harnessing Nonlinearity - Predicting Chaotic Systems and Saving Energy in Wireless Communication
24. [Torch7 - A Matlab-like Environment for Machine Learning](2011-torch7-a-matlab-like-environment-for-machine-learning.md)
25. Building a Large Annotated Corpus of English - The Penn Treebank
26. Modelling Brain Function - The World of Attractor Neural Networks
27. Untersuchungen zu dynamischen neuronalen Netzen
