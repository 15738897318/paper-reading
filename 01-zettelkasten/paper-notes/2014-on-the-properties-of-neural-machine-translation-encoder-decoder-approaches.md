---
title: On the Properties of Neural Machine Translation - Encoder-Decoder Approaches
authors:
- Kyunghyun Cho
- Bart van Merrienboer
- Dzmitry Bahdanau
- Yoshua Bengio
fieldsOfStudy:
- Computer Science
meta_key: 2014-on-the-properties-of-neural-machine-translation-encoder-decoder-approaches
numCitedBy: 4150
reading_status: TBD
ref_count: 15
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/On-the-Properties-of-Neural-Machine-Translation:-Cho-Merrienboer/1eb09fecd75eb27825dce4f964b97f4f5cc399d7?sort=total-citations
venue: SSST@EMNLP
year: 2014
---

[semanticscholar url](https://www.semanticscholar.org/paper/On-the-Properties-of-Neural-Machine-Translation:-Cho-Merrienboer/1eb09fecd75eb27825dce4f964b97f4f5cc399d7?sort=total-citations)

# On the Properties of Neural Machine Translation - Encoder-Decoder Approaches

## Abstract

Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.

## Paper References

1. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
2. [Recurrent Continuous Translation Models](2013-recurrent-continuous-translation-models)
3. Sequence Transduction with Recurrent Neural Networks
4. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
5. [Statistical Phrase-Based Translation](2003-statistical-phrase-based-translation)
6. Domain Adaptation via Pseudo In-Domain Data Selection
7. Better Evaluation Metrics Lead to Better Machine Translation
8. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
9. Audio Chord Recognition with Recurrent Neural Networks
10. BLEU Deconstructed - Designing a Better MT Evaluation Metric
11. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method)
