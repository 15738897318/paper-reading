---
title: Oscar - Object-Semantics Aligned Pre-training for Vision-Language Tasks
authors:
- Xiujun Li
- Xi Yin
- Chunyuan Li
- Xiaowei Hu
- Pengchuan Zhang
- Lei Zhang
- Lijuan Wang
- Houdong Hu
- Li Dong
- Furu Wei
- Yejin Choi
- Jianfeng Gao
fieldsOfStudy:
- Computer Science
meta_key: 2020-oscar-object-semantics-aligned-pre-training-for-vision-language-tasks
numCitedBy: 562
reading_status: TBD
ref_count: 52
tags:
- gen-from-ref
- other-default
- paper
venue: ECCV
year: 2020
---

# Oscar - Object-Semantics Aligned Pre-training for Vision-Language Tasks

## Abstract

Large-scale pre-training methods of learning cross-modal representations on image-text pairs are becoming popular for vision-language tasks. While existing methods simply concatenate image region features and text features as input to the model to be pre-trained and use self-attention to learn image-text semantic alignments in a brute force manner, in this paper, we propose a new learning method Oscar (Object-Semantics Aligned Pre-training), which uses object tags detected in images as anchor points to significantly ease the learning of alignments. Our method is motivated by the observation that the salient objects in an image can be accurately detected, and are often mentioned in the paired text. We pre-train an Oscar model on the public corpus of 6.5 million text-image pairs, and fine-tune it on downstream tasks, creating new state-of-the-arts on six well-established vision-language understanding and generation tasks.

## Paper References

1. [Unified Vision-Language Pre-Training for Image Captioning and VQA](2020-unified-vision-language-pre-training-for-image-captioning-and-vqa)
2. [UNITER - UNiversal Image-TExt Representation Learning](2020-uniter-universal-image-text-representation-learning)
3. ImageBERT - Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data
4. [Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training](2020-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training)
5. [UNITER - Learning UNiversal Image-TExt Representations](2019-uniter-learning-universal-image-text-representations)
6. [DeViSE - A Deep Visual-Semantic Embedding Model](2013-devise-a-deep-visual-semantic-embedding-model)
7. [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks)
8. [Stacked Cross Attention for Image-Text Matching](2018-stacked-cross-attention-for-image-text-matching)
9. Connecting modalities - Semi-supervised segmentation and annotation of images using unaligned text corpora
10. [The Open Images Dataset V4](2020-the-open-images-dataset-v4)
11. [Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](2014-unifying-visual-semantic-embeddings-with-multimodal-neural-language-models)
12. [LXMERT - Learning Cross-Modality Encoder Representations from Transformers](2019-lxmert-learning-cross-modality-encoder-representations-from-transformers)
13. Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training
14. [Dual-path Convolutional Image-Text Embeddings with Instance Loss](2020-dual-path-convolutional-image-text-embeddings-with-instance-loss)
15. [Image Captioning with Semantic Attention](2016-image-captioning-with-semantic-attention)
16. [CAMP - Cross-Modal Adaptive Message Passing for Text-Image Retrieval](2019-camp-cross-modal-adaptive-message-passing-for-text-image-retrieval)
17. Zero-Shot Learning by Convex Combination of Semantic Embeddings
18. [VideoBERT - A Joint Model for Video and Language Representation Learning](2019-videobert-a-joint-model-for-video-and-language-representation-learning)
19. [What Value Do Explicit High Level Concepts Have in Vision to Language Problems?](2016-what-value-do-explicit-high-level-concepts-have-in-vision-to-language-problems)
20. [The Open Images Dataset V4](2020-the-open-images-dataset-v4)
21. Joint Image-Text Representation by Gaussian Visual-Semantic Embedding
22. [VisualBERT - A Simple and Performant Baseline for Vision and Language](2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language)
23. [Zero-Shot Learning Through Cross-Modal Transfer](2013-zero-shot-learning-through-cross-modal-transfer)
24. Knowledge Aware Semantic Concept Expansion for Image-Text Matching
25. Guided Open Vocabulary Image Captioning with Constrained Beam Search
26. Position Focused Attention Network for Image-Text Matching
27. [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations)
28. [Multi-Task Deep Neural Networks for Natural Language Understanding](2019-multi-task-deep-neural-networks-for-natural-language-understanding)
29. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
30. [The Open Images Dataset V4](2020-the-open-images-dataset-v4)
31. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
32. [Attention is All you Need](2017-attention-is-all-you-need)
33. nocaps - novel object captioning at scale
34. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
35. [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning)
36. [VSE++ - Improving Visual-Semantic Embeddings with Hard Negatives](2018-vse-improving-visual-semantic-embeddings-with-hard-negatives)
37. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
38. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks)
39. [Attention on Attention for Image Captioning](2019-attention-on-attention-for-image-captioning)
40. [The Open Images Dataset V4](2020-the-open-images-dataset-v4)
41. [A Corpus for Reasoning about Natural Language Grounded in Photographs](2019-a-corpus-for-reasoning-about-natural-language-grounded-in-photographs)
42. [Learning by Abstraction - The Neural State Machine](2019-learning-by-abstraction-the-neural-state-machine)
43. [Self-Critical Sequence Training for Image Captioning](2017-self-critical-sequence-training-for-image-captioning)
44. Meta Module Network for Compositional Visual Reasoning
45. Im2Text - Describing Images Using 1 Million Captioned Photographs
46. Aligning Sentences in Parallel Corpora
47. [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](2014-from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions)
48. [Visualizing Data using t-SNE](2008-visualizing-data-using-t-sne)
49. [The Open Images Dataset V4](2020-the-open-images-dataset-v4)
