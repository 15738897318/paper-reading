---
title: DaDianNao - A Machine-Learning Supercomputer
authors:
- Yunji Chen
- Tao Luo
- Shaoli Liu
- Shijin Zhang
- Liqiang He
- Jia Wang
- Ling Li
- Tianshi Chen
- Zhiwei Xu
- Ninghui Sun
- O. Temam
fieldsOfStudy:
- Computer Science
meta_key: 2014-dadiannao-a-machine-learning-supercomputer
numCitedBy: 1065
reading_status: TBD
ref_count: 56
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/DaDianNao:-A-Machine-Learning-Supercomputer-Chen-Luo/4157ed3db4c656854e69931cb6089b64b08784b9?sort=total-citations
venue: 2014 47th Annual IEEE/ACM International Symposium on Microarchitecture
year: 2014
---

# DaDianNao - A Machine-Learning Supercomputer

## Abstract

Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.

## Paper References

1. [DianNao - a small-footprint high-throughput accelerator for ubiquitous machine-learning](2014-diannao-a-small-footprint-high-throughput-accelerator-for-ubiquitous-machine-learning.md)
2. Improving the speed of neural networks on CPUs
3. A Massively Parallel, Energy Efficient Programmable Accelerator for Learning and Classification
4. Deep learning with COTS HPC systems
5. A digital neurosynaptic core using embedded crossbar memory with 45pJ per spike in 45nm
6. A defect-tolerant accelerator for emerging high-performance applications
7. Understanding sources of inefficiency in general-purpose chips
8. SpiNNaker - Mapping neural networks onto a massively-parallel chip multiprocessor
9. [Large Scale Distributed Deep Networks](2012-large-scale-distributed-deep-networks.md)
10. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
11. Bridging the computation gap between programmable processors and hardwired accelerators
12. A case for neuromorphic ISAs
13. Continuous real-world inputs can open up alternative accelerator designs
14. [Flexible, High Performance Convolutional Neural Networks for Image Classification](2011-flexible-high-performance-convolutional-neural-networks-for-image-classification.md)
15. Convolution engine - balancing efficiency & flexibility in specialized computing
16. [Multi-column deep neural networks for image classification](2012-multi-column-deep-neural-networks-for-image-classification.md)
17. [Building high-level features using large scale unsupervised learning](2013-building-high-level-features-using-large-scale-unsupervised-learning.md)
18. NeuFlow - A runtime reconfigurable dataflow processor for vision
19. Logic-based eDRAM - Origins and rationale for use
20. [GradientBased Learning Applied to Document Recognition](2001-gradientbased-learning-applied-to-document-recognition.md)
21. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition.md)
22. Low-Cost Binary128 Floating-Point FMA Unit Design with SIMD Support
23. Automatic abstraction and fault tolerance in cortical microachitectures
24. Wafer-scale integration of analog neural networks
25. Neural Acceleration for General-Purpose Approximate Programs
26. An Efficient Learning Procedure for Deep Boltzmann Machines
27. The PARSEC benchmark suite - Characterization and architectural implications
28. An empirical evaluation of deep architectures on problems with many factors of variation
29. [Improving deep neural networks for LVCSR using rectified linear units and dropout](2013-improving-deep-neural-networks-for-lvcsr-using-rectified-linear-units-and-dropout.md)
30. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database.md)
31. Scaling deep trench based eDRAM on SOI to 32nm and Beyond
32. Introduction to This is Watson
33. What is the best multi-stage architecture for object recognition?
34. Dark Silicon and the End of Multicore Scaling
35. QSCORES - Trading dark silicon for scalable energy efficiency with quasi-specific cores
36. A high-performance, high-density 28nm eDRAM technology with high-K/metal-gate
37. ORION 2.0 - A Power-Area Simulator for Interconnection Networks
38. Learning long-range vision for autonomous off-road driving
39. Learning deep structured semantic models for web search using clickthrough data
40. Learning long‐range vision for autonomous off‐road driving
41. Learning to Label Aerial Images from Noisy Data
42. The IBM Blue Gene/Q Interconnection Fabric
43. Compute-and-Forward Strategies for Cooperative Distributed Antenna Systems
44. Principles and Practices of Interconnection Networks
45. A 0.41µA standby leakage 32Kb embedded SRAM with Low-Voltage resume-standby utilizing all digital current comparator in 28nm HKMG CMOS
46. Anton - A specialized ASIC for molecular dynamics
47. Recognition, Mining and Synthesis Moves Comp uters to the Era of Tera
