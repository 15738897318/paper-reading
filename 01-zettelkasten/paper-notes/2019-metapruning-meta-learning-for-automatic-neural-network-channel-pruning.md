---
title: MetaPruning - Meta Learning for Automatic Neural Network Channel Pruning
authors:
- Zechun Liu
- Haoyuan Mu
- X. Zhang
- Zichao Guo
- Xin Yang
- K. Cheng
- Jian Sun
fieldsOfStudy:
- Computer Science
meta_key: 2019-metapruning-meta-learning-for-automatic-neural-network-channel-pruning
numCitedBy: 258
reading_status: TBD
ref_count: 69
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/MetaPruning:-Meta-Learning-for-Automatic-Neural-Liu-Mu/bd3df472bc848083068a76e9ce2b2ab49543dc78?sort=total-citations
venue: 2019 IEEE/CVF International Conference on Computer Vision (ICCV)
year: 2019
---

# MetaPruning - Meta Learning for Automatic Neural Network Channel Pruning

## Abstract

In this paper, we propose a novel meta learning approach for automatic channel pruning of very deep neural networks. We first train a PruningNet, a kind of meta network, which is able to generate weight parameters for any pruned structure given the target network. We use a simple stochastic structure sampling method for training the PruningNet. Then, we apply an evolutionary procedure to search for good-performing pruned networks. The search is highly efficient because the weights are directly generated by the trained PruningNet and we do not need any finetuning at search time. With a single PruningNet trained for the target network, we can search for various Pruned Networks under different constraints with little human participation. Compared to the state-of-the-art pruning methods, we have demonstrated superior performances on MobileNet V1/V2 and ResNet. Codes are available on https://github.com/liuzechun/MetaPruning.

## Paper References

1. Pruning Convolutional Neural Networks for Resource Efficient Inference
2. Rethinking the Value of Network Pruning
3. Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning
4. [Efficient Neural Architecture Search via Parameter Sharing](2018-efficient-neural-architecture-search-via-parameter-sharing.md)
5. [SMASH - One-Shot Model Architecture Search through HyperNetworks](2018-smash-one-shot-model-architecture-search-through-hypernetworks.md)
6. Network Trimming - A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures
7. Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks
8. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
9. [Channel Pruning for Accelerating Very Deep Neural Networks](2017-channel-pruning-for-accelerating-very-deep-neural-networks.md)
10. [Learning both Weights and Connections for Efficient Neural Network](2015-learning-both-weights-and-connections-for-efficient-neural-network.md)
11. [Designing Neural Network Architectures using Reinforcement Learning](2017-designing-neural-network-architectures-using-reinforcement-learning.md)
12. Data-Driven Sparse Structure Selection for Deep Neural Networks
13. Structured Pruning of Deep Convolutional Neural Networks
14. Diversity Networks
15. Optimization as a Model for Few-Shot Learning
16. ThiNet - A Filter Level Pruning Method for Deep Neural Network Compression
17. Compact Deep Convolutional Neural Networks With Coarse Pruning
18. Learning the Number of Neurons in Deep Networks
19. [ProxylessNAS - Direct Neural Architecture Search on Target Task and Hardware](2019-proxylessnas-direct-neural-architecture-search-on-target-task-and-hardware.md)
20. Dynamic Network Surgery for Efficient DNNs
21. [Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding](2016-deep-compression-compressing-deep-neural-network-with-pruning-trained-quantization-and-huffman-coding.md)
22. Slimmable Neural Networks
23. [Genetic CNN](2017-genetic-cnn.md)
24. [HyperNetworks](2017-hypernetworks.md)
25. [Learning Efficient Convolutional Networks through Network Slimming](2017-learning-efficient-convolutional-networks-through-network-slimming.md)
26. [Understanding and Simplifying One-Shot Architecture Search](2018-understanding-and-simplifying-one-shot-architecture-search.md)
27. ChamNet - Towards Efficient Network Design Through Platform-Aware Model Adaptation
28. LQ-Nets - Learned Quantization for Highly Accurate and Compact Deep Neural Networks
29. [Pruning Filters for Efficient ConvNets](2017-pruning-filters-for-efficient-convnets.md)
30. [DARTS - Differentiable Architecture Search](2019-darts-differentiable-architecture-search.md)
31. DPP-Net - Device-aware Progressive Search for Pareto-optimal Neural Architectures
32. Optimal Brain Surgeon and general network pruning
33. Sparse Convolutional Neural Networks
34. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
35. [ShuffleNet V2 - Practical Guidelines for Efficient CNN Architecture Design](2018-shufflenet-v2-practical-guidelines-for-efficient-cnn-architecture-design.md)
36. [FBNet - Hardware-Aware Efficient ConvNet Design via Differentiable Neural Architecture Search](2019-fbnet-hardware-aware-efficient-convnet-design-via-differentiable-neural-architecture-search.md)
37. Learning to Learn - Model Regression Networks for Easy Small Sample Learning
38. Efficient Processing of Deep Neural Networks - A Tutorial and Survey
39. [NetAdapt - Platform-Aware Neural Network Adaptation for Mobile Applications](2018-netadapt-platform-aware-neural-network-adaptation-for-mobile-applications.md)
40. Loss-aware Weight Quantization of Deep Networks
41. [Large-Scale Evolution of Image Classifiers](2017-large-scale-evolution-of-image-classifiers.md)
42. [XNOR-Net - ImageNet Classification Using Binary Convolutional Neural Networks](2016-xnor-net-imagenet-classification-using-binary-convolutional-neural-networks.md)
43. Structured Binary Neural Networks for Accurate Image Classification and Semantic Segmentation
44. [Predicting Parameters in Deep Learning](2013-predicting-parameters-in-deep-learning.md)
45. Constraint-Aware Deep Neural Network Compression
46. Explicit Loss-Error-Aware Quantization for Low-Bit Deep Neural Networks
47. [SqueezeNet - AlexNet-level accuracy with 50x fewer parameters and <1MB model size](2016-squeezenet-alexnet-level-accuracy-with-50x-fewer-parameters-and-1mb-model-size.md)
48. Optimal Brain Damage
49. [AMC - AutoML for Model Compression and Acceleration on Mobile Devices](2018-amc-automl-for-model-compression-and-acceleration-on-mobile-devices.md)
50. [ShuffleNet - An Extremely Efficient Convolutional Neural Network for Mobile Devices](2018-shufflenet-an-extremely-efficient-convolutional-neural-network-for-mobile-devices.md)
51. [MobileNetV2 - Inverted Residuals and Linear Bottlenecks](2018-mobilenetv2-inverted-residuals-and-linear-bottlenecks.md)
52. Meta-SR - A Magnification-Arbitrary Network for Super-Resolution
53. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
54. [MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications](2017-mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications.md)
55. Bi-Real Net - Enhancing the Performance of 1-bit CNNs With Improved Representational Capability and Advanced Training Algorithm
56. Metalearning - a survey of trends and technologies
57. MetaAnchor - Learning to Detect Objects with Customized Anchors
58. Write a Classifier - Zero-Shot Learning Using Purely Textual Descriptions
59. Learning to Segment Every Thing
60. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database.md)
61. Et al
