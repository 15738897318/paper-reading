---
title: Training Products of Experts by Minimizing Contrastive Divergence
authors:
- Geoffrey E. Hinton
fieldsOfStudy:
- Computer Science
meta_key: 2002-training-products-of-experts-by-minimizing-contrastive-divergence
numCitedBy: 4582
reading_status: TBD
ref_count: 34
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Training-Products-of-Experts-by-Minimizing-Hinton/9360e5ce9c98166bb179ad479a9d2919ff13d022?sort=total-citations
venue: Neural Computation
year: 2002
---

[semanticscholar url](https://www.semanticscholar.org/paper/Training-Products-of-Experts-by-Minimizing-Hinton/9360e5ce9c98166bb179ad479a9d2919ff13d022?sort=total-citations)

# Training Products of Experts by Minimizing Contrastive Divergence

## Abstract

It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual expert models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called contrastive divergence whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.

## Paper References

1. Recognizing Hand-written Digits Using Hierarchical Products of Experts
2. Connectionist Learning of Belief Networks
3. Rate-coded Restricted Boltzmann Machines for Face Recognition
4. Unsupervised Learning of Distributions of Binary Vectors Using 2-Layer Networks
5. Using Generative Models for Handwritten Digit Recognition
6. A Gradient-Based Boosting Algorithm for Regression Problems
7. Products of Hidden Markov Models
8. MAXIMUM ENTROPY
9. Unsupervised learning of distributions
10. Combining Probability Distributions - A Critique and an Annotated Bibliography
11. Learning Representations by Recirculation
12. Learning Continuous Attractors in Recurrent Networks
13. Bias/Variance Decompositions for Likelihood-Based Estimators
14. Information processing in dynamical systems - foundations of harmony theory
15. Attractor Dynamics in Feedforward Neural Networks
16. A Maximum Entropy Approach to Natural Language Processing
17. Mean Field Theory for Sigmoid Belief Networks
18. Learning and relearning in Boltzmann machines
19. The wake-sleep algorithm for unsupervised neural networks.
20. Learning Structural Descriptions From Examples
21. The psychology of computer vision
22. Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images
23. Biologically Plausible Error-Driven Learning Using Local Activation Differences - The Generalized Recirculation Algorithm
