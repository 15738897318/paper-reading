---
title: Curriculum learning
authors:
- Yoshua Bengio
- J. Louradour
- Ronan Collobert
- J. Weston
fieldsOfStudy:
- Education
meta_key: 2009-curriculum-learning
numCitedBy: 3220
reading_status: TBD
ref_count: 42
tags:
- gen-from-ref
- other-default
- paper
venue: ICML '09
year: 2009
---

# Curriculum learning

## Abstract

Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).

## Paper References

1. Flexible shaping - How learning in small steps helps
2. The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training
3. [Greedy Layer-Wise Training of Deep Networks](2006-greedy-layer-wise-training-of-deep-networks)
4. [Learning Deep Architectures for AI](2007-learning-deep-architectures-for-ai)
5. Learning and development in neural networks - the importance of starting small
6. Explanation-based neural network learning a lifelong learning approach
7. Language acquisition in the absence of explicit negative evidence - how important is starting small?
8. Active Learning with Statistical Models
9. Sparse Feature Learning for Deep Belief Networks
10. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
11. [Extracting and composing robust features with denoising autoencoders](2008-extracting-and-composing-robust-features-with-denoising-autoencoders)
12. An empirical evaluation of deep architectures on problems with many factors of variation
13. A Neural Probabilistic Language Model
14. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
15. Efficient Learning of Sparse Representations with an Energy-Based Model
16. Restricted Boltzmann machines for collaborative filtering
17. Neural network learning control of robot manipulators using gradually increasing task difficulty
18. Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes
19. Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure
20. Unsupervised Learning of Distributions of Binary Vectors Using 2-Layer Networks
21. [Reducing the Dimensionality of Data with Neural Networks](2006-reducing-the-dimensionality-of-data-with-neural-networks)
22. Deep learning via semi-supervised embedding
23. Connectionist language modeling for large vocabulary continuous speech recognition
24. A day of great illumination - B. F. Skinner's discovery of shaping.
25. Parallel continuation-based global optimization for molecular conformation and protein folding
26. Global continuation for distance geometry problems
27. Numerical continuation methods - an introduction
28. Smoothing techniques for macromolecular global optimization
29. On the power of small-depth threshold circuits
30. Numerical Continuation Methods
31. Generalization in the programed teaching of a perceptron.
