---
title: Cross-lingual Language Model Pretraining
authors:
- Guillaume Lample
- A. Conneau
fieldsOfStudy:
- Computer Science
meta_key: 2019-cross-lingual-language-model-pretraining
numCitedBy: 1539
reading_status: TBD
ref_count: 52
tags:
- gen-from-ref
- other-default
- paper
venue: NeurIPS
year: 2019
---

# Cross-lingual Language Model Pretraining

## Abstract

Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT’16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.

## Paper References

1. XNLI - Evaluating Cross-lingual Sentence Representations
2. Phrase-Based & Neural Unsupervised Machine Translation
3. Word Translation Without Parallel Data
4. Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond
5. Unsupervised Neural Machine Translation
6. Unsupervised Cross-lingual Word Embedding by Multilingual Neural Language Models
7. Unsupervised Machine Translation Using Monolingual Corpora Only
8. Unsupervised Pretraining for Sequence to Sequence Learning
9. Google's Multilingual Neural Machine Translation System - Enabling Zero-Shot Translation
10. Zero-Shot Cross-lingual Classification Using Multilingual Neural Machine Translation
11. SemEval-2017 Task 2 - Multilingual and Cross-lingual Semantic Word Similarity
12. [Neural Machine Translation of Rare Words with Subword Units](2016-neural-machine-translation-of-rare-words-with-subword-units)
13. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
14. Offline bilingual word vectors, orthogonal transformations and the inverted softmax
15. [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](2018-a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference)
16. [Universal Language Model Fine-tuning for Text Classification](2018-universal-language-model-fine-tuning-for-text-classification)
17. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
18. [Exploring the Limits of Language Modeling](2016-exploring-the-limits-of-language-modeling)
19. Edinburgh Neural Machine Translation Systems for WMT 16
20. [Attention is All you Need](2017-attention-is-all-you-need.md)
21. Exploiting Similarities among Languages for Machine Translation
22. Multilingual Models for Compositional Distributed Semantics
23. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
24. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference)
25. [Enriching Word Vectors with Subword Information](2017-enriching-word-vectors-with-subword-information)
26. The IIT Bombay English-Hindi Parallel Corpus
27. Massively Multilingual Word Embeddings
28. The United Nations Parallel Corpus v1.0
29. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model)
30. Optimizing Chinese Word Segmentation for Machine Translation Performance
31. [Character-Level Language Modeling with Deeper Self-Attention](2019-character-level-language-modeling-with-deeper-self-attention)
32. Improving Vector Space Word Representations Using Multilingual Correlation
33. SentEval - An Evaluation Toolkit for Universal Sentence Representations
34. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
35. [Moses - Open Source Toolkit for Statistical Machine Translation](2007-moses-open-source-toolkit-for-statistical-machine-translation)
36. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
37. Normalized Word Embedding and Orthogonal Transform for Bilingual Word Translation
38. [Long Short-Term Memory](1997-long-short-term-memory)
39. Parallel Data, Tools and Interfaces in OPUS
40. [Automatic differentiation in PyTorch](2017-automatic-differentiation-in-pytorch)
41. Backpropagation Through Time - What It Does and How to Do It
42. [Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units](2016-bridging-nonlinearities-and-stochastic-regularizers-with-gaussian-error-linear-units)
43. “Cloze Procedure” - A New Tool for Measuring Readability
44. Transformer-XL - Language Modeling with Longer-Term Dependency
45. Open Source Toolkit for Statistical Machine Translation - Factored Translation Models and Lattice Decoding
46. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
