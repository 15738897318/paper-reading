---
title: Bilinear Attention Networks
authors:
- Jin-Hwa Kim
- Jaehyun Jun
- Byoung-Tak Zhang
fieldsOfStudy:
- Computer Science
meta_key: 2018-bilinear-attention-networks
numCitedBy: 418
reading_status: TBD
ref_count: 52
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Bilinear-Attention-Networks-Kim-Jun/a5d10341717c0519cf63151b496a6d2ed67aa05f?sort=total-citations
venue: NeurIPS
year: 2018
---

# Bilinear Attention Networks

## Abstract

Attention networks in multimodal learning provide an efficient way to utilize given visual information selectively. However, the computational cost to learn attention distributions for every pair of multimodal input channels is prohibitively expensive. To solve this problem, co-attention builds two separate attention distributions for each modality neglecting the interaction between multimodal inputs. In this paper, we propose bilinear attention networks (BAN) that find bilinear attention distributions to utilize given vision-language information seamlessly. BAN considers bilinear interactions among two groups of input channels, while low-rank bilinear pooling extracts the joint representations for each pair of channels. Furthermore, we propose a variant of multimodal residual networks to exploit eight-attention maps of the BAN efficiently. We quantitatively and qualitatively evaluate our model on visual question answering (VQA 2.0) and Flickr30k Entities datasets, showing that BAN significantly outperforms previous methods and achieves new state-of-the-arts on both datasets.

## Paper References

1. [Dual Attention Networks for Multimodal Reasoning and Matching](2017-dual-attention-networks-for-multimodal-reasoning-and-matching)
2. Multimodal Residual Learning for Visual QA
3. Beyond Bilinear - Generalized Multimodal Factorized High-Order Pooling for Visual Question Answering
4. [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](2016-multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding)
5. [Hierarchical Question-Image Co-Attention for Visual Question Answering](2016-hierarchical-question-image-co-attention-for-visual-question-answering)
6. [Ask, Attend and Answer - Exploring Question-Guided Spatial Attention for Visual Question Answering](2016-ask-attend-and-answer-exploring-question-guided-spatial-attention-for-visual-question-answering)
7. Top-Down Neural Attention by Excitation Backprop
8. A Simple Loss Function for Improving the Convergence and Accuracy of Visual Question Answering Models
9. Learning to Count Objects in Natural Images for Visual Question Answering
10. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
11. [Spatial Transformer Networks](2015-spatial-transformer-networks)
12. [Grounding of Textual Phrases in Images by Reconstruction](2016-grounding-of-textual-phrases-in-images-by-reconstruction)
13. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
14. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks)
15. Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts
16. [Deep Residual Learning for Image Recognition](2015-resnet.md)
17. The VQA-Machine - Learning How to Use Existing Vision Algorithms to Answer New Questions
18. Tips and Tricks for Visual Question Answering - Learnings from the 2017 Challenge
19. [Learning Deep Structure-Preserving Image-Text Embeddings](2016-learning-deep-structure-preserving-image-text-embeddings)
20. [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations)
21. Bilinear classifiers for visual recognition
22. Residual Networks are Exponential Ensembles of Relatively Shallow Networks
23. Interpretable Counting for Visual Question Answering
24. [Natural Language Object Retrieval](2016-natural-language-object-retrieval)
25. Modeling Appearances with Low-Rank SVM
26. Query-Adaptive R-CNN for Open-Vocabulary Object Detection and Retrieval
27. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
28. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
29. [Weight Normalization - A Simple Reparameterization to Accelerate Training of Deep Neural Networks](2016-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks)
30. [Fast R-CNN](2015-fast-r-cnn)
31. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting)
32. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
33. [Multiscale Combinatorial Grouping](2014-multiscale-combinatorial-grouping)
34. [YOLO9000 - Better, Faster, Stronger](2017-yolo9000-better-faster-stronger)
35. Structured Matching for Phrase Localization
36. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
37. [Edge Boxes - Locating Object Proposals from Edges](2014-edge-boxes-locating-object-proposals-from-edges)
38. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines)
39. [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](2014-from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions)
40. MUTAN - Multimodal Tucker Fusion for Visual Question Answering
