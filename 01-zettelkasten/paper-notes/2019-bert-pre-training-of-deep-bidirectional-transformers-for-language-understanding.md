---
title: BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
authors:
- Jacob Devlin
- Ming-Wei Chang
- Kenton Lee
- Kristina Toutanova
fieldsOfStudy:
- Computer Science
meta_key: 2019-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding
numCitedBy: 34640
reading_status: TBD
ref_count: 60
tags:
- gen-from-ref
- paper
venue: NAACL
year: 2019
---

# BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding

## Abstract

We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).

## Paper References

1. [Attention is All you Need](2017-attention-is-all-you-need)
2. Dissecting Contextual Word Embeddings - Architecture and Representation
3. Semi-Supervised Sequence Modeling with Cross-View Training
4. Semi-supervised sequence tagging with bidirectional language models
5. [Character-Level Language Modeling with Deeper Self-Attention](2019-character-level-language-modeling-with-deeper-self-attention)
6. QANet - Combining Local Convolution with Global Self-Attention for Reading Comprehension
7. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
8. [Skip-Thought Vectors](2015-skip-thought-vectors)
9. MaskGAN - Better Text Generation via Filling in the ______
10. Contextual String Embeddings for Sequence Labeling
11. Multi-Granularity Hierarchical Attention Fusion Networks for Reading Comprehension and Question Answering
12. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
13. U-Net - Machine Reading Comprehension with Unanswerable Questions
14. [Bidirectional Attention Flow for Machine Comprehension](2017-bidirectional-attention-flow-for-machine-comprehension)
15. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
16. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
17. [Universal Language Model Fine-tuning for Text Classification](2018-universal-language-model-fine-tuning-for-text-classification)
18. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation)
19. One billion word benchmark for measuring progress in statistical language modeling
20. A Decomposable Attention Model for Natural Language Inference
21. [TriviaQA - A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension](2017-triviaqa-a-large-scale-distantly-supervised-challenge-dataset-for-reading-comprehension)
22. [Learned in Translation - Contextualized Word Vectors](2017-learned-in-translation-contextualized-word-vectors)
23. [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](2017-supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data)
24. [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](2017-semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation)
25. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference)
26. Semi-supervised Sequence Learning
27. context2vec - Learning Generic Context Embedding with Bidirectional LSTM
28. SWAG - A Large-Scale Adversarial Dataset for Grounded Commonsense Inference
29. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
30. [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](2018-a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference)
31. Simple and Effective Multi-Paragraph Reading Comprehension
32. Word Representations - A Simple and General Method for Semi-Supervised Learning
33. [Learning Distributed Representations of Sentences from Unlabelled Data](2016-learning-distributed-representations-of-sentences-from-unlabelled-data)
34. An efficient framework for learning sentence representations
35. Domain Adaptation with Structural Correspondence Learning
36. A Scalable Hierarchical Distributed Language Model
37. Reinforced Mnemonic Reader for Machine Reading Comprehension
38. [Distributed Representations of Sentences and Documents](2014-distributed-representations-of-sentences-and-documents)
39. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
40. How transferable are features in deep neural networks?
41. The Sixth PASCAL Recognizing Textual Entailment Challenge
42. A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data
43. [Neural Network Acceptability Judgments](2019-neural-network-acceptability-judgments)
44. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
45. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
46. [Extracting and composing robust features with denoising autoencoders](2008-extracting-and-composing-robust-features-with-denoising-autoencoders)
47. The Seventh PASCAL Recognizing Textual Entailment Challenge
48. Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning
49. Automatically Constructing a Corpus of Sentential Paraphrases
50. The PASCAL Recognising Textual Entailment Challenge
51. The Winograd Schema Challenge
52. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database)
53. Class-Based n-gram Models of Natural Language
54. [Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units](2016-bridging-nonlinearities-and-stochastic-regularizers-with-gaussian-error-linear-units)
55. Introduction to the CoNLL-2003 Shared Task - Language-Independent Named Entity Recognition
56. “Cloze Procedure” - A New Tool for Measuring Readability
