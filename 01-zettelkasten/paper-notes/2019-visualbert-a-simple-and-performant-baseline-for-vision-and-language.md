---
title: VisualBERT - A Simple and Performant Baseline for Vision and Language
authors:
- Liunian Harold Li
- Mark Yatskar
- Da Yin
- Cho-Jui Hsieh
- Kai-Wei Chang
fieldsOfStudy:
- Computer Science
meta_key: 2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language
numCitedBy: 664
reading_status: TBD
ref_count: 40
tags:
- gen-from-ref
- other-default
- paper
venue: ArXiv
year: 2019
---

# VisualBERT - A Simple and Performant Baseline for Vision and Language

## Abstract

We propose VisualBERT, a simple and flexible framework for modeling a broad range of vision-and-language tasks. VisualBERT consists of a stack of Transformer layers that implicitly align elements of an input text and regions in an associated input image with self-attention. We further propose two visually-grounded language model objectives for pre-training VisualBERT on image caption data. Experiments on four vision-and-language tasks including VQA, VCR, NLVR2, and Flickr30K show that VisualBERT outperforms or rivals with state-of-the-art models while being significantly simpler. Further analysis demonstrates that VisualBERT can ground elements of language to image regions without any explicit supervision and is even sensitive to syntactic relationships, tracking, for example, associations between verbs and image regions corresponding to their arguments.

## Paper References

1. [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks)
2. [Deep Visual-Semantic Alignments for Generating Image Descriptions](2017-deep-visual-semantic-alignments-for-generating-image-descriptions)
3. MUREL - Multimodal Relational Reasoning for Visual Question Answering
4. Learning Conditioned Graph Structures for Interpretable Visual Question Answering
5. [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations)
6. [VideoBERT - A Joint Model for Video and Language Representation Learning](2019-videobert-a-joint-model-for-video-and-language-representation-learning)
7. [Deep Modular Co-Attention Networks for Visual Question Answering](2019-deep-modular-co-attention-networks-for-visual-question-answering)
8. [From Recognition to Cognition - Visual Commonsense Reasoning](2019-from-recognition-to-cognition-visual-commonsense-reasoning)
9. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
10. Relation-Aware Graph Attention Network for Visual Question Answering
11. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
12. Multimodal Transformer With Multi-View Visual Representation for Image Captioning
13. [A Corpus for Reasoning about Natural Language Grounded in Photographs](2019-a-corpus-for-reasoning-about-natural-language-grounded-in-photographs)
14. Flickr30k Entities - Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models
15. [Language Models are Unsupervised Multitask Learners](2019-language-models-are-unsupervised-multitask-learners)
16. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
17. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
18. [Attention is All you Need](2017-transformer.md)
19. Towards VQA Models That Can Read
20. [Bilinear Attention Networks](2018-bilinear-attention-networks)
21. What Does BERT Look at? An Analysis of BERT's Attention
22. [Stacked Attention Networks for Image Question Answering](2016-stacked-attention-networks-for-image-question-answering)
23. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
24. [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning)
25. [A simple neural network module for relational reasoning](2017-a-simple-neural-network-module-for-relational-reasoning)
26. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
27. [Deep Residual Learning for Image Recognition](2015-resnet.md)
28. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge)
29. Analyzing Multi-Head Self-Attention - Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned
30. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks)
31. AllenNLP - A Deep Semantic Natural Language Processing Platform
32. Image retrieval using scene graphs
33. Deep Biaffine Attention for Neural Dependency Parsing
34. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
35. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation)
36. Pythia v0.1 - the Winning Entry to the VQA Challenge 2018
37. Stanford typed dependencies manual
38. [Microsoft COCO Captions - Data Collection and Evaluation Server](2015-microsoft-coco-captions-data-collection-and-evaluation-server)
39. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
40. ReferItGame - Referring to Objects in Photographs of Natural Scenes
