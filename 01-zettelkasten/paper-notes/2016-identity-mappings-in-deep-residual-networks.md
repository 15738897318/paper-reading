---
title: Identity Mappings in Deep Residual Networks
authors:
- Kaiming He
- X. Zhang
- Shaoqing Ren
- Jian Sun
fieldsOfStudy:
- Computer Science
meta_key: 2016-identity-mappings-in-deep-residual-networks
numCitedBy: 6499
reading_status: TBD
ref_count: 33
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Identity-Mappings-in-Deep-Residual-Networks-He-Zhang/77f0a39b8e02686fd85b01971f8feb7f60971f80?sort=total-citations
venue: ECCV
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/Identity-Mappings-in-Deep-Residual-Networks-He-Zhang/77f0a39b8e02686fd85b01971f8feb7f60971f80?sort=total-citations)

# Identity Mappings in Deep Residual Networks

## Abstract

Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers.

## Paper References

1. Learning Strict Identity Mappings in Deep Residual Networks
2. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](2017-inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning)
3. [Network In Network](2014-network-in-network)
4. Highway Networks
5. [Training Very Deep Networks](2015-training-very-deep-networks)
6. FitNets - Hints for Thin Deep Nets
7. [Going deeper with convolutions](2015-going-deeper-with-convolutions)
8. [Deeply-Supervised Nets](2015-deeply-supervised-nets)
9. [Deep Residual Learning for Image Recognition](2015-resnet.md)
10. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2014-vggnet.md)
11. All you need is a good init
12. [Rethinking the Inception Architecture for Computer Vision](2016-rethinking-the-inception-architecture-for-computer-vision)
13. [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](2016-fast-and-accurate-deep-network-learning-by-exponential-linear-units-elus)
14. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift)
15. On the complexity of shallow and deep neural network classifiers
16. [Striving for Simplicity - The All Convolutional Net](2015-striving-for-simplicity-the-all-convolutional-net)
17. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images)
18. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification)
19. Fractional Max-Pooling
20. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors)
21. [Long Short-Term Memory](1997-long-short-term-memory)
22. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines)
23. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
24. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge)
25. Backpropagation Applied to Handwritten Zip Code Recognition
