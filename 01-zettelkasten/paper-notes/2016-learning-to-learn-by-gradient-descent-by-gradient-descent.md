---
title: Learning to learn by gradient descent by gradient descent
authors:
- Marcin Andrychowicz
- Misha Denil
- Sergio Gomez Colmenarejo
- Matthew W. Hoffman
- D. Pfau
- T. Schaul
- N. D. Freitas
fieldsOfStudy:
- Computer Science
meta_key: 2016-learning-to-learn-by-gradient-descent-by-gradient-descent
numCitedBy: 1347
reading_status: TBD
ref_count: 48
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Learning-to-learn-by-gradient-descent-by-gradient-Andrychowicz-Denil/395dd01c0d24777c660cf195c4cfadcdf51fb7e8?sort=total-citations
venue: NIPS
year: 2016
---

# Learning to learn by gradient descent by gradient descent

## Abstract

The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.

## Paper References

1. Learning to Learn Using Gradient Descent
2. Meta-learning with backpropagation
3. Local Gain Adaptation in Stochastic Gradient Descent
4. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization.md)
5. Learning a synaptic learning rule
6. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
7. Meta-neural networks that learn by learning
8. An Incremental Gradient(-Projection) Method with Momentum Term and Adaptive Stepsize Rule
9. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method.md)
10. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
11. Adapting Bias by Gradient Descent - An Incremental Version of Delta-Bar-Delta
12. Optimizing Neural Networks with Kronecker-factored Approximate Curvature
13. Learning Step Size Controllers for Robust Neural Network Training
14. Fixed-weight on-line learning
15. Meta-Learning with Memory-Augmented Neural Networks
16. A direct adaptive method for faster backpropagation learning - the RPROP algorithm
17. Advances in optimizing recurrent networks
18. [Building machines that learn and think like people](2016-building-machines-that-learn-and-think-like-people.md)
19. Evolution and design of distributed learning rules
20. Learning to Control Fast-Weight Memories - An Alternative to Dynamic Recurrent Networks
21. Optimization with Sparsity-Inducing Penalties
22. Optimization with Sparsity-Inducing Penalties (Foundations and Trends(R) in Machine Learning)
23. A Neural Algorithm of Artistic Style
24. On the search for new learning rules for ANNs
25. [Long Short-Term Memory](1997-long-short-term-memory.md)
26. A signal processing framework based on dynamic neural networks with application to problems in adaptation, filtering, and classification
27. Learning to Learn
28. No free lunch theorems for optimization
29. [Neural Turing Machines](2014-neural-turing-machines.md)
30. Fixed-weight networks can learn
31. A neural network that embeds its own meta-levels
32. Numerical Optimization
33. Shifting Inductive Bias with Success-Story Algorithm, Adaptive Levin Search, and Incremental Self-Improvement
34. Adaptive behavior with fixed weights in RNN - an overview
35. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database.md)
36. Integer and Combinatorial Optimization
37. Compressed Sensing
38. A method for solving the convex programming problem with convergence rate O(1/k^2)
