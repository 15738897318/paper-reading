---
title: Learning Deep Architectures for AI
authors:
- Yoshua Bengio
fieldsOfStudy:
- Computer Science
meta_key: 2007-learning-deep-architectures-for-ai
numCitedBy: 7579
reading_status: TBD
ref_count: 346
tags:
- gen-from-ref
- other-default
- paper
venue: Found. Trends Mach. Learn.
year: 2007
---

# Learning Deep Architectures for AI

## Abstract

Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.

## Paper References

1. [Greedy Layer-Wise Training of Deep Networks](2006-greedy-layer-wise-training-of-deep-networks)
2. Scaling learning algorithms towards AI
3. Exploring Strategies for Training Deep Neural Networks
4. Representational Power of Restricted Boltzmann Machines and Deep Belief Networks
5. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations
6. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
7. On the quantitative analysis of deep belief networks
8. Sparse Feature Learning for Deep Belief Networks
9. Deep Boltzmann Machines
10. An empirical evaluation of deep architectures on problems with many factors of variation
11. Recursive Distributed Representations
12. Sparse deep belief net model for visual area V2
13. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
14. Efficient training of large neural networks for language modeling
15. Connectionist Learning of Belief Networks
16. Learning internal representations
17. A Scalable Hierarchical Distributed Language Model
18. Classification using discriminative restricted Boltzmann machines
19. A Learning Algorithm for Boltzmann Machines
20. Training Hierarchical Feed-Forward Visual Recognition Models Using Transfer Learning from Pseudo-Tasks
21. Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes
22. The Helmholtz Machine
23. Many-Layered Learning
24. Learning long-term dependencies with gradient descent is difficult
25. A Neural Probabilistic Language Model
26. Learning representations by back-propagating errors
27. Modeling image patches with a directed hierarchy of Markov random fields
28. A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling
29. [Extracting and composing robust features with denoising autoencoders](2008-extracting-and-composing-robust-features-with-denoising-autoencoders)
30. Convex Neural Networks
31. Slow, Decorrelated Features for Pretraining Complex Cell-like Networks
32. Generative models for discovering sparse distributed representations.
33. Natural Language Processing With Modular PDP Networks and Distributed Lexicon
34. Learning Multilevel Distributed Representations for High-Dimensional Sequences
35. Factored conditional restricted Boltzmann Machines for modeling motion style
36. Bayesian Learning for Neural Networks
37. Constituent Parsing with Incremental Sigmoid Belief Networks
38. Learning Continuous Attractors in Recurrent Networks
39. Parallel distributed processing - explorations in the microstructure of cognition, vol. 1 - foundations
40. Exploiting Generative Models in Discriminative Classifiers
41. The Curse of Highly Variable Functions for Local Kernel Machines
42. [Training Products of Experts by Minimizing Contrastive Divergence](2002-training-products-of-experts-by-minimizing-contrastive-divergence)
43. Learning and development in neural networks - the importance of starting small
44. A Tutorial on Energy-Based Learning
45. Unsupervised Learning of Distributions of Binary Vectors Using 2-Layer Networks
46. Training Connectionist Models for the Structured Language Model
47. Using fast weights to improve persistent contrastive divergence
48. On Contrastive Divergence Learning
49. Unsupervised Learning of Image Transformations
50. Self Supervised Boosting
51. Energy-Based Models for Sparse Overcomplete Representations
52. A Two-Layer ICA-Like Model Estimated by Score Matching
53. Restricted Boltzmann machines for collaborative filtering
54. Three new graphical models for statistical language modelling
55. Principled Hybrids of Generative and Discriminative Models
56. [Reinforcement Learning - An Introduction](2005-reinforcement-learning-an-introduction)
57. Large-scale kernel machines
58. Deep learning via semi-supervised embedding
59. [Support-Vector Networks](2004-support-vector-networks)
60. Efficient Non-Parametric Function Induction in Semi-Supervised Learning
61. Semi-supervised learning of compact document representations with deep networks
62. Shift-Invariance Sparse Coding for Audio Classification
63. [Reducing the Dimensionality of Data with Neural Networks](2006-reducing-the-dimensionality-of-data-with-neural-networks)
64. Active Learning with Statistical Models
65. Efficient sparse coding algorithms
66. Products of Experts
67. Semantic hashing
68. Differentiable Sparse Coding
69. Learning Overcomplete Representations
70. Links between perceptrons, MLPs and SVMs
71. A Formal Theory of Inductive Inference. Part II
72. A Unified Energy-Based Framework for Unsupervised Learning
73. Best practices for convolutional neural networks applied to visual document analysis
74. Exponential Family Harmoniums with an Application to Information Retrieval
75. Universal Artificial Intellegence - Sequential Decisions Based on Algorithmic Probability
76. Efficient Learning of Sparse Representations with an Energy-Based Model
77. Nonlocal Estimation of Manifold Structure
78. Sparse coding with an overcomplete basis set - A strategy employed by V1?
79. Stacked generalization
80. Establishing Good Benchmarks and Baselines for Face Recognition
81. Unsupervised Discovery of Nonlinear Structure Using Contrastive Backpropagation
82. Unsupervised Learning of Invariant Feature Hierarchies with Applications to Object Recognition
83. Deep belief net learning in a long-range vision system for autonomous off-road driving
84. Is Learning The n-th Thing Any Easier Than Learning The First?
85. Justifying and Generalizing Contrastive Divergence
86. Autoencoders, Minimum Description Length and Helmholtz Free Energy
87. A New View of ICA
88. Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure
89. Non-Local Manifold Tangent Learning
90. Connectionist language modeling for large vocabulary continuous speech recognition
91. Learning Despite Concept Variation by Finding Structure in Attribute-based Data
92. The Cost of Cortical Computation
93. Loss Functions for Discriminative Training of Energy-Based Models
94. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition)
95. Flexible shaping - How learning in small steps helps
96. Unsupervised Learning of Models for Recognition
97. Parallel Models of Associative Memory
98. Information processing in dynamical systems - foundations of harmony theory
99. Deep learning from temporal coherence in video
100. [A global geometric framework for nonlinear dimensionality reduction.](2000-a-global-geometric-framework-for-nonlinear-dimensionality-reduction)
101. Learning in Graphical Models
102. Rational Kernels - Theory and Algorithms
103. An Information-Maximization Approach to Blind Separation and Blind Deconvolution
104. Backpropagation Applied to Handwritten Zip Code Recognition
105. A logical calculus of the ideas immanent in nervous activity
106. An Introduction to Kolmogorov Complexity and Its Applications
107. [Neocognitron - A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position](2004-neocognitron-a-self-organizing-neural-network-model-for-a-mechanism-of-pattern-recognition-unaffected-by-shift-in-position)
108. Object Class Recognition and Localization Using Sparse Features with Limited Receptive Fields
109. Training restricted Boltzmann machines using approximations to the likelihood gradient
110. Using manifold structure for partially labelled classification
111. [Visualizing Data using t-SNE](2008-visualizing-data-using-t-sne)
112. Descartes' Rule of Signs for Radial Basis Function Neural Networks
113. Modeling Human Motion Using Binary Latent Variables
114. On Discriminative vs. Generative Classifiers - A comparison of logistic regression and naive Bayes
115. How to Make a Low-Dimensional Representation Suitable for Diverse Tasks
116. Building continuous space language models for transcribing european languages
117. Some extensions of score matching
118. [Random Forests](2004-random-forests)
119. Slow Feature Analysis - Unsupervised Learning of Invariances
120. A Hierarchical Model of Shape and Appearance for Human Action Classification
121. Input space versus feature space in kernel-based methods
122. Nonlinear Autoassociation Is Not Equivalent to PCA
123. Learning and relearning in Boltzmann machines
124. A Nonparametric Bayesian Approach to Modeling Overlapping Clusters
125. Learning methods for generic object recognition with invariance to pose and lighting
126. Describing Visual Scenes Using Transformed Objects and Parts
127. A Theoretical Analysis of Robust Coding over Noisy Overcomplete Channels
128. [A training algorithm for optimal margin classifiers](1992-a-training-algorithm-for-optimal-margin-classifiers)
129. Gaussian Processes for Regression
130. Estimation of Non-Normalized Statistical Models by Score Matching
131. A Sparse and Locally Shift Invariant Feature Extractor Applied to Document Images
132. On Kernel-Target Alignment
133. Small codes and large image databases for recognition
134. A Context-Sensitive Generalization of ICA
135. [Learning with Local and Global Consistency](2003-learning-with-local-and-global-consistency)
136. A survey of kernels for structured data
137. Mean Field Theory for Sigmoid Belief Networks
138. Learning Eigenfunctions Links Spectral Embedding and Kernel PCA
139. Extracting distributed representations of concepts and relations from positive and negative propositions
140. Transformation Invariant Autoassociation with Application to Handwritten Character Recognition
141. Statistical models for partial membership
142. Computational complexity of neural networks - a survey
143. Receptive fields, binocular interaction and functional architecture in the cat's visual cortex
144. The wake-sleep algorithm for unsupervised neural networks.
145. Global Data Analysis and the Fragmentation Problem in Decision Tree Induction
146. Compressed sensing
147. Parallel Distributed Processing - Explorations in the Microstructure of Cognition, vol 1 - Foundations, vol 2 - Psychological and Biological Models
148. [Nonlinear dimensionality reduction by locally linear embedding.](2000-nonlinear-dimensionality-reduction-by-locally-linear-embedding)
149. Decoding by linear programming
150. Connections Between Score Matching, Contrastive Divergence, and Pseudolikelihood for Continuous-Valued Variables
151. Parallel continuation-based global optimization for molecular conformation and protein folding
152. [Optimization by Simulated Annealing](1983-optimization-by-simulated-annealing)
153. An Introduction to MCMC for Machine Learning
154. Online Convex Programming and Generalized Infinitesimal Gradient Ascent
155. Explorations in parallel distributed processing - a handbook of models, programs, and exercises
156. An Information Measure for Classification
157. Introduction to Reinforcement Learning
158. Book Review - An introduction to Kolmogorov Complexity and its Applications Second Edition, 1997 by Ming Li and Paul Vitanyi (Springer (Graduate Text Series))
159. Learning kernel parameters by using class separability measure
160. Indexing by Latent Semantic Analysis
161. Segmentation using eigenvectors - a unifying view
162. Training Invariant Support Vector Machines using Selective Sampling
163. Almost optimal lower bounds for small depth circuits
164. Efficient Pattern Recognition Using a New Transformation Distance
165. Charting a Manifold
166. An introduction to hidden Markov models.
167. Regularization and Semi-supervised Learning on Large Graphs
168. Global Continuation for Distance Geometry Problems
169. The complexity of Boolean functions
170. An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators
171. The Entire Regularization Path for the Support Vector Machine
172. Introduction to mathematical logic
173. A day of great illumination - B. F. Skinner's discovery of shaping.
174. Separating the polynomial-time hierarchy by oracles
175. Numerical continuation methods - an introduction
176. Is postnatal neocortical maturation hierarchical?
177. The Origins of Intelligence in Children
178. Independent Component Analysis
179. Information Processing Systems
180. Smoothing techniques for macromolecular global optimization
181. On the power of small-depth threshold circuits
182. [The Nature of Statistical Learning Theory](2000-the-nature-of-statistical-learning-theory)
183. Global Continuation for Distance Geometry Problems Global Continuation for Distance Geometry Problems
184. Hybrid Monte Carlo
185. Complexity Lower Bounds for Approximation Algebraic Computation Trees
186. Three approaches to the quantitative definition of information
187. Energy Budget
188. To recognize shapes, first learn to generate images.
189. Self-taught learning - transfer learning from unlabeled data
190. The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training
191. Analysis of a complex of statistical variables into principal components.
192. Learning distributed representations of concepts.
193. An Energy Budget for Signaling in the Grey Matter of the Brain
194. Experiments with a New Boosting Algorithm
195. Supervised Dictionary Learning
196. An interactive activation model of context effects in letter perception - I. An account of basic findings.
197. Numerical Continuation Methods
198. Fundamentals of statistical exponential families - with applications in statistical decision theory
199. Markov fields on finite graphs and lattices
200. What is learning
201. Memoires associatives distribuees - Une comparaison (Distributed associative memories - A comparison)
202. Advances in Neural Information Processing Systems 26 (NIPS 2013)
203. Editors. Advances in Neural Information Processing Systems
204. Learning long-term dependencies is not as difficult with NARX recurrent neural networks
205. Untersuchungen zu dynamischen neuronalen Netzen
206. Explorations In Parallel Distributed Processing
207. Advances in kernel methods - support vector learning
208. Stochastic Complexity in Statistical Inquiry
209. Introduction to mathematical logic (3. ed.)
210. An Introduction to Kolmogorov Complexity and Its Applications
211. Learning in neural networks
212. Asymptotic analysis
213. Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images
214. Auto-association by multilayer perceptrons and singular value decomposition
215. Learning Nonlinear Overcomplete Representations for Efficient Coding
216. Dimensionality Reduction by Learning an Invariant Mapping
217. Hierarchical Bayesian inference in the visual cortex.
218. [Semi-Supervised Learning Using Gaussian Fields and Harmonic Functions](2003-semi-supervised-learning-using-gaussian-fields-and-harmonic-functions)
219. A Formal Theory of Inductive Inference. Part I
220. Learning the Kernel Matrix with Semidefinite Programming
221. Nonlinear Component Analysis as a Kernel Eigenvalue Problem
222. How to invent a lexicon - the development of shared symbols in interaction
223. [Curriculum learning](2009-curriculum-learning)
224. Evaluating probabilities under high-dimensional latent variable models
225. A quantitative theory of immediate visual recognition.
