---
title: Transformers - State-of-the-Art Natural Language Processing
authors:
- Thomas Wolf
- Lysandre Debut
- Victor Sanh
- Julien Chaumond
- Clement Delangue
- Anthony Moi
- Pierric Cistac
- T. Rault
- "R\xE9mi Louf"
- Morgan Funtowicz
- Jamie Brew
fieldsOfStudy:
- Computer Science
meta_key: 2020-transformers-state-of-the-art-natural-language-processing
numCitedBy: 2430
reading_status: TBD
ref_count: 48
tags:
- gen-from-ref
- other-default
- paper
venue: EMNLP
year: 2020
---

# Transformers - State-of-the-Art Natural Language Processing

## Abstract

Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.

## Paper References

1. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding)
2. AllenNLP - A Deep Semantic Natural Language Processing Platform
3. [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](2020-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer)
4. Reformer - The Efficient Transformer
5. DistilBERT, a distilled version of BERT - smaller, faster, cheaper and lighter
6. [Transformer-XL - Attentive Language Models beyond a Fixed-Length Context](2019-transformer-xl-attentive-language-models-beyond-a-fixed-length-context)
7. SuperGLUE - A Stickier Benchmark for General-Purpose Language Understanding Systems
8. [ALBERT - A Lite BERT for Self-supervised Learning of Language Representations](2020-albert-a-lite-bert-for-self-supervised-learning-of-language-representations)
9. FLAIR - An Easy-to-Use Framework for State-of-the-Art NLP
10. [Language Models are Unsupervised Multitask Learners](2019-language-models-are-unsupervised-multitask-learners)
11. FlauBERT - Unsupervised Language Model Pre-training for French
12. [Attention is All you Need](2017-attention-is-all-you-need)
13. [Universal Language Model Fine-tuning for Text Classification](2018-universal-language-model-fine-tuning-for-text-classification)
14. SciBERT - A Pretrained Language Model for Scientific Text
15. exBERT - A Visual Analysis Tool to Explore Learned Representations in Transformer Models
16. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
17. [XLNet - Generalized Autoregressive Pretraining for Language Understanding](2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding)
18. Megatron-LM - Training Multi-Billion Parameter Language Models Using Model Parallelism
19. [Cross-lingual Language Model Pretraining](2019-cross-lingual-language-model-pretraining)
20. Longformer - The Long-Document Transformer
21. BART - Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
22. Stanza - A Python Natural Language Processing Toolkit for Many Human Languages
23. jiant - A Software Toolkit for Research on General-Purpose Text Understanding Models
24. [Learned in Translation - Contextualized Word Vectors](2017-learned-in-translation-contextualized-word-vectors)
25. BERT Rediscovers the Classical NLP Pipeline
26. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
27. ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators
28. COMET - Commonsense Transformers for Automatic Knowledge Graph Construction
29. [RoBERTa - A Robustly Optimized BERT Pretraining Approach](2019-roberta-a-robustly-optimized-bert-pretraining-approach)
30. Texar - A Modularized, Versatile, and Extensible Toolkit for Text Generation
31. OpenNMT - Open-Source Toolkit for Neural Machine Translation
32. [The Stanford CoreNLP Natural Language Processing Toolkit](2014-the-stanford-corenlp-natural-language-processing-toolkit)
33. fairseq - A Fast, Extensible Toolkit for Sequence Modeling
34. [SpanBERT - Improving Pre-training by Representing and Predicting Spans](2020-spanbert-improving-pre-training-by-representing-and-predicting-spans)
35. LSTMVis - A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks
36. [NLTK - The Natural Language Toolkit](2004-nltk-the-natural-language-toolkit)
37. TVM - An Automated End-to-End Optimizing Compiler for Deep Learning
38. Supervised Multimodal Bitransformers for Classifying Images and Text
39. Model Cards for Model Reporting
