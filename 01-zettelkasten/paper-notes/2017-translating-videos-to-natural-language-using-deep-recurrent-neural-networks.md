---
title: Translating Videos to Natural Language Using Deep Recurrent Neural Networks
authors:
- U. Austin
- Austin
- UMass Lowell
- Lowell
fieldsOfStudy:
- Computer Science
meta_key: 2017-translating-videos-to-natural-language-using-deep-recurrent-neural-networks
numCitedBy: 654
reading_status: TBD
ref_count: 50
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Translating-Videos-to-Natural-Language-Using-Deep-Austin-Austin/43795b7bac3d921c4e579964b54187bdbf6c6330?sort=total-citations
venue: ''
year: 2017
---

[semanticscholar url](https://www.semanticscholar.org/paper/Translating-Videos-to-Natural-Language-Using-Deep-Austin-Austin/43795b7bac3d921c4e579964b54187bdbf6c6330?sort=total-citations)

# Translating Videos to Natural Language Using Deep Recurrent Neural Networks

## Abstract

Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.

## Paper References

1. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator)
2. [Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](2014-unifying-visual-semantic-embeddings-with-multimodal-neural-language-models)
3. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
4. Integrating Language and Vision to Generate Natural Language Descriptions of Videos in the Wild
5. Translating Video Content to Natural Language Descriptions
6. [Long-term recurrent convolutional networks for visual recognition and description](2015-long-term-recurrent-convolutional-networks-for-visual-recognition-and-description)
7. Explain Images with Multimodal Recurrent Neural Networks
8. A Thousand Frames in Just a Few Words - Lingual Description of Videos through Latent Topics and Sparse Object Stitching
9. Jointly Modeling Deep Video and Compositional Text to Bridge Vision and Language in a Unified Framework
10. [From captions to visual concepts and back](2015-from-captions-to-visual-concepts-and-back)
11. [DeCAF - A Deep Convolutional Activation Feature for Generic Visual Recognition](2014-decaf-a-deep-convolutional-activation-feature-for-generic-visual-recognition)
12. Generating Natural-Language Video Descriptions Using Text-Mined Knowledge
13. Learning to Execute
14. YouTube2Text - Recognizing and Describing Arbitrary Activities Using Semantic Hierarchies and Zero-Shot Recognition
15. Deep Fragment Embeddings for Bidirectional Image Sentence Mapping
16. Describing Video Contents in Natural Language
17. [On the Properties of Neural Machine Translation - Encoder-Decoder Approaches](2014-on-the-properties-of-neural-machine-translation-encoder-decoder-approaches)
18. [Towards End-To-End Speech Recognition with Recurrent Neural Networks](2014-towards-end-to-end-speech-recognition-with-recurrent-neural-networks)
19. Grounded Language Learning from Video Described with Sentences
20. [ImageNet classification with deep convolutional neural networks](2012-alexnet.md)
21. [Caffe - Convolutional Architecture for Fast Feature Embedding](2014-caffe-convolutional-architecture-for-fast-feature-embedding)
22. Translating related words to videos and back through latent topics
23. Natural Language Description of Human Activities from Video Images Based on Concept Hierarchy of Actions
24. [Visualizing and Understanding Convolutional Networks](2014-visualizing-and-understanding-convolutional-networks)
25. Coherent Multi-sentence Video Description with Variable Level of Detail
26. Every Picture Tells a Story - Generating Sentences from Images
27. Beyond audio and video retrieval - towards multimedia summarization
28. Video2Text - Learning to Annotate Video Content
29. [Long Short-Term Memory](1997-long-short-term-memory)
30. I2T - Image Parsing to Text Description
31. Baby talk - Understanding and generating simple image descriptions
32. Comparing Automatic Evaluation Measures for Image Description
33. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
34. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge)
35. TreeTalk - Composition and Compression of Trees for Image Descriptions
36. Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
37. Generating Image Descriptions Using Dependency Relational Patterns
38. TRECVID 2015 - An Overview of the Goals, Tasks, Data, Evaluation Mechanisms and Metrics
39. Improving Video Activity Recognition using Object Recognition and Text Mining
40. SAVE - A framework for semantic annotation of visual events
41. [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](2014-from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions)
42. [Bleu - a Method for Automatic Evaluation of Machine Translation](2002-bleu-a-method-for-automatic-evaluation-of-machine-translation)
43. MULTIMODAL FUSION FOR VIDEO SEARCH RERANKING
44. Statistical machine translation
45. A Multi-modal Clustering Method for Web Videos
46. [METEOR - An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments](2005-meteor-an-automatic-metric-for-mt-evaluation-with-improved-correlation-with-human-judgments)
47. Collecting Highly Parallel Data for Paraphrase Evaluation
