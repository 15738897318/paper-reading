---
title: Knowing When to Look - Adaptive Attention via a Visual Sentinel for Image Captioning
authors:
- Jiasen Lu
- Caiming Xiong
- Devi Parikh
- R. Socher
fieldsOfStudy:
- Computer Science
meta_key: 2017-knowing-when-to-look-adaptive-attention-via-a-visual-sentinel-for-image-captioning
numCitedBy: 971
reading_status: TBD
ref_count: 38
tags:
- gen-from-ref
- other-default
- paper
venue: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2017
---

# Knowing When to Look - Adaptive Attention via a Visual Sentinel for Image Captioning

## Abstract

Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as the and of. Other words that may seem visual can often be predicted reliably just from the language model e.g., sign after behind a red stop or phone following talking on a cell. In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.

## Paper References

1. [Hierarchical Question-Image Co-Attention for Visual Question Answering](2016-hierarchical-question-image-co-attention-for-visual-question-answering)
2. Mind's eye - A recurrent visual representation for image caption generation
3. [From captions to visual concepts and back](2015-from-captions-to-visual-concepts-and-back)
4. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator)
5. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
6. [Image Captioning with Semantic Attention](2016-image-captioning-with-semantic-attention)
7. [What Value Do Explicit High Level Concepts Have in Vision to Language Problems?](2016-what-value-do-explicit-high-level-concepts-have-in-vision-to-language-problems)
8. [Boosting Image Captioning with Attributes](2017-boosting-image-captioning-with-attributes)
9. Grad-CAM - Visual Explanations from Deep Networks via Gradient-Based Localization
10. [Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)](2015-deep-captioning-with-multimodal-recurrent-neural-networks-m-rnn)
11. [Deep Visual-Semantic Alignments for Generating Image Descriptions](2017-deep-visual-semantic-alignments-for-generating-image-descriptions)
12. [Dynamic Memory Networks for Visual and Textual Question Answering](2016-dynamic-memory-networks-for-visual-and-textual-question-answering)
13. [Stacked Attention Networks for Image Question Answering](2016-stacked-attention-networks-for-image-question-answering)
14. [Long-term recurrent convolutional networks for visual recognition and description](2015-long-term-recurrent-convolutional-networks-for-visual-recognition-and-description)
15. Every Picture Tells a Story - Generating Sentences from Images
16. [Rethinking the Inception Architecture for Computer Vision](2016-rethinking-the-inception-architecture-for-computer-vision)
17. Grad-CAM - Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization
18. BabyTalk - Understanding and Generating Simple Image Descriptions
19. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
20. Encode, Review, and Decode - Reviewer Module for Caption Generation
21. Grounded Compositional Semantics for Finding and Describing Images with Sentences
22. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition)
23. [CIDEr - Consensus-based image description evaluation](2015-cider-consensus-based-image-description-evaluation)
24. [SPICE - Semantic Propositional Image Caption Evaluation](2016-spice-semantic-propositional-image-caption-evaluation)
25. Learning Deep Features for Discriminative Localization
26. Multimodal Neural Language Models
27. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
28. Baby talk - Understanding and generating simple image descriptions
29. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
30. Collective Generation of Natural Image Descriptions
31. Exploring Nearest Neighbor Approaches for Image Captioning
32. Pointer Sentinel Mixture Models
33. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
34. Midge - Generating Image Descriptions From Computer Vision Detections
35. [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](2014-from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions)
36. [ROUGE - A Package for Automatic Evaluation of Summaries](2004-rouge-a-package-for-automatic-evaluation-of-summaries)
37. [Meteor Universal - Language Specific Translation Evaluation for Any Target Language](2014-meteor-universal-language-specific-translation-evaluation-for-any-target-language)
38. [Bleu - a Method for Automatic Evaluation of Machine Translation](2002-bleu-a-method-for-automatic-evaluation-of-machine-translation)
