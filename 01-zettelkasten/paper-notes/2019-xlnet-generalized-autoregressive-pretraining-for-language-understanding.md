---
title: XLNet - Generalized Autoregressive Pretraining for Language Understanding
authors:
- Zhilin Yang
- Zihang Dai
- Yiming Yang
- J. Carbonell
- R. Salakhutdinov
- Quoc V. Le
fieldsOfStudy:
- Computer Science
meta_key: 2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding
numCitedBy: 4296
reading_status: TBD
ref_count: 48
tags:
- gen-from-ref
- other-default
- paper
venue: NeurIPS
year: 2019
---

# XLNet - Generalized Autoregressive Pretraining for Language Understanding

## Abstract

With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.

## Paper References

1. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
2. [ALBERT - A Lite BERT for Self-supervised Learning of Language Representations](2020-albert-a-lite-bert-for-self-supervised-learning-of-language-representations)
3. MADE - Masked Autoencoder for Distribution Estimation
4. [Character-Level Language Modeling with Deeper Self-Attention](2019-character-level-language-modeling-with-deeper-self-attention)
5. [Multi-Task Deep Neural Networks for Natural Language Understanding](2019-multi-task-deep-neural-networks-for-natural-language-understanding)
6. Revisiting LSTM Networks for Semi-Supervised Text Classification via Mixed Objective Function
7. MaskGAN - Better Text Generation via Filling in the ______
8. [Learned in Translation - Contextualized Word Vectors](2017-learned-in-translation-contextualized-word-vectors)
9. [Transformer-XL - Attentive Language Models beyond a Fixed-Length Context](2019-transformer-xl-attentive-language-models-beyond-a-fixed-length-context)
10. Breaking the Softmax Bottleneck - A High-Rank RNN Language Model
11. [Attention is All you Need](2017-attention-is-all-you-need)
12. Adaptive Input Representations for Neural Language Modeling
13. [RoBERTa - A Robustly Optimized BERT Pretraining Approach](2019-roberta-a-robustly-optimized-bert-pretraining-approach)
14. Semi-supervised Sequence Learning
15. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
16. Snorkel - Rapid Training Data Creation with Weak Supervision
17. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
18. BAM! Born-Again Multi-Task Networks for Natural Language Understanding
19. [Universal Language Model Fine-tuning for Text Classification](2018-universal-language-model-fine-tuning-for-text-classification)
20. Unsupervised Data Augmentation
21. Adversarial Training Methods for Semi-Supervised Text Classification
22. Virtual Adversarial Training for Semi-Supervised Text Classification
23. Neural Autoregressive Distribution Estimation
24. SentencePiece - A simple and language independent subword tokenizer and detokenizer for Neural Text Processing
25. A Surprisingly Robust Trick for the Winograd Schema Challenge
26. End-to-End Neural Ad-hoc Ranking with Kernel Pooling
27. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
28. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
29. Convolutional Neural Networks for Soft-Matching N-Grams in Ad-hoc Search
30. [Know What You Don't Know - Unanswerable Questions for SQuAD](2018-know-what-you-don-t-know-unanswerable-questions-for-squad)
31. Improving Question Answering with External Knowledge
32. Deep Pyramid Convolutional Neural Networks for Text Categorization
33. A Deep Relevance Matching Model for Ad-hoc Retrieval
34. Dual Co-Matching Network for Multi-choice Reading Comprehension
35. Word-Entity Duet Representations for Document Ranking
36. Option Comparison Network for Multiple-choice Reading Comprehension
37. [Character-level Convolutional Networks for Text Classification](2015-character-level-convolutional-networks-for-text-classification)
38. Modeling High-Dimensional Discrete Data with Multi-Layer Neural Networks
39. [RACE - Large-scale ReAding Comprehension Dataset From Examinations](2017-race-large-scale-reading-comprehension-dataset-from-examinations)
40. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
41. [NewsQA - A Machine Comprehension Dataset](2017-newsqa-a-machine-comprehension-dataset)
42. Pixel Recurrent Neural Networks
