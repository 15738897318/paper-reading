---
title: Google Vizier - A Service for Black-Box Optimization
authors:
- D. Golovin
- Benjamin Solnik
- Subhodeep Moitra
- G. Kochanski
- J. Karro
- D. Sculley
fieldsOfStudy:
- Computer Science
meta_key: 2017-google-vizier-a-service-for-black-box-optimization
numCitedBy: 524
reading_status: TBD
ref_count: 38
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Google-Vizier:-A-Service-for-Black-Box-Optimization-Golovin-Solnik/938f6ef7eed095919e6a482c7f1836a01d62db4b?sort=total-citations
venue: KDD
year: 2017
---

# Google Vizier - A Service for Black-Box Optimization

## Abstract

Any sufficiently complex system acts as a black box when it becomes easier to experiment with than to understand. Hence, black-box optimization has become increasingly important as systems have become more complex. In this paper we describe Google Vizier, a Google-internal service for performing black-box optimization that has become the de facto parameter tuning engine at Google. Google Vizier is used to optimize many of our machine learning models and other systems, and also provides core capabilities to Google's Cloud Machine Learning HyperTune subsystem. We discuss our requirements, infrastructure design, underlying algorithms, and advanced features such as transfer learning and automated early stopping that the service provides.

## Paper References

1. Real-Parameter Black-Box Optimization Benchmarking 2009 - Noiseless Functions Definitions
2. Collaborative hyperparameter tuning
3. [Sequential Model-Based Optimization for General Algorithm Configuration](2011-sequential-model-based-optimization-for-general-algorithm-configuration.md)
4. [Taking the Human Out of the Loop - A Review of Bayesian Optimization](2016-taking-the-human-out-of-the-loop-a-review-of-bayesian-optimization.md)
5. Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves
6. Bayesian Optimization with Robust Bayesian Neural Networks
7. [Practical Bayesian Optimization of Machine Learning Algorithms](2012-practical-bayesian-optimization-of-machine-learning-algorithms.md)
8. Scalable Bayesian Optimization Using Deep Neural Networks
9. [Algorithms for Hyper-Parameter Optimization](2011-algorithms-for-hyper-parameter-optimization.md)
10. Hyperband - A Novel Bandit-Based Approach to Hyperparameter Optimization
11. Efficient Transfer Learning Method for Automatic Hyperparameter Tuning
12. Optimization Under Unknown Constraints
13. Capacity and Trainability in Recurrent Neural Networks
14. Parallelizing Exploration-Exploitation Tradeoffs with Gaussian Process Bandit Optimization
15. Deep Kernel Learning
16. Bayesian Optimization with Unknown Constraints
17. DÂ³ Data-Driven Documents
18. D 3 - Data-Driven Documents
19. Derivative-free optimization - a review of algorithms and comparison of software implementations
20. Freeze-Thaw Bayesian Optimization
21. Completely Derandomized Self-Adaptation in Evolution Strategies
22. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
23. Gaussian Process Optimization in the Bandit Setting - No Regret and Experimental Design
24. Bayesian Optimization with Inequality Constraints
25. Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)
26. State of the Art of Parallel Coordinates
27. Response surface bandits
28. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization.md)
29. A Simplex Method for Function Minimization
30. Sequential Design of Experiments
31. Introduction to derivative-free optimization
