---
title: Bitwise Neural Networks
authors:
- Minje Kim
- P. Smaragdis
fieldsOfStudy:
- Computer Science
meta_key: 2016-bitwise-neural-networks
numCitedBy: 186
reading_status: TBD
ref_count: 22
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Bitwise-Neural-Networks-Kim-Smaragdis/8fcfd67f21738eff12d853fdf2b31ee192e2312a?sort=total-citations
venue: ArXiv
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/Bitwise-Neural-Networks-Kim-Smaragdis/8fcfd67f21738eff12d853fdf2b31ee192e2312a?sort=total-citations)

# Bitwise Neural Networks

## Abstract

Based on the assumption that there exists a neural network that efficiently represents a set of Boolean functions between all binary inputs and outputs, we propose a process for developing and deploying neural networks whose weight parameters, bias terms, input, and intermediate hidden layer output signals, are all binary-valued, and require only basic bit logic for the feedforward pass. The proposed Bitwise Neural Network (BNN) is especially suitable for resource-constrained environments, since it replaces either floating or fixed-point arithmetic with significantly more efficient bitwise operations. Hence, the BNN requires for less spatial complexity, less memory bandwidth, and less power consumption in hardware. In order to design such networks, we propose to add a few training schemes, such as weight compression and noisy backpropagation, which result in a bitwise network that performs almost as well as its corresponding real-valued network. We test the proposed network on the MNIST dataset, represented using binary features, and show that BNNs result in competitive performance while offering dramatic computational savings.

## Paper References

1. Fixed-point feedforward deep neural network design using weights +1, 0, and −1
2. Training deep neural networks with low precision multiplications
3. Low precision arithmetic for deep learning
4. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
5. On Learning µ-Perceptron Networks with Binary Weights
6. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets.md)
7. Weight discretization paradigm for optical neural networks
8. [GradientBased Learning Applied to Document Recognition](2001-gradientbased-learning-applied-to-document-recognition.md)
9. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition.md)
10. [Learning Deep Architectures for AI](2007-learning-deep-architectures-for-ai.md)
11. Expectation Backpropagation - Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights
12. Approximation capabilities of multilayer feedforward networks
13. An Experimental Study on Speech Enhancement Based on Deep Neural Networks
14. [Deep Neural Networks for Acoustic Modeling in Speech Recognition - The Shared Views of Four Research Groups](2012-deep-neural-networks-for-acoustic-modeling-in-speech-recognition-the-shared-views-of-four-research-groups.md)
15. [Maxout Networks](2013-maxout-networks.md)
16. Semantic hashing
17. A logical calculus of the ideas immanent in nervous activity
18. Computational limitations on learning from examples
19. A survey on context-aware systems
