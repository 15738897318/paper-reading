---
title: Ask Me Anything - Dynamic Memory Networks for Natural Language Processing
authors:
- A. Kumar
- Ozan Irsoy
- Peter Ondruska
- Mohit Iyyer
- James Bradbury
- Ishaan Gulrajani
- Victor Zhong
- Romain Paulus
- R. Socher
fieldsOfStudy:
- Computer Science
meta_key: 2016-ask-me-anything-dynamic-memory-networks-for-natural-language-processing
numCitedBy: 1004
reading_status: TBD
ref_count: 55
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Ask-Me-Anything:-Dynamic-Memory-Networks-for-Kumar-Irsoy/452059171226626718eb677358836328f884298e?sort=total-citations
venue: ICML
year: 2016
---

# Ask Me Anything - Dynamic Memory Networks for Natural Language Processing

## Abstract

Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.

## Paper References

1. [A Convolutional Neural Network for Modelling Sentences](2014-a-convolutional-neural-network-for-modelling-sentences)
2. Context dependent recurrent neural network language model
3. A Neural Network for Factoid Question Answering over Paragraphs
4. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
5. [Memory Networks](2015-memory-networks)
6. [Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks](2015-improved-semantic-representations-from-tree-structured-long-short-term-memory-networks)
7. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
8. [Learning to Compose Neural Networks for Question Answering](2016-learning-to-compose-neural-networks-for-question-answering)
9. [Towards AI-Complete Question Answering - A Set of Prerequisite Toy Tasks](2016-towards-ai-complete-question-answering-a-set-of-prerequisite-toy-tasks)
10. Recursive Neural Networks for Learning Logical Semantics
11. Lexicon Infused Phrase Embeddings for Named Entity Resolution
12. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
13. Distributed representations, simple recurrent networks, and grammatical structure
14. [Stacked Attention Networks for Image Question Answering](2016-stacked-attention-networks-for-image-question-answering)
15. [Convolutional Neural Networks for Sentence Classification](2014-convolutional-neural-networks-for-sentence-classification)
16. Modeling Compositionality with Multiplicative Recurrent Neural Networks
17. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend)
18. Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing
19. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
20. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
21. Reasoning With Neural Tensor Networks for Knowledge Base Completion
22. [Recurrent Continuous Translation Models](2013-recurrent-continuous-translation-models)
23. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
24. [Long Short-Term Memory](1997-long-short-term-memory)
25. [On the Properties of Neural Machine Translation - Encoder-Decoder Approaches](2014-on-the-properties-of-neural-machine-translation-encoder-decoder-approaches)
26. [Distributed Representations of Sentences and Documents](2014-distributed-representations-of-sentences-and-documents)
27. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](2014-empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling)
28. [Deep Visual-Semantic Alignments for Generating Image Descriptions](2017-deep-visual-semantic-alignments-for-generating-image-descriptions)
29. Learning a Recurrent Visual Representation for Image Caption Generation
30. A Bayesian Analysis of Dynamics in Free Recall
31. Deep Networks with Internal Selective Attention through Feedback Connections
32. Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
33. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
34. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
35. A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input
36. Removing the Training Wheels - A Coreference Dataset that Entertains Humans and Challenges Computers
37. Easy Victories and Uphill Battles in Coreference Resolution
38. Building a Large Annotated Corpus of English - The Penn Treebank
39. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
40. The hippocampus and memory for orderly stimulus relations.
41. Neural GPUs Learn Algorithms
42. A distributed representation of temporal context
43. The neurobiology of semantic memory
44. [Neural Turing Machines](2014-neural-turing-machines)
45. TextRunner - Open Information Extraction on the Web
46. From Conditioning to Conscious Recollection Memory Systems of the Brain. Oxford Psychology Series, Volume 35.
47. Hippocampal activation during transitive inference in humans
48. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization)
49. Semi-supervised condensed nearest neighbor for part-of-speech tagging
