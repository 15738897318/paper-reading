---
title: End-To-End Memory Networks
authors:
- Sainbayar Sukhbaatar
- Arthur D. Szlam
- J. Weston
- R. Fergus
fieldsOfStudy:
- Computer Science
meta_key: 2015-end-to-end-memory-networks
numCitedBy: 1998
reading_status: TBD
ref_count: 27
tags:
- gen-from-ref
- other-default
- paper
venue: NIPS
year: 2015
---

# End-To-End Memory Networks

## Abstract

We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.

## Paper References

1. [LSTM Neural Networks for Language Modeling](2012-lstm-neural-networks-for-language-modeling)
2. A Clockwork RNN
3. Learning Longer Memory in Recurrent Neural Networks
4. [Long Short-Term Memory](1997-long-short-term-memory)
5. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](2014-empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling)
6. Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
7. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
8. Learning Context-free Grammars - Capabilities and Limitations of a Recurrent Neural Network with an External Stack Memory
9. [Towards AI-Complete Question Answering - A Set of Prerequisite Toy Tasks](2016-towards-ai-complete-question-answering-a-set-of-prerequisite-toy-tasks)
10. Statistical Language Models Based on Neural Networks
11. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
12. [DRAW - A Recurrent Neural Network For Image Generation](2015-draw-a-recurrent-neural-network-for-image-generation)
13. A Neural Probabilistic Language Model
14. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
15. A bit of progress in language modeling
16. The Induction of Dynamical Recognizers
17. Memory-based neural networks for robot learning
18. A Connectionist Symbol Manipulator that Discovers the Structure of Context-Free Languages
19. [Neural Turing Machines](2014-neural-turing-machines)
20. Building a Large Annotated Corpus of English - The Penn Treebank
21. Learning Matrices and Their Applications
22. Pattern recognition by means of automatic analogue apparatus
23. [Memory Networks](2015-memory-networks)
24. Pattern Recognition by Means of Automatic Analogue Apparatus
25. Towards Neural Network-based Reasoning
26. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization)
