---
title: Hybrid computing using a neural network with dynamic external memory
authors:
- A. Graves
- Greg Wayne
- Malcolm Reynolds
- Tim Harley
- Ivo Danihelka
- Agnieszka Grabska-Barwinska
- Sergio Gomez Colmenarejo
- Edward Grefenstette
- Tiago Ramalho
- J. Agapiou
- Adrià Puigdomènech Badia
- K. Hermann
- Yori Zwols
- Georg Ostrovski
- Adam Cain
- Helen King
- C. Summerfield
- P. Blunsom
- K. Kavukcuoglu
- D. Hassabis
fieldsOfStudy:
- Computer Science
meta_key: 2016-hybrid-computing-using-a-neural-network-with-dynamic-external-memory
numCitedBy: 1215
reading_status: TBD
ref_count: 42
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Hybrid-computing-using-a-neural-network-with-memory-Graves-Wayne/784ee73d5363c711118f784428d1ab89f019daa5?sort=total-citations
venue: Nature
year: 2016
---

# Hybrid computing using a neural network with dynamic external memory

## Abstract

Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read–write memory.

## Paper References

1. Sparse Distributed Memory
2. Human-level control through deep reinforcement learning
3. Pointer Networks
4. Memory traces in dynamical systems
5. [Long Short-Term Memory](1997-long-short-term-memory)
6. [End-To-End Memory Networks](2015-end-to-end-memory-networks)
7. Human-level concept learning through probabilistic program induction
8. [Simple statistical gradient-following algorithms for connectionist reinforcement learning](2004-simple-statistical-gradient-following-algorithms-for-connectionist-reinforcement-learning)
9. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
10. From machine learning to machine reasoning
11. [Speech recognition with deep recurrent neural networks](2013-speech-recognition-with-deep-recurrent-neural-networks)
12. Backpropagation Through Time - What It Does and How to Do It
13. What Learning Systems do Intelligent Agents Need? Complementary Learning Systems Theory Updated
14. [Large Scale Distributed Deep Networks](2012-large-scale-distributed-deep-networks)
15. Indirection and symbol-like processing in the prefrontal cortex and basal ganglia
16. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
17. Considerations arising from a complementary learning systems perspective on hippocampus and neocortex
18. Characteristics of sparsely encoded associative memory
19. Why there are complementary learning systems in the hippocampus and neocortex - insights from the successes and failures of connectionist models of learning and memory.
20. Hippocampal conjunctive encoding, storage, and recall - Avoiding a trade‐off
21. A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning
22. MINERVA 2 - A simulation model of human memory
23. One-Shot Generalization in Deep Generative Models
24. The Algebraic Mind - Integrating Connectionism and Cognitive Science
25. [DRAW - A Recurrent Neural Network For Image Generation](2015-draw-a-recurrent-neural-network-for-image-generation)
26. Cascade Models of Synaptically Stored Memories
27. [ImageNet classification with deep convolutional neural networks](2012-alexnet.md)
28. A distributed representation of temporal context
29. Search-based structured prediction
30. Paradox of pattern separation and adult neurogenesis - A dual role for new neurons balancing memory resolution and robustness
31. [Visualizing Data using t-SNE](2008-visualizing-data-using-t-sne)
32. Policy Gradient Methods for Reinforcement Learning with Function Approximation
33. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend)
34. A Synaptically Controlled, Associative Signal for Hebbian Plasticity in Hippocampal Neurons
35. Procedures As A Representation For Data In A Computer Program For Understanding Natural Language
36. Dynamic Storage Allocation - A Survey and Critical Review
37. Symbolic Communication Between Two Pigeons, (Columba livia domestica)
38. Learning distributed representations of concepts.
39. Meta-Learning with Memory-Augmented Neural Networks
40. [Memory Networks](2015-memory-networks)
41. [Curriculum learning](2009-curriculum-learning)
42. The role of context in object recognition
