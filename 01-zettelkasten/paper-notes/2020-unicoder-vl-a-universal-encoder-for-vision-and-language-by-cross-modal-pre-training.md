---
title: Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training
authors:
- Gen Li
- Nan Duan
- Yuejian Fang
- Daxin Jiang
- Ming Zhou
fieldsOfStudy:
- Computer Science
meta_key: 2020-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training
numCitedBy: 394
reading_status: TBD
ref_count: 46
tags:
- gen-from-ref
- other-default
- paper
venue: AAAI
year: 2020
---

# Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training

## Abstract

We propose Unicoder-VL, a universal encoder that aims to learn joint representations of vision and language in a pre-training manner. Borrow ideas from cross-lingual pre-trained models, such as XLM (Lample and Conneau 2019) and Unicoder (Huang et al. 2019), both visual and linguistic contents are fed into a multi-layer Transformer (Vaswani et al. 2017) for the cross-modal pre-training, where three pre-trained tasks are employed, including Masked Language Modeling(MLM), Masked Object Classification(MOC) and Visual-linguistic Matching(VLM). The first two tasks learn context-aware representations for input tokens based on linguistic and visual contents jointly. The last task tries to predict whether an image and a text describe each other. After pretraining on large-scale image-caption pairs, we transfer Unicoder-VL to caption-based image-text retrieval and visual commonsense reasoning, with just one additional output layer. We achieve state-of-the-art or comparable results on both two tasks and show the powerful ability of the cross-modal pre-training.

## Paper References

1. Unicoder - A Universal Language Encoder by Pre-training with Multiple Cross-lingual Tasks
2. [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks)
3. [UNITER - Learning UNiversal Image-TExt Representations](2019-uniter-learning-universal-image-text-representations)
4. [VL-BERT - Pre-training of Generic Visual-Linguistic Representations](2020-vl-bert-pre-training-of-generic-visual-linguistic-representations)
5. [VideoBERT - A Joint Model for Video and Language Representation Learning](2019-videobert-a-joint-model-for-video-and-language-representation-learning)
6. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding)
7. Dual-Path Convolutional Image-Text Embedding
8. [VisualBERT - A Simple and Performant Baseline for Vision and Language](2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language)
9. [Attention is All you Need](2017-attention-is-all-you-need)
10. [Cross-lingual Language Model Pretraining](2019-cross-lingual-language-model-pretraining)
11. [Deep Visual-Semantic Alignments for Generating Image Descriptions](2017-deep-visual-semantic-alignments-for-generating-image-descriptions)
12. [Dual-path Convolutional Image-Text Embeddings with Instance Loss](2020-dual-path-convolutional-image-text-embeddings-with-instance-loss)
13. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
14. [Stacked Cross Attention for Image-Text Matching](2018-stacked-cross-attention-for-image-text-matching)
15. [XLNet - Generalized Autoregressive Pretraining for Language Understanding](2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding)
16. Position Focused Attention Network for Image-Text Matching
17. [VSE++ - Improved Visual-Semantic Embeddings](2017-vse-improved-visual-semantic-embeddings)
18. Pythia-A platform for vision & language research
19. [From Recognition to Cognition - Visual Commonsense Reasoning](2019-from-recognition-to-cognition-visual-commonsense-reasoning)
20. [Learning Semantic Concepts and Order for Image and Sentence Matching](2018-learning-semantic-concepts-and-order-for-image-and-sentence-matching)
21. [Learning Deep Structure-Preserving Image-Text Embeddings](2016-learning-deep-structure-preserving-image-text-embeddings)
22. Multimodal Convolutional Neural Networks for Matching Image and Sentence
23. [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations)
24. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation)
25. [Fusion of Detected Objects in Text for Visual Question Answering](2019-fusion-of-detected-objects-in-text-for-visual-question-answering)
26. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
27. [VSE++ - Improving Visual-Semantic Embeddings with Hard Negatives](2018-vse-improving-visual-semantic-embeddings-with-hard-negatives)
28. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
29. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition)
30. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition)
31. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks)
32. [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning)
33. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
34. Knowledge Aware Semantic Concept Expansion for Image-Text Matching
35. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference)
36. [RoBERTa - A Robustly Optimized BERT Pretraining Approach](2019-roberta-a-robustly-optimized-bert-pretraining-approach)
37. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database)
38. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
39. Im2Text - Describing Images Using 1 Million Captioned Photographs
40. [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](2014-from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions)
41. [Microsoft COCO Captions - Data Collection and Evaluation Server](2015-microsoft-coco-captions-data-collection-and-evaluation-server)
42. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
