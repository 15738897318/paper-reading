---
title: Convolutional Sequence to Sequence Learning
authors:
- Jonas Gehring
- Michael Auli
- David Grangier
- Denis Yarats
- Yann Dauphin
fieldsOfStudy:
- Computer Science
meta_key: 2017-convolutional-sequence-to-sequence-learning
numCitedBy: 2438
reading_status: TBD
ref_count: 52
tags:
- gen-from-ref
- other-default
- paper
venue: ICML
year: 2017
---

# Convolutional Sequence to Sequence Learning

## Abstract

The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.

## Paper References

1. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
2. A Convolutional Encoder Model for Neural Machine Translation
3. [Language Modeling with Gated Convolutional Networks](2017-language-modeling-with-gated-convolutional-networks)
4. Quasi-Recurrent Neural Networks
5. [End-To-End Memory Networks](2015-end-to-end-memory-networks)
6. [Weight Normalization - A Simple Reparameterization to Accelerate Training of Deep Neural Networks](2016-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks)
7. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
8. [Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](2017-outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer)
9. Conditional Image Generation with PixelCNN Decoders
10. [On the importance of initialization and momentum in deep learning](2013-on-the-importance-of-initialization-and-momentum-in-deep-learning)
11. Neural Headline Generation with Sentence-wise Optimization
12. Neural Machine Translation with Recurrent Attention Modeling
13. Montreal Neural Machine Translation Systems for WMT'15
14. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift)
15. [On the difficulty of training recurrent neural networks](2013-on-the-difficulty-of-training-recurrent-neural-networks)
16. [Attention-Based Models for Speech Recognition](2015-attention-based-models-for-speech-recognition)
17. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
18. [Deep Residual Learning for Image Recognition](2015-resnet.md)
19. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation)
20. Edinburgh Neural Machine Translation Systems for WMT 16
21. [Long Short-Term Memory](1997-long-short-term-memory)
22. [Effective Approaches to Attention-based Neural Machine Translation](2015-effective-approaches-to-attention-based-neural-machine-translation)
23. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification)
24. Vocabulary Selection Strategies for Neural Machine Translation
25. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting)
26. [Neural Machine Translation of Rare Words with Subword Units](2016-neural-machine-translation-of-rare-words-with-subword-units)
27. Cutting-off Redundant Repeating Generations for Neural Abstractive Summarization
28. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks)
29. [A Neural Attention Model for Abstractive Sentence Summarization](2015-a-neural-attention-model-for-abstractive-sentence-summarization)
30. Vocabulary Manipulation for Neural Machine Translation
31. Phoneme recognition using time-delay neural networks
32. [Torch7 - A Matlab-like Environment for Machine Learning](2011-torch7-a-matlab-like-environment-for-machine-learning)
33. Key-Value Memory Networks for Directly Reading Documents
34. Finding Structure in Time
35. Findings of the 2016 Conference on Machine Translation
36. Japanese and Korean voice search
37. A Simple, Fast, and Effective Reparameterization of IBM Model 2
38. [ROUGE - A Package for Automatic Evaluation of Summaries](2004-rouge-a-package-for-automatic-evaluation-of-summaries)
39. DUC in context
40. The handbook of brain theory and neural networks
41. Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation
42. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond
43. Pixel Recurrent Neural Networks
44. Encoding Source Language with Convolutional Neural Network for Machine Translation
45. Linguistic Data Consortium
46. Convolutional networks for images, speech, and time series
