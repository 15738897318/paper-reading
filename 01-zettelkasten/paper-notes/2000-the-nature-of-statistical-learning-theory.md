---
title: The Nature of Statistical Learning Theory
authors:
- V. Vapnik
fieldsOfStudy:
- Computer Science
meta_key: 2000-the-nature-of-statistical-learning-theory
numCitedBy: 38825
reading_status: TBD
ref_count: 72
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/The-Nature-of-Statistical-Learning-Theory-Vapnik/8213dbed4db44e113af3ed17d6dad57471a0c048?sort=total-citations
venue: Statistics for Engineering and Information Science
year: 2000
---

# The Nature of Statistical Learning Theory

## Abstract

Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?.

## Paper References

1. Three fundamental concepts of the capacity of learning machines
2. [Statistical learning theory](1998-statistical-learning-theory)
3. Statistical Decision Theory and Bayesian Analysis
4. Subset Selection in Regression
5. Statistical predictor identification
6. Contributions to Mathematical Statistics
7. Vapnik-Chervonenkis bounds for generalization
8. Projection Pursuit Regression
9. Classification into two Multivariate Normal Distributions with Different Covariance Matrices
10. On the Length of Programs for Computing Finite Binary Sequences
11. VC Dimension and Uniform Learnability of Sparse Polynomials and Rational Functions
12. A Theory of Adaptive Pattern Classifiers
13. Interpolation of scattered data - Distance matrices and conditionally positive definite functions
14. Local Algorithms for Pattern Recognition and Dependencies Estimation
15. Automatic Pattern Recognition - A Study of the Probability of Error
16. Chervonenkis - On the uniform convergence of relative frequencies of events to their probabilities
17. Local Learning Algorithms
18. [A training algorithm for optimal margin classifiers](1992-a-training-algorithm-for-optimal-margin-classifiers)
19. Hinging hyperplanes for regression, classification, and function approximation
20. Universal approximation bounds for superpositions of a sigmoidal function
21. Transformation Invariance in Pattern Recognition-Tangent Distance and Tangent Propagation
22. Comparison of classifier methods - a case study in handwritten digit recognition
23. Ridge Regression - Biased Estimation for Nonorthogonal Problems
24. A PRELIMINARY REPORT ON A GENERAL THEORY OF INDUCTIVE INFERENCE
25. Boosting Performance in Neural Networks
26. Approximation by superpositions of a sigmoidal function
27. On the Density of Families of Sets
28. Central Limit Theorems for Empirical Measures
29. A Technique for the Numerical Solution of Certain Integral Equations of the First Kind
30. Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning
31. [Gradient-based learning applied to document recognition](1998-lenet5.md)
32. Universal Donsker Classes and Metric Entropy
33. Efficient Pattern Recognition Using a New Transformation Distance
34. Learning Internal Representations by Error Propagation, Parallel Distributed Processing
35. ON CONVERGENCE PROOFS FOR PERCEPTRONS
36. Necessary and Sufficient Conditions for the Uniform Convergence of Means to their Expectations
37. Nonparametric probability density estimation
38. On some asymptotic properties of maximum likelihood estimates and related Bayes' estimates
39. Learning processes in an asymmetric threshold network
40. On robust estimation of the location parameter
41. Networks for approximation and learning
42. Estimating the Dimension of a Model
