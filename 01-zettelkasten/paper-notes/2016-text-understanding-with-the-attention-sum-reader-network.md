---
title: Text Understanding with the Attention Sum Reader Network
authors:
- Rudolf Kadlec
- Martin Schmid
- Ondrej Bajgar
- Jan Kleindienst
fieldsOfStudy:
- Computer Science
meta_key: 2016-text-understanding-with-the-attention-sum-reader-network
numCitedBy: 290
reading_status: TBD
ref_count: 48
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Text-Understanding-with-the-Attention-Sum-Reader-Kadlec-Schmid/f2e50e2ee4021f199877c8920f1f984481c723aa?sort=total-citations
venue: ACL
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/Text-Understanding-with-the-Attention-Sum-Reader-Kadlec-Schmid/f2e50e2ee4021f199877c8920f1f984481c723aa?sort=total-citations)

# Text Understanding with the Attention Sum Reader Network

## Abstract

Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets.

## Paper References

1. Learning Answer-Entailing Structures for Machine Comprehension
2. [A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task](2016-a-thorough-examination-of-the-cnn-daily-mail-reading-comprehension-task.md)
3. Dynamic Entity Representation with Max-pooling Improves Machine Reading
4. MCTest - A Challenge Dataset for the Open-Domain Machine Comprehension of Text
5. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend.md)
6. [Memory Networks](2015-memory-networks.md)
7. [The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations](2016-the-goldilocks-principle-reading-children-s-books-with-explicit-memory-representations.md)
8. Machine Comprehension with Discourse Relations
9. [Pointer Networks](2015-pointer-networks.md)
10. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
11. Building Watson - An Overview of the DeepQA Project
12. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
13. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](2014-empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling.md)
14. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
15. [On the difficulty of training recurrent neural networks](2013-on-the-difficulty-of-training-recurrent-neural-networks.md)
16. Blocks and Fuel - Frameworks for deep learning
17. [Theano - new features and speed improvements](2012-theano-new-features-and-speed-improvements.md)
18. [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](2014-exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks.md)
19. A Direct Search Optimization Method That Models the Objective and Constraint Functions by Linear Interpolation
20. “Cloze Procedure” - A New Tool for Measuring Readability
