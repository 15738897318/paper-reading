---
title: Unified Vision-Language Pre-Training for Image Captioning and VQA
authors:
- Luowei Zhou
- H. Palangi
- Lei Zhang
- Houdong Hu
- Jason J. Corso
- Jianfeng Gao
fieldsOfStudy:
- Computer Science
meta_key: 2020-unified-vision-language-pre-training-for-image-captioning-and-vqa
numCitedBy: 363
reading_status: TBD
ref_count: 48
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Unified-Vision-Language-Pre-Training-for-Image-and-Zhou-Palangi/6648b4db5f12c30941ea78c695e77aded19672bb?sort=total-citations
venue: AAAI
year: 2020
---

# Unified Vision-Language Pre-Training for Image Captioning and VQA

## Abstract

This paper presents a unified Vision-Language Pre-training (VLP) model. The model is unified in that (1) it can be fine-tuned for either vision-language generation (e.g., image captioning) or understanding (e.g., visual question answering) tasks, and (2) it uses a shared multi-layer transformer network for both encoding and decoding, which differs from many existing methods where the encoder and decoder are implemented using separate models. The unified VLP model is pre-trained on a large amount of image-text pairs using the unsupervised learning objectives of two tasks: bidirectional and sequence-to-sequence (seq2seq) masked vision-language prediction. The two tasks differ solely in what context the prediction conditions on. This is controlled by utilizing specific self-attention masks for the shared transformer network. To the best of our knowledge, VLP is the first reported model that achieves state-of-the-art results on both vision-language generation and understanding tasks, as disparate as image captioning and visual question answering, across three challenging benchmark datasets: COCO Captions, Flickr30k Captions, and VQA 2.0. The code and the pre-trained models are available at https://github.com/LuoweiZhou/VLP.

## Paper References

1. [Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training](2020-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training)
2. [UNITER - UNiversal Image-TExt Representation Learning](2020-uniter-universal-image-text-representation-learning)
3. [Unified Language Model Pre-training for Natural Language Understanding and Generation](2019-unified-language-model-pre-training-for-natural-language-understanding-and-generation)
4. [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks)
5. [UNITER - Learning UNiversal Image-TExt Representations](2019-uniter-learning-universal-image-text-representations)
6. [LXMERT - Learning Cross-Modality Encoder Representations from Transformers](2019-lxmert-learning-cross-modality-encoder-representations-from-transformers)
7. [VideoBERT - A Joint Model for Video and Language Representation Learning](2019-videobert-a-joint-model-for-video-and-language-representation-learning)
8. [VL-BERT - Pre-training of Generic Visual-Linguistic Representations](2020-vl-bert-pre-training-of-generic-visual-linguistic-representations)
9. [VisualBERT - A Simple and Performant Baseline for Vision and Language](2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language)
10. [Neural Baby Talk](2018-neural-baby-talk)
11. [Language Models are Unsupervised Multitask Learners](2019-language-models-are-unsupervised-multitask-learners)
12. [Multi-Task Deep Neural Networks for Natural Language Understanding](2019-multi-task-deep-neural-networks-for-natural-language-understanding)
13. [Attention is All you Need](2017-transformer.md)
14. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
15. Exploring Visual Relationship for Image Captioning
16. End-to-End Dense Video Captioning with Masked Transformer
17. [Attention on Attention for Image Captioning](2019-attention-on-attention-for-image-captioning)
18. [Yin and Yang - Balancing and Answering Binary Visual Questions](2016-yin-and-yang-balancing-and-answering-binary-visual-questions)
19. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
20. Contrastive Bidirectional Transformer for Temporal Representation Learning
21. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
22. [Fusion of Detected Objects in Text for Visual Question Answering](2019-fusion-of-detected-objects-in-text-for-visual-question-answering)
23. Auto-Encoding Scene Graphs for Image Captioning
24. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
25. Grounded Video Description
26. [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations)
27. Learning Video Representations using Contrastive Bidirectional Transformer
28. Don't Just Assume; Look and Answer - Overcoming Priors for Visual Question Answering
29. [Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering](2019-dynamic-fusion-with-intra-and-inter-modality-attention-flow-for-visual-question-answering)
30. [Self-Critical Sequence Training for Image Captioning](2017-self-critical-sequence-training-for-image-captioning)
31. [Aggregated Residual Transformations for Deep Neural Networks](2017-aggregated-residual-transformations-for-deep-neural-networks)
32. [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning)
33. Pythia v0.1 - the Winning Entry to the VQA Challenge 2018
34. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks)
35. [Bilinear Attention Networks](2018-bilinear-attention-networks)
36. [RoBERTa - A Robustly Optimized BERT Pretraining Approach](2019-roberta-a-robustly-optimized-bert-pretraining-approach)
37. [Microsoft COCO Captions - Data Collection and Evaluation Server](2015-microsoft-coco-captions-data-collection-and-evaluation-server)
38. [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](2014-from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions)
39. Generative Question Answering - Learning to Answer the Whole Question
40. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
