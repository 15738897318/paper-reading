---
title: Deep Pyramidal Residual Networks
authors:
- Dongyoon Han
- Jiwhan Kim
- Junmo Kim
fieldsOfStudy:
- Computer Science
meta_key: 2017-deep-pyramidal-residual-networks
numCitedBy: 459
reading_status: TBD
ref_count: 44
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Deep-Pyramidal-Residual-Networks-Han-Kim/5bdf07c9897ca70788fff61dec56178a2bd0c29c?sort=total-citations
venue: 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2017
---

# Deep Pyramidal Residual Networks

## Abstract

Deep convolutional neural networks (DCNNs) have shown remarkable performance in image classification tasks in recent years. Generally, deep neural network architectures are stacks consisting of a large number of convolutional layers, and they perform downsampling along the spatial dimension via pooling to reduce memory usage. Concurrently, the feature map dimension (i.e., the number of channels) is sharply increased at downsampling locations, which is essential to ensure effective performance because it increases the diversity of high-level attributes. This also applies to residual networks and is very closely related to their performance. In this research, instead of sharply increasing the feature map dimension at units that perform downsampling, we gradually increase the feature map dimension at all units to involve as many locations as possible. This design, which is discussed in depth together with our new insights, has proven to be an effective means of improving generalization ability. Furthermore, we propose a novel residual unit capable of further improving the classification accuracy with our new network architecture. Experiments on benchmark CIFAR-10, CIFAR-100, and ImageNet datasets have shown that our network architecture has superior generalization ability compared to the original residual networks. Code is available at https://github.com/jhkim89/PyramidNet.

## Paper References

1. [Deeply-Supervised Nets](2015-deeply-supervised-nets.md)
2. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks.md)
3. [Wide Residual Networks](2016-wide-residual-networks.md)
4. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
5. [Deep Networks with Stochastic Depth](2016-deep-networks-with-stochastic-depth.md)
6. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
7. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
8. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
9. [Network In Network](2014-network-in-network.md)
10. [Striving for Simplicity - The All Convolutional Net](2015-striving-for-simplicity-the-all-convolutional-net.md)
11. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](2017-inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning.md)
12. Weighted residuals for very deep networks
13. [DeCAF - A Deep Convolutional Activation Feature for Generic Visual Recognition](2014-decaf-a-deep-convolutional-activation-feature-for-generic-visual-recognition.md)
14. [Rethinking the Inception Architecture for Computer Vision](2016-rethinking-the-inception-architecture-for-computer-vision.md)
15. [Visualizing and Understanding Convolutional Networks](2014-visualizing-and-understanding-convolutional-networks.md)
16. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
17. [Fully Convolutional Networks for Semantic Segmentation](2017-fully-convolutional-networks-for-semantic-segmentation.md)
18. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
19. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
20. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks.md)
21. [Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation](2014-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.md)
22. [FitNets - Hints for Thin Deep Nets](2015-fitnets-hints-for-thin-deep-nets.md)
23. Residual Networks Behave Like Ensembles of Relatively Shallow Networks
24. Residual Networks are Exponential Ensembles of Relatively Shallow Networks
25. [OverFeat - Integrated Recognition, Localization and Detection using Convolutional Networks](2014-overfeat-integrated-recognition-localization-and-detection-using-convolutional-networks.md)
26. Fractional Max-Pooling
27. Swapout - Learning an ensemble of deep architectures
28. [Training Very Deep Networks](2015-training-very-deep-networks.md)
29. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
30. [FractalNet - Ultra-Deep Neural Networks without Residuals](2017-fractalnet-ultra-deep-neural-networks-without-residuals.md)
31. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge.md)
32. [GradientBased Learning Applied to Document Recognition](2001-gradientbased-learning-applied-to-document-recognition.md)
33. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition.md)
34. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines.md)
35. Backpropagation Applied to Handwritten Zip Code Recognition
36. [Torch7 - A Matlab-like Environment for Machine Learning](2011-torch7-a-matlab-like-environment-for-machine-learning.md)
