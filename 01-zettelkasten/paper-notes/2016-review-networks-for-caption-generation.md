---
title: Review Networks for Caption Generation
authors:
- Zhilin Yang
- Ye Yuan
- Yuexin Wu
- William W. Cohen
- R. Salakhutdinov
fieldsOfStudy:
- Computer Science
meta_key: 2016-review-networks-for-caption-generation
numCitedBy: 213
reading_status: TBD
ref_count: 23
tags:
- gen-from-ref
- paper
venue: NIPS
year: 2016
---

# Review Networks for Caption Generation

## Abstract

We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder- decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of- the-art encoder-decoder systems on the tasks of image captioning and source code captioning.

## Paper References

1. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
2. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator)
3. [End-To-End Memory Networks](2015-end-to-end-memory-networks)
4. [From captions to visual concepts and back](2015-from-captions-to-visual-concepts-and-back)
5. [Image Captioning with Semantic Attention](2016-image-captioning-with-semantic-attention)
6. [A Neural Attention Model for Abstractive Sentence Summarization](2015-a-neural-attention-model-for-abstractive-sentence-summarization)
7. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
8. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
9. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition)
10. [Effective Approaches to Attention-based Neural Machine Translation](2015-effective-approaches-to-attention-based-neural-machine-translation)
11. [Rethinking the Inception Architecture for Computer Vision](2016-rethinking-the-inception-architecture-for-computer-vision)
12. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
13. Order Matters - Sequence to sequence for sets
14. [Microsoft COCO Captions - Data Collection and Evaluation Server](2015-microsoft-coco-captions-data-collection-and-evaluation-server)
15. Adaptive Computation Time for Recurrent Neural Networks
16. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization)
17. Structured Generative Models of Natural Source Code
18. Natural Language Models for Predicting Programming Comments
19. [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](2016-ask-me-anything-dynamic-memory-networks-for-natural-language-processing)
20. [Memory Networks](2015-memory-networks)
