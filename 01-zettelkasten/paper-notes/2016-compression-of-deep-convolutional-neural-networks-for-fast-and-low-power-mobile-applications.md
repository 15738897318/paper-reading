---
title: Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications
authors:
- Yong-Deok Kim
- Eunhyeok Park
- S. Yoo
- Taelim Choi
- Lu Yang
- Dongjun Shin
fieldsOfStudy:
- Computer Science
meta_key: 2016-compression-of-deep-convolutional-neural-networks-for-fast-and-low-power-mobile-applications
numCitedBy: 672
reading_status: TBD
ref_count: 47
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Compression-of-Deep-Convolutional-Neural-Networks-Kim-Park/4ca3b996d888d7178dbbf9855bb2ab253bdfa43d?sort=total-citations
venue: ICLR
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/Compression-of-Deep-Convolutional-Neural-Networks-Kim-Park/4ca3b996d888d7178dbbf9855bb2ab253bdfa43d?sort=total-citations)

# Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications

## Abstract

Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.

## Paper References

1. Compressing Deep Convolutional Networks using Vector Quantization
2. A Deep Neural Network Compression Pipeline - Pruning, Quantization, Huffman Encoding
3. Speeding-up Convolutional Neural Networks Using Fine-tuned CP-Decomposition
4. [Speeding up Convolutional Neural Networks with Low Rank Expansions](2014-speeding-up-convolutional-neural-networks-with-low-rank-expansions.md)
5. [Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding](2016-deep-compression-compressing-deep-neural-network-with-pruning-trained-quantization-and-huffman-coding.md)
6. Improving the speed of neural networks on CPUs
7. [Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation](2014-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.md)
8. Fast Training of Convolutional Networks through FFTs
9. [Accelerating Very Deep Convolutional Networks for Classification and Detection](2016-accelerating-very-deep-convolutional-networks-for-classification-and-detection.md)
10. Tensorizing Neural Networks
11. An Exploration of Parameter Redundancy in Deep Networks with Circulant Projections
12. Efficient and accurate approximations of nonlinear convolutional networks
13. Compressing Neural Networks with the Hashing Trick
14. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
15. High Performance Convolutional Neural Networks for Document Processing
16. [Learning both Weights and Connections for Efficient Neural Network](2015-learning-both-weights-and-connections-for-efficient-neural-network.md)
17. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
18. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
19. Fast Neural Networks with Circulant Projections
20. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
21. [Network In Network](2014-network-in-network.md)
22. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks.md)
23. [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](2016-under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
24. [Predicting Parameters in Deep Learning](2013-predicting-parameters-in-deep-learning.md)
25. Tensor Decompositions and Applications
26. Generalized low rank approximations of matrices
27. Non-negative tensor factorization with applications to statistics and computer vision
28. Perfect Dimensionality Recovery by Variational Bayesian PCA
29. Nonnegative Tucker Decomposition
30. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
31. Tensor Rank and the Ill-Posedness of the Best Low-Rank Approximation Problem
32. Global analytic solution of fully-observed variational Bayesian matrix factorization
33. Automatic relevance determination for multi‐way models
34. PARAFAC - parallel factor analysis
35. [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](2018-published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks.md)
36. Bayesian Interpolation
37. A Multilinear Singular Value Decomposition
38. Analysis of individual differences in multidimensional scaling via an n-way generalization of “Eckart-Young” decomposition
39. Some mathematical notes on three-mode factor analysis
40. [Sparse Bayesian Learning and the Relevance Vector Machine](2001-sparse-bayesian-learning-and-the-relevance-vector-machine.md)
