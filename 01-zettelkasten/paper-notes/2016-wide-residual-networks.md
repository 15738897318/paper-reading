---
title: Wide Residual Networks
authors:
- Sergey Zagoruyko
- N. Komodakis
fieldsOfStudy:
- Computer Science
meta_key: 2016-wide-residual-networks
numCitedBy: 4354
reading_status: TBD
ref_count: 41
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Wide-Residual-Networks-Zagoruyko-Komodakis/1c4e9156ca07705531e45960b7a919dc473abb51?sort=total-citations
venue: BMVC
year: 2016
---

# Wide Residual Networks

## Abstract

Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL

## Paper References

1. [Deep Networks with Stochastic Depth](2016-deep-networks-with-stochastic-depth.md)
2. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
3. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](2017-inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning.md)
4. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks.md)
5. [FitNets - Hints for Thin Deep Nets](2015-fitnets-hints-for-thin-deep-nets.md)
6. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
7. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks.md)
8. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
9. [Deeply-Supervised Nets](2015-deeply-supervised-nets.md)
10. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
11. On the complexity of shallow and deep neural network classifiers
12. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
13. On the Number of Linear Regions of Deep Neural Networks
14. Deep Learning Made Easier by Linear Transformations in Perceptrons
15. [On the importance of initialization and momentum in deep learning](2013-on-the-importance-of-initialization-and-momentum-in-deep-learning.md)
16. An empirical evaluation of deep architectures on problems with many factors of variation
17. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
18. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
19. [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](2016-fast-and-accurate-deep-network-learning-by-exponential-linear-units-elus.md)
20. Fractional Max-Pooling
21. [Maxout Networks](2013-maxout-networks.md)
22. Scaling learning algorithms towards AI
23. Net2Net - Accelerating Learning via Knowledge Transfer
24. A MultiPath Network for Object Detection
25. Learning Complex, Extended Sequences Using the Principle of History Compression
26. LocNet - Improving Localization Accuracy for Object Detection
27. [Torch7 - A Matlab-like Environment for Machine Learning](2011-torch7-a-matlab-like-environment-for-machine-learning.md)
