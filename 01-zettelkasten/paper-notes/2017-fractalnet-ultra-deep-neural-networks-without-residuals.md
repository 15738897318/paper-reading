---
title: FractalNet - Ultra-Deep Neural Networks without Residuals
authors:
- Gustav Larsson
- M. Maire
- Gregory Shakhnarovich
fieldsOfStudy:
- Computer Science
meta_key: 2017-fractalnet-ultra-deep-neural-networks-without-residuals
numCitedBy: 639
reading_status: TBD
ref_count: 50
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/FractalNet:-Ultra-Deep-Neural-Networks-without-Larsson-Maire/d0156126edbfc524c8d108bdc0cf811cfe3129aa?sort=total-citations
venue: ICLR
year: 2017
---

# FractalNet - Ultra-Deep Neural Networks without Residuals

## Abstract

We introduce a design strategy for neural network macro-architecture based on self-similarity. Repeated application of a simple expansion rule generates deep networks whose structural layouts are precisely truncated fractals. These networks contain interacting subpaths of different lengths, but do not include any pass-through or residual connections; every internal signal is transformed by a filter and nonlinearity before being seen by subsequent layers. In experiments, fractal networks match the excellent performance of standard residual networks on both CIFAR and ImageNet classification tasks, thereby demonstrating that residual representations may not be fundamental to the success of extremely deep convolutional neural networks. Rather, the key may be the ability to transition, during training, from effectively shallow to deep. We note similarities with student-teacher behavior and develop drop-path, a natural extension of dropout, to regularize co-adaptation of subpaths in fractal architectures. Such regularization allows extraction of high-performance fixed-depth subnetworks. Additionally, fractal networks exhibit an anytime property: shallow subnetworks provide a quick answer, while deeper subnetworks, with higher latency, provide a more accurate answer.

## Paper References

1. [Network In Network](2014-network-in-network.md)
2. [Wide Residual Networks](2016-wide-residual-networks.md)
3. Residual Networks Behave Like Ensembles of Relatively Shallow Networks
4. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks.md)
5. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks.md)
6. [Deep Networks with Stochastic Depth](2016-deep-networks-with-stochastic-depth.md)
7. Highway and Residual Networks learn Unrolled Iterative Estimation
8. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks.md)
9. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
10. Recurrent convolutional neural network for object recognition
11. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
12. [Striving for Simplicity - The All Convolutional Net](2015-striving-for-simplicity-the-all-convolutional-net.md)
13. [Highway Networks](2015-highway-networks.md)
14. [FitNets - Hints for Thin Deep Nets](2015-fitnets-hints-for-thin-deep-nets.md)
15. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
16. [Regularization of Neural Networks using DropConnect](2013-regularization-of-neural-networks-using-dropconnect.md)
17. [Generalizing Pooling Functions in Convolutional Neural Networks - Mixed, Gated, and Tree](2016-generalizing-pooling-functions-in-convolutional-neural-networks-mixed-gated-and-tree.md)
18. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
19. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
20. Fractional Max-Pooling
21. [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](2016-fast-and-accurate-deep-network-learning-by-exponential-linear-units-elus.md)
22. Competitive Multi-scale Convolution
23. [Deeply-Supervised Nets](2015-deeply-supervised-nets.md)
24. A Simple Way to Initialize Recurrent Networks of Rectified Linear Units
25. All you need is a good init
26. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
27. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
28. Scalable Bayesian Optimization Using Deep Neural Networks
29. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
30. [Hypercolumns for object segmentation and fine-grained localization](2015-hypercolumns-for-object-segmentation-and-fine-grained-localization.md)
31. [SqueezeNet - AlexNet-level accuracy with 50x fewer parameters and <1MB model size](2016-squeezenet-alexnet-level-accuracy-with-50x-fewer-parameters-and-1mb-model-size.md)
32. Do Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?
33. [Caffe - Convolutional Architecture for Fast Feature Embedding](2014-caffe-convolutional-architecture-for-fast-feature-embedding.md)
34. Do Deep Convolutional Nets Really Need to be Deep and Convolutional?
35. [Reading Digits in Natural Images with Unsupervised Feature Learning](2011-reading-digits-in-natural-images-with-unsupervised-feature-learning.md)
36. Reconstructive Sparse Code Transfer for Contour Detection and Semantic Labeling
37. Path-SGD - Path-Normalized Optimization in Deep Neural Networks
38. Do Deep Nets Really Need to be Deep?
39. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines.md)
40. Resnet in Resnet - Generalizing Residual Architectures
41. [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](2018-published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks.md)
42. Et al
