---
title: Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS
authors:
- A. Bosselut
- Omer Levy
- Ari Holtzman
- C. Ennis
- D. Fox
- Yejin Choi
fieldsOfStudy:
- Computer Science
meta_key: 2018-published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks
numCitedBy: 156
reading_status: TBD
ref_count: 30
tags:
- gen-from-ref
- other-default
- paper
venue: ''
year: 2018
---

# Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS

## Abstract

Understanding procedural language requires anticipating the causal effects of actions, even when they are not explicitly stated. In this work, we introduce Neural Process Networks to understand procedural text through (neural) simulation of action dynamics. Our model complements existing memory architectures with dynamic entity tracking by explicitly modeling actions as state transformers. The model updates the states of the entities by executing learned action operators. Empirical results demonstrate that our proposed model can reason about the unstated causal effects of actions, allowing it to provide more accurate contextual information for understanding and generating procedural text, all while offering more interpretable internal representations than existing alternatives.

## Paper References

1. Mise en Place - Unsupervised Interpretation of Instructional Recipes
2. [End-To-End Memory Networks](2015-end-to-end-memory-networks)
3. Jointly Learning Grounded Task Structures from Language Instruction and Visual Demonstration
4. Tracking the World State with Recurrent Entity Networks
5. Action-Conditional Video Prediction using Deep Networks in Atari Games
6. Reference-Aware Language Models
7. Query-Reduction Networks for Question Answering
8. [The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations](2016-the-goldilocks-principle-reading-children-s-books-with-explicit-memory-representations)
9. Physical Causality of Action Verbs in Grounded Language Understanding
10. Dynamic Entity Representations in Neural Language Models
11. Learning a Natural Language Interface with Neural Programmer
12. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
13. Adversarial Examples for Evaluating Reading Comprehension Systems
14. Modeling Coverage for Neural Machine Translation
15. Globally Coherent Text Generation with Neural Checklist Models
16. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
17. From Pixels to Torques - Policy Learning with Deep Dynamical Models
18. Unsupervised learning of event AND-OR grammar and semantics from video
19. Key-Value Memory Networks for Directly Reading Documents
20. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
21. A Machine Learning Approach to Recipe Text Processing
22. Are Distributional Representations Ready for the Real World? Evaluating Word Vectors for Grounded Perceptual Meaning
23. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
24. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
25. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting)
26. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
27. Do Supervised Distributional Methods Really Learn Lexical Inference Relations?
28. Flow Graph Corpus from Recipe Texts
29. Neural Programmer - Inducing Latent Programs with Gradient Descent
