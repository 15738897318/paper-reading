---
title: Generating Sequences With Recurrent Neural Networks
authors:
- A. Graves
fieldsOfStudy:
- Computer Science
meta_key: 2013-generating-sequences-with-recurrent-neural-networks
numCitedBy: 3163
reading_status: TBD
ref_count: 36
tags:
- gen-from-ref
- paper
venue: ArXiv
year: 2013
---

# Generating Sequences With Recurrent Neural Networks

## Abstract

This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.

## Paper References

1. Learning long-term dependencies with gradient descent is difficult
2. [Speech recognition with deep recurrent neural networks](2013-speech-recognition-with-deep-recurrent-neural-networks)
3. [Generating Text with Recurrent Neural Networks](2011-generating-text-with-recurrent-neural-networks)
4. Sequence Transduction with Recurrent Neural Networks
5. Learning Precise Timing with LSTM Recurrent Networks
6. SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
7. [Long Short-Term Memory](1997-long-short-term-memory)
8. Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks
9. Modeling Temporal Dependencies in High-Dimensional Sequences - Application to Polyphonic Music Generation and Transcription
10. [Framewise phoneme classification with bidirectional LSTM and other neural network architectures](2005-framewise-phoneme-classification-with-bidirectional-lstm-and-other-neural-network-architectures)
11. A First Look at Music Composition using LSTM Recurrent Neural Networks
12. Statistical Language Models Based on Neural Networks
13. Better Generative Models for Sequential Data Problems - Bidirectional Recurrent Mixture Density Networks
14. The Recurrent Temporal Restricted Boltzmann Machine
15. [Neural Networks for Pattern Recognition](1993-neural-networks-for-pattern-recognition)
16. A fast and simple algorithm for training neural probabilistic language models
17. A Scalable Hierarchical Distributed Language Model
18. [Practical Variational Inference for Neural Networks](2011-practical-variational-inference-for-neural-networks)
19. An analysis of noise in recurrent neural networks - convergence and generalization
20. IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard
21. Building a Large Annotated Corpus of English - The Penn Treebank
22. Low-rank matrix factorization for Deep Neural Network training with high-dimensional output targets
23. Data Compression Using Adaptive Coding and Partial String Matching
24. Factored conditional restricted Boltzmann Machines for modeling motion style
25. A Practical Guide to Training Restricted Boltzmann Machines
26. Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
27. A Machine Learning Perspective on Predictive Coding with PAQ8
28. Mixture Density Networks
29. The Minimum Description Length Principle (Adaptive Computation and Machine Learning)
30. The tagged LOB Corpus - user's manual
31. Gradient-based learning algorithms for recurrent networks and their computational complexity
