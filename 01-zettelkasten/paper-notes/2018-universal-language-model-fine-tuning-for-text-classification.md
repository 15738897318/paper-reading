---
title: Universal Language Model Fine-tuning for Text Classification
authors:
- Jeremy Howard
- Sebastian Ruder
fieldsOfStudy:
- Computer Science
meta_key: 2018-universal-language-model-fine-tuning-for-text-classification
numCitedBy: 2283
reading_status: TBD
ref_count: 57
tags:
- gen-from-ref
- paper
venue: ACL
year: 2018
---

# Universal Language Model Fine-tuning for Text Classification

## Abstract

Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.

## Paper References

1. Semi-supervised sequence tagging with bidirectional language models
2. Semi-supervised Multitask Learning for Sequence Labeling
3. Question Answering through Transfer Learning from Large Fine-grained Supervision Data
4. Empower Sequence Labeling with Task-Aware Neural Language Model
5. [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](2017-supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data)
6. [Learned in Translation - Contextualized Word Vectors](2017-learned-in-translation-contextualized-word-vectors)
7. Supervised and Semi-Supervised Text Categorization using LSTM for Region Embeddings
8. Improving Neural Machine Translation Models with Monolingual Data
9. Regularizing and Optimizing LSTM Language Models
10. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
11. Pointer Sentinel Mixture Models
12. Adversarial Training Methods for Semi-Supervised Text Classification
13. Deep Biaffine Attention for Neural Dependency Parsing
14. Exploring the Limits of Weakly Supervised Pretraining
15. Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings
16. Text Classification Improved by Integrating Bidirectional LSTM with Two-dimensional Max Pooling
17. Biographies, Bollywood, Boom-boxes and Blenders - Domain Adaptation for Sentiment Classification
18. Learning to Generate Reviews and Discovering Sentiment
19. What makes ImageNet good for transfer learning?
20. Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies
21. UNITN - Training Deep Convolutional Neural Network for Twitter Sentiment Classification
22. [Character-level Convolutional Networks for Text Classification](2015-character-level-convolutional-networks-for-text-classification)
23. Deep Pyramid Convolutional Neural Networks for Text Categorization
24. [Learning Word Vectors for Sentiment Analysis](2011-learning-word-vectors-for-sentiment-analysis)
25. Discriminative Neural Sentence Modeling by Tree-Based Convolution
26. [DeCAF - A Deep Convolutional Activation Feature for Generic Visual Recognition](2014-decaf-a-deep-convolutional-activation-feature-for-generic-visual-recognition)
27. A Survey on Transfer Learning
28. Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm
29. [Fully Convolutional Networks for Semantic Segmentation](2017-fully-convolutional-networks-for-semantic-segmentation)
30. Learning Transferable Features with Deep Adaptation Networks
31. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition)
32. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
33. Why Does Unsupervised Pre-training Help Deep Learning?
34. Multitask Learning - A Knowledge-Based Source of Inductive Bias
35. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift)
36. GradNorm - Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks
37. Colorless Green Recurrent Networks Dream Hierarchically
38. [CNN Features Off-the-Shelf - An Astounding Baseline for Recognition](2014-cnn-features-off-the-shelf-an-astounding-baseline-for-recognition)
39. [Hypercolumns for object segmentation and fine-grained localization](2015-hypercolumns-for-object-segmentation-and-fine-grained-localization)
40. How transferable are features in deep neural networks?
41. The TREC-8 Question Answering Track Evaluation
42. A Model of Inductive Bias Learning
43. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks)
44. Cyclical Learning Rates for Training Neural Networks
45. Deep Boltzmann Machines
46. [Greedy Layer-Wise Training of Deep Networks](2006-greedy-layer-wise-training-of-deep-networks)
47. Super-convergence - very fast training of neural networks using large learning rates
48. SGDR - Stochastic Gradient Descent with Warm Restarts
49. Document categorization in legal electronic discovery - computer classification vs. manual review
50. Detecting Automation of Twitter Accounts - Are You a Human, Bot, or Cyborg?
51. Review spam detection
52. Classifying text messages for the haiti earthquake
53. An overview of gradient descent optimization algorithms
54. The application of data mining techniques in financial fraud detection - A classification framework and an academic review of literature
55. Estimation of Dependences Based on Empirical Data
56. How Transferable are Neural Networks in NLP Applications?
