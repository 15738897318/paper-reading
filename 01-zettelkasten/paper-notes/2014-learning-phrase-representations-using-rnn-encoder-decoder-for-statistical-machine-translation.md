---
title: Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation
authors:
- Kyunghyun Cho
- Bart van Merrienboer
- "\xC7aglar G\xFCl\xE7ehre"
- Dzmitry Bahdanau
- Fethi Bougares
- Holger Schwenk
- Yoshua Bengio
fieldsOfStudy:
- Computer Science
meta_key: 2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation
numCitedBy: 15156
reading_status: TBD
ref_count: 39
tags:
- gen-from-ref
- other-default
- paper
venue: EMNLP
year: 2014
---

# Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation

## Abstract

In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.

## Paper References

1. Joint Language and Translation Modeling with Recurrent Neural Networks
2. Learning Semantic Representations for the Phrase Translation Model
3. Continuous Space Translation Models with Neural Networks
4. Decoding with Large-Scale Neural Language Models Improves Translation
5. [Recurrent Continuous Translation Models](2013-recurrent-continuous-translation-models)
6. A Neural Probabilistic Language Model
7. [Statistical Phrase-Based Translation](2003-statistical-phrase-based-translation)
8. Continuous space language models
9. How to Construct Deep Recurrent Neural Networks
10. Fast and Robust Neural Network Joint Models for Statistical Machine Translation
11. [Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition](2012-context-dependent-pre-trained-deep-neural-networks-for-large-vocabulary-speech-recognition)
12. An Autoencoder Approach to Learning Bilingual Word Representations
13. Supervised Sequence Labelling with Recurrent Neural Networks
14. Bilingual Word Embeddings for Phrase-Based Machine Translation
15. Continuous space language models for the IWSLT 2006 task
16. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
17. Domain Adaptation via Pseudo In-Domain Data Selection
18. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
19. Intelligent Selection of Language Model Training Data
20. Advances in optimizing recurrent networks
21. [Long Short-Term Memory](1997-long-short-term-memory)
22. A Phrase-Based, Joint Probability Model for Statistical Machine Translation
23. Europarl - A Parallel Corpus for Statistical Machine Translation
24. [Theano - new features and speed improvements](2012-theano-new-features-and-speed-improvements)
25. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks)
26. [Maxout Networks](2013-maxout-networks)
27. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method)
28. [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](2014-exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks)
29. [Deep Sparse Rectifier Neural Networks](2011-deep-sparse-rectifier-neural-networks)
30. Barnes-Hut-SNE
31. Continuous Space Translation Models for Phrase-Based Statistical Machine Translation
