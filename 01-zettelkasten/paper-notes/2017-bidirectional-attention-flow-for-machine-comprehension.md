---
title: Bidirectional Attention Flow for Machine Comprehension
authors:
- Minjoon Seo
- Aniruddha Kembhavi
- Ali Farhadi
- Hannaneh Hajishirzi
fieldsOfStudy:
- Computer Science
meta_key: 2017-bidirectional-attention-flow-for-machine-comprehension
numCitedBy: 1726
reading_status: TBD
ref_count: 39
tags:
- gen-from-ref
- paper
venue: ICLR
year: 2017
---

# Bidirectional Attention Flow for Machine Comprehension

## Abstract

Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.

## Paper References

1. Gated-Attention Readers for Text Comprehension
2. Learning Recurrent Span Representations for Extractive Question Answering
3. Text Understanding with the Attention Sum Reader Network
4. Iterative Alternating Neural Attention for Machine Reading
5. Machine Comprehension Using Match-LSTM and Answer Pointer
6. End-to-End Answer Chunk Extraction and Ranking for Reading Comprehension
7. [Dynamic Memory Networks for Visual and Textual Question Answering](2016-dynamic-memory-networks-for-visual-and-textual-question-answering)
8. [Stacked Attention Networks for Image Question Answering](2016-stacked-attention-networks-for-image-question-answering)
9. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
10. ReasoNet - Learning to Stop Reading in Machine Comprehension
11. [Hierarchical Question-Image Co-Attention for Visual Question Answering](2016-hierarchical-question-image-co-attention-for-visual-question-answering)
12. MCTest - A Challenge Dataset for the Open-Domain Machine Comprehension of Text
13. A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task
14. Dynamic Entity Representation with Max-pooling Improves Machine Reading
15. [Ask, Attend and Answer - Exploring Question-Guided Spatial Attention for Visual Question Answering](2016-ask-attend-and-answer-exploring-question-guided-spatial-attention-for-visual-question-answering)
16. Natural Language Comprehension with the EpiReader
17. [Visual7W - Grounded Question Answering in Images](2016-visual7w-grounded-question-answering-in-images)
18. Words or Characters? Fine-grained Gating for Reading Comprehension
19. [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](2016-multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding)
20. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
21. Ask Your Neurons - A Neural-Based Approach to Answering Questions about Images
22. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend)
23. [The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations](2016-the-goldilocks-principle-reading-children-s-books-with-explicit-memory-representations)
24. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
25. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
26. [Convolutional Neural Networks for Sentence Classification](2014-convolutional-neural-networks-for-sentence-classification)
27. [Long Short-Term Memory](1997-long-short-term-memory)
28. [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](2018-published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks)
29. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting)
30. [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](2016-under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks)
31. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method)
32. [Memory Networks](2015-memory-networks)
33. End-to-End Reading Comprehension with Dynamic Answer Chunk Ranking
34. Dynamic Coattention Networks For Question Answering
35. Attention-over-Attention Neural Networks for Reading Comprehension
