---
title: XNOR-Net - ImageNet Classification Using Binary Convolutional Neural Networks
authors:
- Mohammad Rastegari
- Vicente Ordonez
- Joseph Redmon
- Ali Farhadi
fieldsOfStudy:
- Computer Science
meta_key: 2016-xnor-net-imagenet-classification-using-binary-convolutional-neural-networks
numCitedBy: 2591
reading_status: TBD
ref_count: 48
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/XNOR-Net:-ImageNet-Classification-Using-Binary-Rastegari-Ordonez/b649a98ce77ece8cd7638bb74ab77d22d9be77e7?sort=total-citations
venue: ECCV
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/XNOR-Net:-ImageNet-Classification-Using-Binary-Rastegari-Ordonez/b649a98ce77ece8cd7638bb74ab77d22d9be77e7?sort=total-citations)

# XNOR-Net - ImageNet Classification Using Binary Convolutional Neural Networks

## Abstract

We propose two efficient approximations to standard convolutional neural networks: Binary-Weight-Networks and XNOR-Networks. In Binary-Weight-Networks, the filters are approximated with binary values resulting in 32\(\times \) memory saving. In XNOR-Networks, both the filters and the input to convolutional layers are binary. XNOR-Networks approximate convolutions using primarily binary operations. This results in 58\(\times \) faster convolutional operations (in terms of number of the high precision operations) and 32\(\times \) memory savings. XNOR-Nets offer the possibility of running state-of-the-art networks on CPUs (rather than GPUs) in real-time. Our binary networks are simple, accurate, efficient, and work on challenging visual tasks. We evaluate our approach on the ImageNet classification task. The classification accuracy with a Binary-Weight-Network version of AlexNet is the same as the full-precision AlexNet. We compare our method with recent network binarization methods, BinaryConnect and BinaryNets, and outperform these methods by large margins on ImageNet, more than \(16\,\%\) in top-1 accuracy. Our code is available at: http://allenai.org/plato/xnornet.

## Paper References

1. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
2. Fixed point optimization of deep convolutional neural networks for object recognition
3. [BinaryNet - Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](2016-binarynet-training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or-1.md)
4. [BinaryConnect - Training Deep Neural Networks with binary weights during propagations](2015-binaryconnect-training-deep-neural-networks-with-binary-weights-during-propagations.md)
5. Compressing Deep Convolutional Networks using Vector Quantization
6. Bitwise Neural Networks
7. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
8. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
9. [Speeding up Convolutional Neural Networks with Low Rank Expansions](2014-speeding-up-convolutional-neural-networks-with-low-rank-expansions.md)
10. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.md)
11. [Network In Network](2014-network-in-network.md)
12. [Fully Convolutional Networks for Semantic Segmentation](2017-fully-convolutional-networks-for-semantic-segmentation.md)
13. [Learning both Weights and Connections for Efficient Neural Network](2015-learning-both-weights-and-connections-for-efficient-neural-network.md)
14. Improving the speed of neural networks on CPUs
15. [Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding](2016-deep-compression-compressing-deep-neural-network-with-pruning-trained-quantization-and-huffman-coding.md)
16. Neural Networks with Few Multiplications
17. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
18. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
19. Training deep neural networks with low precision multiplications
20. [Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation](2014-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.md)
21. [Fast R-CNN](2015-fast-r-cnn.md)
22. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](2017-inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning.md)
23. [Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation](2014-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.md)
24. [SqueezeNet - AlexNet-level accuracy with 50x fewer parameters and <1MB model size](2016-squeezenet-alexnet-level-accuracy-with-50x-fewer-parameters-and-1mb-model-size.md)
25. Fixed-point feedforward deep neural network design using weights +1, 0, and âˆ’1
26. Compressing Neural Networks with the Hashing Trick
27. Do Deep Nets Really Need to be Deep?
28. Backpropagation for Energy-Efficient Neuromorphic Computing
29. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
30. [Regularization of Neural Networks using DropConnect](2013-regularization-of-neural-networks-using-dropconnect.md)
31. Expectation Backpropagation - Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights
32. Cross-Domain Synthesis of Medical Images Using Efficient Location-Sensitive Deep Network
33. Second Order Derivatives for Network Pruning - Optimal Brain Surgeon
34. Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses.
35. Provable Bounds for Learning Some Deep Representations
36. Comparing Biases for Minimal Network Construction with Back-Propagation
37. Big Neural Networks Waste Capacity
38. Approximation by superpositions of a sigmoidal function
39. [Predicting Parameters in Deep Learning](2013-predicting-parameters-in-deep-learning.md)
40. Optimal Brain Damage
41. Conversational Speech Transcription Using Context-Dependent Deep Neural Networks
42. In Advances in Neural Information Processing Systems
43. Merging Reality and Virtuality with Microsoft HoloLens
