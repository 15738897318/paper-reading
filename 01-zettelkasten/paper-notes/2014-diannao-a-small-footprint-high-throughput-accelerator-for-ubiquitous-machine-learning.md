---
title: DianNao - a small-footprint high-throughput accelerator for ubiquitous machine-learning
authors:
- Tianshi Chen
- Zidong Du
- Ninghui Sun
- Jia Wang
- Chengyong Wu
- Yunji Chen
- O. Temam
fieldsOfStudy:
- Computer Science
meta_key: 2014-diannao-a-small-footprint-high-throughput-accelerator-for-ubiquitous-machine-learning
numCitedBy: 1235
reading_status: TBD
ref_count: 47
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/DianNao:-a-small-footprint-high-throughput-for-Chen-Du/22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd?sort=total-citations
venue: ASPLOS
year: 2014
---

# DianNao - a small-footprint high-throughput accelerator for ubiquitous machine-learning

## Abstract

Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope. Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy. We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.

## Paper References

1. Deep learning with COTS HPC systems
2. Improving the speed of neural networks on CPUs
3. BenchNN - On the broad potential application scope of hardware neural network accelerators
4. Leveraging the error resilience of machine-learning applications for designing highly energy efficient accelerators
5. Bridging the computation gap between programmable processors and hardwired accelerators
6. A defect-tolerant accelerator for emerging high-performance applications
7. A dynamically configurable coprocessor for convolutional neural networks
8. A digital neurosynaptic core using embedded crossbar memory with 45pJ per spike in 45nm
9. Understanding sources of inefficiency in general-purpose chips
10. Towards Hardware Acceleration of Neuroevolution for Multimedia Processing Applications on Mobile Devices
11. A case for neuromorphic ISAs
12. Convolution engine - balancing efficiency & flexibility in specialized computing
13. [Building high-level features using large scale unsupervised learning](2013-building-high-level-features-using-large-scale-unsupervised-learning.md)
14. Low-power, high-performance analog neural branch prediction
15. NeuFlow - A runtime reconfigurable dataflow processor for vision
16. Accelerating neuromorphic vision algorithms for recognition
17. SpiNNaker - Mapping neural networks onto a massively-parallel chip multiprocessor
18. McPAT - An integrated power, area, and timing modeling framework for multicore and manycore architectures
19. On the capabilities of neural networks using limited precision weights
20. A 201.4 GOPS 496 mW Real-Time Multi-Object Recognition Processor With Bio-Inspired Neural Perception Engine
21. Neural Acceleration for General-Purpose Approximate Programs
22. Wafer-scale integration of analog neural networks
23. Finite Precision Error Analysis of Neural Network Hardware Implementations
24. An empirical evaluation of deep architectures on problems with many factors of variation
25. Traffic sign recognition with multi-scale Convolutional Networks
26. Dynamically Reconfigurable Silicon Array of Spiking Neurons With Conductance-Based Synapses
27. Reconciling specialization and flexibility through compound circuits
28. Convolutional neural networks applied to house numbers digit classification
29. [GradientBased Learning Applied to Document Recognition](2001-gradientbased-learning-applied-to-document-recognition.md)
30. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition.md)
31. [Improving deep neural networks for LVCSR using rectified linear units and dropout](2013-improving-deep-neural-networks-for-lvcsr-using-rectified-linear-units-and-dropout.md)
32. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
33. An electrically trainable artificial neural network (ETANN) with 10240 'floating gate' synapses
34. QSCORES - Trading dark silicon for scalable energy efficiency with quasi-specific cores
35. The PARSEC benchmark suite - Characterization and architectural implications
36. Dark Silicon and the End of Multicore Scaling
37. Robust Object Recognition with Cortex-Like Mechanisms
38. An Efficient Hardware Architecture for a Neural Network Activation Function Generator
39. [Support-Vector Networks](2004-support-vector-networks.md)
40. Learning to Label Aerial Images from Noisy Data
41. Learning deep structured semantic models for web search using clickthrough data
42. Data-dependent truncation scheme for parallel multipliers
