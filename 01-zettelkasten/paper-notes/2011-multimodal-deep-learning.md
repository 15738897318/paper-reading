---
title: Multimodal Deep Learning
authors:
- Jiquan Ngiam
- A. Khosla
- Mingyu Kim
- Juhan Nam
- Honglak Lee
- A. Ng
fieldsOfStudy:
- Computer Science
meta_key: 2011-multimodal-deep-learning
numCitedBy: 2447
reading_status: TBD
ref_count: 29
tags:
- gen-from-ref
- paper
venue: ICML
year: 2011
---

# Multimodal Deep Learning

## Abstract

Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.

## Paper References

1. Multimodal Fusion and Learning with Uncertain Features Applied to Audiovisual Speech Recognition
2. Adaptive Multimodal Fusion by Uncertainty Compensation With Application to Audiovisual Speech Recognition
3. Patch-Based Representation of Visual Speech
4. [Extracting and composing robust features with denoising autoencoders](2008-extracting-and-composing-robust-features-with-denoising-autoencoders)
5. Information Theoretic Feature Extraction for Audio-Visual Speech Recognition
6. CUAVE - A new audio-visual database for multimodal human-computer interface research
7. Sparse deep belief net model for visual area V2
8. Adaptive multimodal fusion by uncertainty compensation
9. See me, hear me - integrating automatic speech recognition and lip-reading
10. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
11. The challenge of multispeaker lip-reading
12. Lipreading With Local Spatiotemporal Descriptors
13. Extraction of Visual Features for Lipreading
14. Audio-Visual Automatic Speech Recognition - An Overview
15. Eigenlips for robust speech recognition
16. Adaptive bimodal sensor fusion for automatic speechreading
17. Integration of acoustic and visual speech signals using neural networks
18. Semantic hashing
19. [Reducing the Dimensionality of Data with Neural Networks](2006-reducing-the-dimensionality-of-data-with-neural-networks)
20. [Histograms of oriented gradients for human detection](2005-histograms-of-oriented-gradients-for-human-detection)
21. Hearing lips and seeing voices
22. [Training Products of Experts by Minimizing Contrastive Divergence](2002-training-products-of-experts-by-minimizing-contrastive-divergence)
23. Lipreading and audio-visual speech perception.
24. Canonical Correlation Analysis - An Overview with Application to Learning Methods
25. Self-taught learning - transfer learning from unlabeled data
26. [Multitask Learning](2004-multitask-learning)
