---
title: Using the Output Embedding to Improve Language Models
authors:
- Ofir Press
- Lior Wolf
fieldsOfStudy:
- Computer Science
meta_key: 2017-using-the-output-embedding-to-improve-language-models
numCitedBy: 574
reading_status: TBD
ref_count: 49
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Using-the-Output-Embedding-to-Improve-Language-Press-Wolf/63e39cdf1ad884da6bc69096bb3413b5b1100559?sort=total-citations
venue: EACL
year: 2017
---

[semanticscholar url](https://www.semanticscholar.org/paper/Using-the-Output-Embedding-to-Improve-Language-Press-Wolf/63e39cdf1ad884da6bc69096bb3413b5b1100559?sort=total-citations)

# Using the Output Embedding to Improve Language Models

## Abstract

We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.

## Paper References

1. [Tying Word Vectors and Word Classifiers - A Loss Framework for Language Modeling](2017-tying-word-vectors-and-word-classifiers-a-loss-framework-for-language-modeling.md)
2. [LSTM Neural Networks for Language Modeling](2012-lstm-neural-networks-for-language-modeling.md)
3. word2vec Explained - deriving Mikolov et al.'s negative-sampling word-embedding method
4. Learning word embeddings efficiently with noise-contrastive estimation
5. [Recurrent Continuous Translation Models](2013-recurrent-continuous-translation-models.md)
6. Gated Word-Character Recurrent Language Model
7. A Scalable Hierarchical Distributed Language Model
8. A fast and simple algorithm for training neural probabilistic language models
9. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks.md)
10. A Dual Embedding Space Model for Document Ranking
11. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model.md)
12. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space.md)
13. [Extensions of recurrent neural network language model](2011-extensions-of-recurrent-neural-network-language-model.md)
14. A Fixed-Size Encoding Method for Variable-Length Sequences with its Application to Neural Network Language Models
15. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
16. Better Word Representations with Recursive Neural Networks for Morphology
17. [Recurrent Highway Networks](2017-recurrent-highway-networks.md)
18. Edinburgh Neural Machine Translation Systems for WMT 16
19. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
20. [Neural Machine Translation of Rare Words with Subword Units](2016-neural-machine-translation-of-rare-words-with-subword-units.md)
21. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
22. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization.md)
23. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality.md)
24. Domain Adaptation via Pseudo In-Domain Data Selection
25. A Neural Probabilistic Language Model
26. Learning Longer Memory in Recurrent Neural Networks
27. [Learning Word Vectors for Sentiment Analysis](2011-learning-word-vectors-for-sentiment-analysis.md)
28. [Distilling the Knowledge in a Neural Network](2015-distilling-the-knowledge-in-a-neural-network.md)
29. How to Construct Deep Recurrent Neural Networks
30. [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](2016-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.md)
31. Large-scale learning of word relatedness with constraints
32. Achieving Human Parity in Conversational Speech Recognition
33. Building a Large Annotated Corpus of English - The Penn Treebank
34. Improving Neural Networks with Dropout
35. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks.md)
36. An Unsupervised Model for Instance Level Subcategorization Acquisition
37. Multimodal Distributional Semantics
38. [Long Short-Term Memory](1997-long-short-term-memory.md)
39. SimLex-999 - Evaluating Semantic Models With (Genuine) Similarity Estimation
40. Noisy Activation Functions
41. Practical solutions to the problem of diagonal dominance in kernel document clustering
42. Dropout as a Bayesian Approximation - Representing Model Uncertainty in Deep Learning
43. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method.md)
44. A new algorithm for data compression
45. A New Algorithm For Data Compression
