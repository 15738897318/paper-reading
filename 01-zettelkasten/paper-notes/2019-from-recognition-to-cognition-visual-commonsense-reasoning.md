---
title: From Recognition to Cognition - Visual Commonsense Reasoning
authors:
- Rowan Zellers
- Yonatan Bisk
- Ali Farhadi
- Yejin Choi
fieldsOfStudy:
- Computer Science
meta_key: 2019-from-recognition-to-cognition-visual-commonsense-reasoning
numCitedBy: 379
reading_status: TBD
ref_count: 98
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/From-Recognition-to-Cognition:-Visual-Commonsense-Zellers-Bisk/6dfc2ff03534a4325d06c6f88c3144831996629b?sort=total-citations
venue: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2019
---

# From Recognition to Cognition - Visual Commonsense Reasoning

## Abstract

Visual understanding goes well beyond object recognition. With one glance at an image, we can effortlessly imagine the world beyond the pixels: for instance, we can infer people's actions, goals, and mental states. While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense Reasoning. Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer. Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe for generating non-trivial and high-quality problems at scale is Adversarial Matching, a new approach to transform rich annotations into multiple choice questions with minimal bias. Experimental results show that while humans find VCR easy (over 90% accuracy), state-of-the-art vision models struggle (~45%). To move towards cognition-level understanding, we present a new reasoning engine, Recognition to Cognition Networks (R2C), that models the necessary layered inferences for grounding, contextualization, and reasoning. R2C helps narrow the gap between humans and machines (~65%); still, the challenge is far from solved, and we provide analysis that suggests avenues for future work.

## Paper References

1. Inferring the Why in Images
2. [Visual7W - Grounded Question Answering in Images](2016-visual7w-grounded-question-answering-in-images)
3. Learning to Act Properly - Predicting and Explaining Affordances from Images
4. [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations)
5. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
6. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
7. Overcoming Language Priors in Visual Question Answering with Adversarial Regularization
8. SWAG - A Large-Scale Adversarial Dataset for Grounded Commonsense Inference
9. Answering Visual What-If Questions - From Actions to Predicted Scene Descriptions
10. Generating Visual Explanations
11. Who Let the Dogs Out? Modeling Dog Behavior from Visual Data
12. Anticipating Visual Representations from Unlabeled Video
13. FVQA - Fact-Based Visual Question Answering
14. Grounding Visual Explanations
15. Predicting Motivations of Actions by Leveraging Text
16. [Ask Me Anything - Free-Form Visual Question Answering Based on Knowledge from External Sources](2016-ask-me-anything-free-form-visual-question-answering-based-on-knowledge-from-external-sources)
17. Visual Madlibs - Fill in the blank Image Generation and Question Answering
18. [Grounding of Textual Phrases in Images by Reconstruction](2016-grounding-of-textual-phrases-in-images-by-reconstruction)
19. Generating Descriptions with Grounded and Co-referenced People
20. Leveraging Visual Question Answering for Image-Caption Ranking
21. A Dataset and Exploration of Models for Understanding Video Data through Fill-in-the-Blank Question-Answering
22. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
23. Learning to Evaluate Image Captioning
24. What Happens If... Learning to Predict the Effect of Forces in Images
25. Textual Explanations for Self-Driving Vehicles
26. Explainable Neural Computation via Stack Neural Module Networks
27. [Unbiased look at dataset bias](2011-unbiased-look-at-dataset-bias)
28. Multimodal Explanations - Justifying Decisions and Pointing to the Evidence
29. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
30. Flickr30k Entities - Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models
31. Learning without Forgetting
32. Annotation Artifacts in Natural Language Inference Data
33. Do explanations make VQA models more predictable to a human?
34. A Corpus and Evaluation Framework for Deeper Understanding of Commonsense Stories
35. TVQA - Localized, Compositional Video Question Answering
36. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
37. [Deep Residual Learning for Image Recognition](2015-resnet.md)
38. MovieGraphs - Towards Understanding Human-Centric Situations from Videos
39. Men Also Like Shopping - Reducing Gender Bias Amplification using Corpus-level Constraints
40. TGIF - A New Dataset and Benchmark on Animated GIF Description
41. Phrase Localization and Visual Relationship Detection with Comprehensive Image-Language Cues
42. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference)
43. Towards Automatic Learning of Procedures From Web Instructional Videos
44. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn.md)
45. [Modeling Context in Referring Expressions](2016-modeling-context-in-referring-expressions)
46. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
47. MovieQA - Understanding Stories in Movies through Question-Answering
48. Interpretable Intuitive Physics Model
49. First-Person Activity Forecasting with Online Inverse Reinforcement Learning
50. Social GAN - Socially Acceptable Trajectories with Generative Adversarial Networks
51. Enhanced LSTM for Natural Language Inference
52. From Lifestyle Vlogs to Everyday Interactions
53. Temporal Perception and Prediction in Ego-Centric Video
54. Localizing Moments in Video with Natural Language
55. Learning Paraphrastic Sentence Embeddings from Back-Translated Bitext
56. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database)
57. The Effect of Different Writing Tasks on Linguistic Style - A Case Study of the ROC Story Cloze Task
58. AllenNLP - A Deep Semantic Natural Language Processing Platform
59. [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](2018-a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference)
60. The price of debiasing automatic metrics in natural language evalaution
61. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
62. Movie Description
63. Social LSTM - Human Trajectory Prediction in Crowded Spaces
64. A Joint Speaker-Listener-Reinforcer Model for Referring Expressions
65. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
66. Hypothesis Only Baselines in Natural Language Inference
67. What will Happen Next? Forecasting Player Moves in Sports Videos
68. [Long Short-Term Memory](1997-long-short-term-memory)
69. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
70. A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
71. [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](2014-exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks)
72. Exploring Nearest Neighbor Approaches for Image Captioning
73. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
74. STAIR Actions - A Video Dataset of Everyday Home Actions
75. Commonsense reasoning and commonsense knowledge in artificial intelligence
76. Explanation and Justification in Machine Learning - A Survey Or
77. Social Bias in Elicited Natural Language Inferences
78. Connotation Frames of Power and Agency in Modern Films
79. Gender-Distinguishing Features in Film Dialogue
80. Nematus - a Toolkit for Neural Machine Translation
81. A shortest augmenting path algorithm for dense and sparse linear assignment problems
82. Algorithms for the Assignment and Transportation Problems
83. Datasheets for datasets
84. [Mask R-CNN](2017-mask-r-cnn.md)
85. MUTAN - Multimodal Tucker Fusion for Visual Question Answering
86. TGIF-QA - Toward Spatio-Temporal Reasoning in Visual Question Answering
87. [Modeling Relationships in Referential Expressions with Compositional Modular Networks](2017-modeling-relationships-in-referential-expressions-with-compositional-modular-networks)
88. [Revisiting Visual Question Answering Baselines](2016-revisiting-visual-question-answering-baselines)
89. KrishnaCam - Using a longitudinal, single-person, egocentric dataset for scene understanding tasks
90. Unsupervised Learning from Narrated Instruction Videos
91. [Generation and Comprehension of Unambiguous Object Descriptions](2016-generation-and-comprehension-of-unambiguous-object-descriptions)
