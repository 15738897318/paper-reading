---
title: The State of Sparsity in Deep Neural Networks
authors:
- Trevor Gale
- Erich Elsen
- Sara Hooker
fieldsOfStudy:
- Computer Science
meta_key: 2019-the-state-of-sparsity-in-deep-neural-networks
numCitedBy: 362
reading_status: TBD
ref_count: 42
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/The-State-of-Sparsity-in-Deep-Neural-Networks-Gale-Elsen/26384278cf5d575fc32cb92c303fb648fa0d5217?sort=total-citations
venue: ArXiv
year: 2019
---

[semanticscholar url](https://www.semanticscholar.org/paper/The-State-of-Sparsity-in-Deep-Neural-Networks-Gale-Elsen/26384278cf5d575fc32cb92c303fb648fa0d5217?sort=total-citations)

# The State of Sparsity in Deep Neural Networks

## Abstract

We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle & Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.

## Paper References

1. To prune, or not to prune - exploring the efficacy of pruning for model compression
2. Memory Bounded Deep Convolutional Networks
3. [Wide Residual Networks](2016-wide-residual-networks.md)
4. [Learning Efficient Convolutional Networks through Network Slimming](2017-learning-efficient-convolutional-networks-through-network-slimming.md)
5. ThiNet - A Filter Level Pruning Method for Deep Neural Network Compression
6. Rethinking the Value of Network Pruning
7. Exploring Sparsity in Recurrent Neural Networks
8. Soft Weight-Sharing for Neural Network Compression
9. [Runtime Neural Pruning](2017-runtime-neural-pruning.md)
10. Compressing Neural Networks using the Variational Information Bottleneck
11. Dynamic Network Surgery for Efficient DNNs
12. [Attention is All you Need](2017-attention-is-all-you-need.md)
13. Bayesian Compression for Deep Learning
14. Variational Dropout Sparsifies Deep Neural Networks
15. [Learning both Weights and Connections for Efficient Neural Network](2015-learning-both-weights-and-connections-for-efficient-neural-network.md)
16. Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science
17. Learning Sparse Neural Networks through L0 Regularization
18. Scaling Neural Machine Translation
19. Efficient Neural Audio Synthesis
20. Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning
21. Deep Rewiring - Training very sparse deep networks
22. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
23. [Auto-Encoding Variational Bayes](2014-auto-encoding-variational-bayes.md)
24. Variational Dropout and the Local Reparameterization Trick
25. [WaveNet - A Generative Model for Raw Audio](2016-wavenet-a-generative-model-for-raw-audio.md)
26. [AMC - AutoML for Model Compression and Acceleration on Mobile Devices](2018-amc-automl-for-model-compression-and-acceleration-on-mobile-devices.md)
27. Faster gaze prediction with dense networks and Fisher pruning
28. The Lottery Ticket Hypothesis - Training Pruned Neural Networks
29. Second Order Derivatives for Network Pruning - Optimal Brain Surgeon
30. [Stochastic Backpropagation and Approximate Inference in Deep Generative Models](2014-stochastic-backpropagation-and-approximate-inference-in-deep-generative-models.md)
31. Sparse connection and pruning in large dynamic artificial neural networks
32. LPCNET - Improving Neural Speech Synthesis through Linear Prediction
33. Optimal Brain Damage
34. Bayesian Variable Selection in Linear Regression
