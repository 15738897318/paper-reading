---
title: Practical Variational Inference for Neural Networks
authors:
- A. Graves
fieldsOfStudy:
- Computer Science
meta_key: 2011-practical-variational-inference-for-neural-networks
numCitedBy: 1080
reading_status: TBD
ref_count: 29
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Practical-Variational-Inference-for-Neural-Networks-Graves/5a9ef216bf11f222438fff130c778267d39a9564?sort=total-citations
venue: NIPS
year: 2011
---

[semanticscholar url](https://www.semanticscholar.org/paper/Practical-Variational-Inference-for-Neural-Networks-Graves/5a9ef216bf11f222438fff130c778267d39a9564?sort=total-citations)

# Practical Variational Inference for Neural Networks

## Abstract

Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural networks. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically reduce the number of network weights and lead to improved generalisation. Experimental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.

## Paper References

1. Probable networks and plausible predictions - a review of practical Bayesian methods for supervised neural networks
2. Ensemble learning in Bayesian neural networks
3. The Variational Gaussian Approximation Revisited
4. Simplifying Neural Networks by Soft Weight-Sharing
5. Pruning recurrent neural networks for improved generalization performance
6. Learning representations by back-propagating errors
7. Keeping the neural networks simple by minimizing the description length of the weights
8. Variational learning and bits-back coding - an information-theoretic view to Bayesian learning
9. Radial Basis Functions - A Bayesian Treatment
10. An analysis of noise in recurrent neural networks - convergence and generalization
11. Optimal Brain Damage
12. Experiments on Learning by Back Propagation.
13. Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine
14. [Connectionist temporal classification - labelling unsegmented sequence data with recurrent neural networks](2006-connectionist-temporal-classification-labelling-unsegmented-sequence-data-with-recurrent-neural-networks)
15. A direct adaptive method for faster backpropagation learning - the RPROP algorithm
16. [Long Short-Term Memory](1997-long-short-term-memory)
17. Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks
18. Classification by Minimum-Message-Length Inference
19. Speaker-independent phone recognition using hidden Markov models
20. Information processing in dynamical systems - foundations of harmony theory
21. Modeling By Shortest Data Description*
22. A Mathematical Theory of Communication
23. Graphical Models for Machine Learning and Digital Communication
24. Arithmetic coding for data compression
25. DARPA TIMIT - - acoustic-phonetic continuous speech corpus CD-ROM, NIST speech disc 1-1.1
