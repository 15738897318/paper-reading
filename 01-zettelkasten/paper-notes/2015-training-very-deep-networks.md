---
title: Training Very Deep Networks
authors:
- R. Srivastava
- Klaus Greff
- J. Schmidhuber
fieldsOfStudy:
- Computer Science
meta_key: 2015-training-very-deep-networks
numCitedBy: 1314
reading_status: TBD
ref_count: 45
tags:
- gen-from-ref
- paper
venue: NIPS
year: 2015
---

# Training Very Deep Networks

## Abstract

Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.

## Paper References

1. Highway Networks
2. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks)
3. Deep Networks with Internal Selective Attention through Feedback Connections
4. Understanding Locally Competitive Networks
5. [Network In Network](2014-network-in-network)
6. On the Complexity of Neural Network Classifiers - A Comparison Between Shallow and Deep Architectures
7. Random Walk Initialization for Training Very Deep Feedforward Networks
8. [On the importance of initialization and momentum in deep learning](2013-on-the-importance-of-initialization-and-momentum-in-deep-learning)
9. FitNets - Hints for Thin Deep Nets
10. [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](2014-exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks)
11. On the Number of Linear Regions of Deep Neural Networks
12. Deep Learning Made Easier by Linear Transformations in Perceptrons
13. [Going deeper with convolutions](2015-going-deeper-with-convolutions)
14. Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks.
15. [Deeply-Supervised Nets](2015-deeply-supervised-nets)
16. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
17. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition)
18. [Maxout Networks](2013-maxout-networks)
19. [Striving for Simplicity - The All Convolutional Net](2015-striving-for-simplicity-the-all-convolutional-net)
20. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks)
21. [Multi-column deep neural networks for image classification](2012-multi-column-deep-neural-networks-for-image-classification)
22. Learning Complex, Extended Sequences Using the Principle of History Compression
23. Grid Long Short-Term Memory
24. [Flexible, High Performance Convolutional Neural Networks for Image Classification](2011-flexible-high-performance-convolutional-neural-networks-for-image-classification)
25. [Long Short-Term Memory](1997-long-short-term-memory)
26. Learning to Forget - Continual Prediction with LSTM
27. Spatially-sparse convolutional neural networks
28. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
29. Binding via Reconstruction Clustering
30. Compete to Compute
31. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification)
32. Bridging Long Time Lags by Weight Guessing and \long Short Term Memory
33. [Caffe - Convolutional Architecture for Fast Feature Embedding](2014-caffe-convolutional-architecture-for-fast-feature-embedding)
34. On the Expressive Efficiency of Sum Product Networks
35. Computational limitations of small-depth circuits
36. On the power of small-depth threshold circuits
37. Untersuchungen zu dynamischen neuronalen Netzen
38. Training Deep and Recurrent Networks with Hessian-Free Optimization
39. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization
