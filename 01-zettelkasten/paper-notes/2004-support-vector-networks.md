---
title: Support-Vector Networks
authors:
- Corinna Cortes
- V. Vapnik
fieldsOfStudy:
- Computer Science
meta_key: 2004-support-vector-networks
numCitedBy: 33537
reading_status: TBD
ref_count: 26
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Support-Vector-Networks-Cortes-Vapnik/52b7bf3ba59b31f362aa07f957f1543a29a4279e?sort=total-citations
venue: Machine Learning
year: 2004
---

[semanticscholar url](https://www.semanticscholar.org/paper/Support-Vector-Networks-Cortes-Vapnik/52b7bf3ba59b31f362aa07f957f1543a29a4279e?sort=total-citations)

# Support-Vector Networks

## Abstract

The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.

## Paper References

1. [A training algorithm for optimal margin classifiers](1992-a-training-algorithm-for-optimal-margin-classifiers)
2. Learning representations by back-propagating errors
3. Neural-Network and k-Nearest-neighbor Classifiers
4. Handwritten Digit Recognition with a Back-Propagation Network
5. Classification into two Multivariate Normal Distributions with Different Covariance Matrices
6. Comparison of classifier methods - a case study in handwritten digit recognition
7. Estimation of Dependences Based on Empirical Data
8. Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning
9. Methods of Mathematical Physics
10. Estimation of Dependences Based on Empirical Data - Springer Series in Statistics (Springer Series in Statistics)
11. Learning representations by back-propagation errors, nature
12. Principles of neurodynamics
13. Learning internal representations by back-propagating errors
14. THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS
15. Learning internal representations by error propagation
