---
title: Efficient Estimation of Word Representations in Vector Space
authors:
- Tomas Mikolov
- Kai Chen
- G. Corrado
- J. Dean
fieldsOfStudy:
- Computer Science
meta_key: 2013-efficient-estimation-of-word-representations-in-vector-space
numCitedBy: 22055
reading_status: TBD
ref_count: 43
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen/330da625c15427c6e42ccfa3b747fb29e5835bf0?sort=total-citations
venue: ICLR
year: 2013
---

[semanticscholar url](https://www.semanticscholar.org/paper/Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen/330da625c15427c6e42ccfa3b747fb29e5835bf0?sort=total-citations)

# Efficient Estimation of Word Representations in Vector Space

## Abstract

We propose two novel model architectures for computing continuous vector
representations of words from very large data sets. The quality of these
representations is measured in a word similarity task, and the results are
compared to the previously best performing techniques based on different types
of neural networks. We observe large improvements in accuracy at much lower
computational cost, i.e. it takes less than a day to learn high quality word
vectors from a 1.6 billion words data set. Furthermore, we show that these
vectors provide state-of-the-art performance on our test set for measuring
syntactic and semantic word similarities.

## Paper References

1. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
2. [Linguistic Regularities in Continuous Space Word Representations](2013-linguistic-regularities-in-continuous-space-word-representations)
3. Strategies for training large scale neural network language models
4. Word Representations - A Simple and General Method for Semi-Supervised Learning
5. A Neural Probabilistic Language Model
6. A fast and simple algorithm for training neural probabilistic language models
7. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
8. Hierarchical Probabilistic Neural Network Language Model
9. Neural network based language models for highly inflective languages
10. Continuous space language models
11. [Learning Word Vectors for Sentiment Analysis](2011-learning-word-vectors-for-sentiment-analysis)
12. Three new graphical models for statistical language modelling
13. The Microsoft Research Sentence Completion Challenge
14. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
15. Statistical Language Models Based on Neural Networks
16. Combining Heterogeneous Models for Measuring Relational Similarity
17. A Scalable Hierarchical Distributed Language Model
18. Empirical Evaluation and Combination of Advanced Language Modeling Techniques
19. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
20. Large Language Models in Machine Translation
21. Measuring Semantic Similarity by Latent Relational Analysis
22. SemEval-2012 Task 2 - Measuring Degrees of Relational Similarity
23. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model)
24. Finding Structure in Time
25. [Extensions of recurrent neural network language model](2011-extensions-of-recurrent-neural-network-language-model)
26. Distributed Representations
27. Scaling learning algorithms towards AI
28. [Large Scale Distributed Deep Networks](2012-large-scale-distributed-deep-networks)
29. Parallel distributed processing - explorations in the microstructure of cognition, vol. 1 - foundations
30. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization)
31. Language Modeling for Speech Recognition of Czech
32. Learning internal representations by back-propagating errors
33. [Improving Word Representations via Global Context and Multiple Word Prototypes](2012-improving-word-representations-via-global-context-and-multiple-word-prototypes)
