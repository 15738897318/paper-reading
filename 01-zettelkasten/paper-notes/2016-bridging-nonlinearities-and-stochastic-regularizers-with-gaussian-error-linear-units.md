---
title: Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units
authors:
- Dan Hendrycks
- Kevin Gimpel
fieldsOfStudy:
- Computer Science
meta_key: 2016-bridging-nonlinearities-and-stochastic-regularizers-with-gaussian-error-linear-units
numCitedBy: 292
reading_status: TBD
ref_count: 24
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Bridging-Nonlinearities-and-Stochastic-Regularizers-Hendrycks-Gimpel/4361e64f2d12d63476fdc88faf72a0f70d9a2ffb?sort=total-citations
venue: ArXiv
year: 2016
---

# Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units

## Abstract

We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic regularizer which randomly applies the identity or zero map, combining the intuitions of dropout and zoneout while respecting neuron values. This connection suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.

## Paper References

1. Zoneout - Regularizing RNNs by Randomly Preserving Hidden Activations
2. [Exact solutions to the nonlinear dynamics of learning in deep linear neural networks](2014-exact-solutions-to-the-nonlinear-dynamics-of-learning-in-deep-linear-neural-networks)
3. [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](2016-fast-and-accurate-deep-network-learning-by-exponential-linear-units-elus)
4. [Rectifier Nonlinearities Improve Neural Network Acoustic Models](2013-rectifier-nonlinearities-improve-neural-network-acoustic-models)
5. Adaptive dropout for training deep neural networks
6. Residual Networks are Exponential Ensembles of Relatively Shallow Networks
7. Learning with Pseudo-Ensembles
8. Generalizing and Improving Weight Initialization
9. Fast dropout training
10. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines)
11. [Weight Normalization - A Simple Reparameterization to Accelerate Training of Deep Neural Networks](2016-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks)
12. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
13. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting)
14. SGDR - Stochastic Gradient Descent with Warm Restarts
15. Improving Neural Networks with Dropout
16. Neural networks and physical systems with emergent collective computational abilities.
17. Adjusting for Dropout Variance in Batch Normalization and Weight Initialization
18. Deep Networks with Stochastic Depth
19. [Acoustic Modeling Using Deep Belief Networks](2012-acoustic-modeling-using-deep-belief-networks)
20. All you need is a good init
21. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2014-vggnet.md)
22. A Simple Approximation to the Area Under Standard Normal Curve
23. Improved Part-of-Speech Tagging for Online Conversational Text with Word Clusters
