---
title: Recurrent Neural Network Regularization
authors:
- Wojciech Zaremba
- Ilya Sutskever
- Oriol Vinyals
fieldsOfStudy:
- Computer Science
meta_key: 2014-recurrent-neural-network-regularization
numCitedBy: 1990
reading_status: TBD
ref_count: 39
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Recurrent-Neural-Network-Regularization-Zaremba-Sutskever/f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97?sort=total-citations
venue: ArXiv
year: 2014
---

[semanticscholar url](https://www.semanticscholar.org/paper/Recurrent-Neural-Network-Regularization-Zaremba-Sutskever/f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97?sort=total-citations)

# Recurrent Neural Network Regularization

## Abstract

We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.

## Paper References

1. [LSTM Neural Networks for Language Modeling](2012-lstm-neural-networks-for-language-modeling)
2. Context dependent recurrent neural network language model
3. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model)
4. Fast and Robust Neural Network Joint Models for Statistical Machine Translation
5. Regularization and nonlinearities for neural language models - when are they needed?
6. Dropout Improves Recurrent Neural Networks for Handwriting Recognition
7. [Speech recognition with deep recurrent neural networks](2013-speech-recognition-with-deep-recurrent-neural-networks)
8. Strategies for training large scale neural network language models
9. Sequence discriminative distributed training of long short-term memory recurrent neural networks
10. How to Construct Deep Recurrent Neural Networks
11. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
12. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
13. THE USE OF RECURRENT NEURAL NETWORKS IN CONTINUOUS SPEECH RECOGNITION
14. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
15. On Fast Dropout and its Applicability to Recurrent Networks
16. A Clockwork RNN
17. [Regularization of Neural Networks using DropConnect](2013-regularization-of-neural-networks-using-dropconnect)
18. Improving Neural Networks with Dropout
19. Statistical Language Models Based on Neural Networks
20. Optimization and applications of echo state networks with leaky- integrator neurons
21. [Long Short-Term Memory](1997-long-short-term-memory)
22. Language modeling with sum-product networks
23. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator)
24. [Recurrent Continuous Translation Models](2013-recurrent-continuous-translation-models)
25. Fast dropout training
26. Connectionist Speech Recognition - A Hybrid Approach
27. [Going deeper with convolutions](2015-going-deeper-with-convolutions)
28. BYBLOS - The BBN continuous speech recognition system
29. LIUM's SMT Machine Translation Systems for WMT 2011
30. Building a Large Annotated Corpus of English - The Penn Treebank
31. Exploiting Similarities among Languages for Machine Translation
32. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
33. A Novel Connectionist System for Unconstrained Handwriting Recognition
