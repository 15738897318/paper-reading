---
title: A new look at the statistical model identification
authors:
- H. Akaike
fieldsOfStudy:
- Mathematics
meta_key: 1974-a-new-look-at-the-statistical-model-identification
numCitedBy: 42118
reading_status: TBD
ref_count: 53
tags:
- gen-from-ref
- paper
venue: ''
year: 1974
---

# A new look at the statistical model identification

## Abstract

The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples.

## Paper References

1. The statistical approach to the analysis of time-series
2. Some tests of separate families of hypotheses in time series analysis.
3. The behavior of maximum likelihood estimates under nonstandard conditions
4. Time Series Analysis
5. On-line identification of linear dynamic systems with applications to Kalman filtering
6. Fitting autoregressive models for prediction
7. On the identification of variances and adaptive Kalman filtering
8. A Monte Carlo Comparison of the Regression Method and the Spectral Methods of Prediction
9. The prediction error of stationary Gaussian time series of unknown covariance
10. The existence of Bartlett-Rajalakshman goodness of fit G -tests for multivariate autoregressive processes with finitely dependent residuals
11. Further Results on Tests of Separate Families of Hypotheses
12. A theory of adaptive filtering
13. Autoregressive model fitting for control
14. Information Theory and an Extension of the Maximum Likelihood Principle
15. Markovian representation of stochastic processes and its application to the analysis of autoregressive moving average processes
16. Statistical predictor identification
17. Stochastic theory of minimal realization
18. Stochastic theory of minimal realization
19. Statistical approach to computer control of cement rotary kilns
20. Information Theory and Statistics
21. Spectral analysis and its applications
22. Testing Statistical Hypothesis.
23. DETERMINATION OF THE ORDER OF DEPENDENCE IN NORMALLY DISTRIBUTED TIME SERIES
24. Extensions of Quenouille's Test for Autoregressive Schemes
25. Note on a Generalization of the Large Sample Goodness of Fit Test for Linear Autoregressive Schemes
26. A Large-Sample Test for Moving Averages
27. Goodness of Fit Tests for Simultaneous Autoregressive Series
28. Some comments on C_p
29. Hypothesis Testing in Time Series Analysis.
30. A Large‚ÄêSample Test for the Goodness of Fit of Autoregressive Schemes
31. Prediction and Regulation.
32. Estimation of power spectra with finite-order autoregressive models
33. Model Building for Prediction in Regression Based Upon Repeated Significance Tests
34. Comparison of different methods for identification of industrial processes
35. Tests of Separate Families of Hypotheses
36. A Study in the Analysis of Stationary Time-Series.
