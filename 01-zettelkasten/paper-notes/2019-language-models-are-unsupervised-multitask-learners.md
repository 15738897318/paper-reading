---
title: Language Models are Unsupervised Multitask Learners
authors:
- Alec Radford
- Jeff Wu
- Rewon Child
- D. Luan
- Dario Amodei
- Ilya Sutskever
fieldsOfStudy:
- Computer Science
meta_key: 2019-language-models-are-unsupervised-multitask-learners
numCitedBy: 6429
reading_status: TBD
ref_count: 75
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe?sort=total-citations
venue: ''
year: 2019
---

[semanticscholar url](https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe?sort=total-citations)

# Language Models are Unsupervised Multitask Learners

## Abstract

Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.

## Paper References

1. Dialog-based Language Learning
2. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
3. [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](2017-supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data)
4. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
5. A Neural Conversational Model
6. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
7. Unsupervised Pretraining for Sequence to Sequence Learning
8. Unsupervised Machine Translation Using Monolingual Corpora Only
9. One billion word benchmark for measuring progress in statistical language modeling
10. [Universal Language Model Fine-tuning for Text Classification](2018-universal-language-model-fine-tuning-for-text-classification)
11. The LAMBADA dataset - Word prediction requiring a broad discourse context
12. Unsupervised Neural Machine Translation
13. [Exploring the Limits of Language Modeling](2016-exploring-the-limits-of-language-modeling)
14. Word Translation Without Parallel Data
15. A Simple Method for Commonsense Reasoning
16. [Learned in Translation - Contextualized Word Vectors](2017-learned-in-translation-contextualized-word-vectors)
17. [Attention is All you Need](2017-transformer.md)
18. Looking for ELMo's friends - Sentence-Level Pretraining Beyond Language Modeling
19. Multilingual Language Processing From Bytes
20. [Neural Machine Translation of Rare Words with Subword Units](2016-neural-machine-translation-of-rare-words-with-subword-units)
21. An Effective Approach to Unsupervised Machine Translation
22. Bottom-Up Abstractive Summarization
23. Wizard of Wikipedia - Knowledge-Powered Conversational agents
24. Natural Questions - A Benchmark for Question Answering Research
25. [Transformer-XL - Attentive Language Models beyond a Fixed-Length Context](2019-transformer-xl-attentive-language-models-beyond-a-fixed-length-context)
26. CoQA - A Conversational Question Answering Challenge
27. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
28. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
29. Semi-supervised Sequence Learning
30. A Neural Probabilistic Language Model
31. Visualizing and Understanding Recurrent Networks
32. Pointer Sentinel Mixture Models
33. [Learning Distributed Representations of Sentences from Unlabelled Data](2016-learning-distributed-representations-of-sentences-from-unlabelled-data)
34. Learning to Generate Reviews and Discovering Sentiment
35. TransferTransfo - A Transfer Learning Approach for Neural Network Based Conversational Agents
36. [The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations](2016-the-goldilocks-principle-reading-children-s-books-with-explicit-memory-representations)
37. Adversarial Examples for Evaluating Reading Comprehension Systems
38. No Training Required - Exploring Random Encoders for Sentence Classification
39. Generating Wikipedia by Summarizing Long Sequences
40. Get To The Point - Summarization with Pointer-Generator Networks
41. [Character-Level Language Modeling with Deeper Self-Attention](2019-character-level-language-modeling-with-deeper-self-attention)
42. Entity Tracking Improves Cloze-style Reading Comprehension
43. FRAGE - Frequency-Agnostic Word Representation
44. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
45. Overcoming catastrophic forgetting in neural networks
46. Story Cloze Task - UW NLP System
47. Deep Speech 2 - End-to-End Speech Recognition in English and Mandarin
48. Deep Learning Scaling is Predictable, Empirically
49. Do We Train on Test Data? Purging CIFAR of Near-Duplicates
50. [Building machines that learn and think like people](2016-building-machines-that-learn-and-think-like-people)
51. Hierarchical Neural Story Generation
52. Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
53. Multitask Learning
54. Do CIFAR-10 Classifiers Generalize to CIFAR-10?
55. A BERT Baseline for the Natural Questions
56. On the Evaluation of Common-Sense Reasoning in Natural Language Understanding
57. Embracing data abundance - BookTest Dataset for Reading Comprehension
58. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
59. Neural Word Embedding as Implicit Matrix Factorization
60. Towards Principled Unsupervised Learning
61. Improving Neural Language Models with a Continuous Cache
62. [ImageNet classification with deep convolutional neural networks](2012-alexnet.md)
63. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks)
64. The Winograd Schema Challenge
65. The Natural Language Decathlon - Multitask Learning as Question Answering
66. Strike (With) a Pose - Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects
67. Content extraction using diverse feature sets
68. The advantages and challenges of “big data” - Insights from the 14 billion word iWeb corpus
69. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
70. Interpolated estimation of Markov source parameters from sparse data
71. Learning and Evaluating General Linguistic Intelligence
72. Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning
73. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond
