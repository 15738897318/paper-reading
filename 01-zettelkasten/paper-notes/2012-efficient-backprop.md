---
title: Efficient BackProp
authors:
- Yann LeCun
- L. Bottou
- G. Orr
- "K. M\xFCller"
fieldsOfStudy:
- Computer Science
meta_key: 2012-efficient-backprop
numCitedBy: 2638
reading_status: TBD
ref_count: 36
tags:
- gen-from-ref
- paper
venue: 'Neural Networks: Tricks of the Trade'
year: 2012
---

# Efficient BackProp

## Paper References

1. PhD thesis - Modeles connexionnistes de l'apprentissage (connectionist learning models)
2. Matrix computations
3. Practical Methods of Optimization
4. Mathematical Approaches to Neural Networks
5. Generalization and network design strategies
6. On-line Learning of Dichotomies - Algorithms and Learning Curves.
7. Regularization in the Selection of Radial Basis Function Centers
8. Exact solution for on-line learning in multilayer neural networks.
9. Adaptive Algorithms and Stochastic Approximations
10. Second Order Properties of Error Surfaces
11. [Statistical learning theory](1998-statistical-learning-theory)
12. Fast Learning in Networks of Locally-Tuned Processing Units
13. Supervised learning on large redundant training sets
14. Increased rates of convergence through learning rate adaptation
15. Neural networks for pattern recognition
16. Stochastic dynamics of learning with momentum in neural networks
17. [The Nature of Statistical Learning Theory](2000-the-nature-of-statistical-learning-theory)
18. Phoneme recognition using time-delay neural networks
19. Note on Learning Rate Schedules for Stochastic Optimization
20. Removing Noise in On-Line Search using Adaptive Batch Sizes
21. Efficient Parallel Learning Algorithms for Neural Networks
22. Minimisation methods for training feedforward neural networks
23. Neural Networks and the Bias/Variance Dilemma
24. On-line learning processes in artificial neural networks
25. Computing second derivatives in feed-forward networks - a review
26. A scaled conjugate gradient algorithm for fast supervised learning
27. Complexity Issues in Natural Gradient Descent Method for Training Multilayer Perceptrons
28. First- and Second-Order Methods for Learning - Between Steepest Descent and Newton's Method
29. Fast Exact Multiplication by the Hessian
30. Optimal Brain Damage
31. Adapting Bias by Gradient Descent - An Incremental Version of Delta-Bar-Delta
32. The Efficiency and the Robustness of Natural Gradient Descent Learning Rule
33. Adaptive On-line Learning in Changing Environments
34. Automatic Learning Rate Maximization by On-Line Estimation of the Hessian's Eigenvectors
35. Natural Gradient Works Efficiently in Learning
36. Neural Learning in Structured Parameter Spaces - Natural Riemannian Gradient
