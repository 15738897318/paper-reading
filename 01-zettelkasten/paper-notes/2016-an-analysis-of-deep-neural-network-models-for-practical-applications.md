---
title: An Analysis of Deep Neural Network Models for Practical Applications
authors:
- A. Canziani
- Adam Paszke
- E. Culurciello
fieldsOfStudy:
- Computer Science
meta_key: 2016-an-analysis-of-deep-neural-network-models-for-practical-applications
numCitedBy: 810
reading_status: TBD
ref_count: 22
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/An-Analysis-of-Deep-Neural-Network-Models-for-Canziani-Paszke/9a786d1ecf77dfba3459a83cd3fa0f1781bbcba4?sort=total-citations
venue: ArXiv
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/An-Analysis-of-Deep-Neural-Network-Models-for-Canziani-Paszke/9a786d1ecf77dfba3459a83cd3fa0f1781bbcba4?sort=total-citations)

# An Analysis of Deep Neural Network Models for Practical Applications

## Abstract

Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilisation of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint are an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.

## Paper References

1. [Rethinking the Inception Architecture for Computer Vision](2016-rethinking-the-inception-architecture-for-computer-vision.md)
2. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
3. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
4. ENet - A Deep Neural Network Architecture for Real-Time Semantic Segmentation
5. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
6. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
7. [Network In Network](2014-network-in-network.md)
8. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](2017-inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning.md)
9. [Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding](2016-deep-compression-compressing-deep-neural-network-with-pruning-trained-quantization-and-huffman-coding.md)
10. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge.md)
11. [cuDNN - Efficient Primitives for Deep Learning](2014-cudnn-efficient-primitives-for-deep-learning.md)
12. [Torch7 - A Matlab-like Environment for Machine Learning](2011-torch7-a-matlab-like-environment-for-machine-learning.md)
13. References
