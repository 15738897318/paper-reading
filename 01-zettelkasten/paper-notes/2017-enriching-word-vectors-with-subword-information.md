---
title: Enriching Word Vectors with Subword Information
authors:
- Piotr Bojanowski
- Edouard Grave
- Armand Joulin
- Tomas Mikolov
fieldsOfStudy:
- Computer Science
meta_key: 2017-enriching-word-vectors-with-subword-information
numCitedBy: 6664
reading_status: TBD
ref_count: 56
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Enriching-Word-Vectors-with-Subword-Information-Bojanowski-Grave/e2dba792360873aef125572812f3673b1a85d850?sort=total-citations
venue: Transactions of the Association for Computational Linguistics
year: 2017
---

# Enriching Word Vectors with Subword Information

## Abstract

Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.

## Paper References

1. Better Word Representations with Recursive Neural Networks for Morphology
2. Co-learning of Word Representations and Morpheme Representations
3. Learning Character-level Representations for Part-of-Speech Tagging
4. Joint Learning of Character and Word Embeddings
5. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
6. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
7. KNET - A General Framework for Learning Word Embedding Using Morphological Knowledge
8. Finding Function in Form - Compositional Character Models for Open Vocabulary Word Representation
9. Word Embeddings Go to Italy - A Comparison of Models and Training Datasets
10. Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models
11. [Neural Machine Translation of Rare Words with Subword Units](2016-neural-machine-translation-of-rare-words-with-subword-units)
12. Letter N-Gram-based Input Encoding for Continuous Space Language Models
13. [Character-Aware Neural Language Models](2016-character-aware-neural-language-models)
14. Compositional Morphology for Word Representations and Language Modelling
15. Multilingual Reliability and “Semantic” Structure of Continuous Word Spaces
16. Distributional Memory - A General Framework for Corpus-Based Semantics
17. Producing high-dimensional semantic spaces from lexical co-occurrence
18. Charagram - Embedding Words and Sentences via Character n-grams
19. Alternative structures for character-level RNNs
20. Morphological Word-Embeddings
21. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
22. Comparison of Semantic Similarity for Different Languages Using the Google n-gram Corpus and Second-Order Co-occurrence Measures
23. Normalizing tweets with edit scripts and recurrent neural embeddings
24. SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
25. Dimensions of meaning
26. Morphology-based and sub-word language modeling for Turkish speech recognition
27. Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs
28. Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics
29. Factored Neural Language Models
30. Word Space
31. From Frequency to Meaning - Vector Space Models of Semantics
32. New word analogy corpus for exploring embeddings of Czech words
33. Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts
34. Unsupervised Morphology Induction Using Word Embeddings
35. [Generating Text with Recurrent Neural Networks](2011-generating-text-with-recurrent-neural-networks)
36. Using the Structure of a Conceptual Network in Computing Semantic Relatedness
37. Indexing by Latent Semantic Analysis
38. Distributional Structure
39. [Character-level Convolutional Networks for Text Classification](2015-character-level-convolutional-networks-for-text-classification)
40. Cross-lingual Semantic Relatedness Using Encyclopedic Knowledge
41. Automatically Creating Datasets for Measures of Semantic Relatedness
42. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
43. Learning representations by back-propagating errors
44. Placing search in context - the concept revisited
45. Neurocomputing - Foundations of Research
46. Hogwild - A Lock-Free Approach to Parallelizing Stochastic Gradient Descent
47. Identifying Opportunities for Valuable Encounters - Toward Context-Aware Social Matching Systems
48. The Proof and Measurement of Association between Two Things.
49. Human and Machine Judgements for Russian Semantic Relatedness
50. The proof and measurement of association between two things.
