---
title: GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding
authors:
- Alex Wang
- Amanpreet Singh
- Julian Michael
- Felix Hill
- Omer Levy
- Samuel R. Bowman
fieldsOfStudy:
- Computer Science
meta_key: 2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding
numCitedBy: 2690
reading_status: TBD
ref_count: 72
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/GLUE:-A-Multi-Task-Benchmark-and-Analysis-Platform-Wang-Singh/93b8da28d006415866bf48f9a6e06b5242129195?sort=total-citations
venue: BlackboxNLP@EMNLP
year: 2018
---

[semanticscholar url](https://www.semanticscholar.org/paper/GLUE:-A-Multi-Task-Benchmark-and-Analysis-Platform-Wang-Singh/93b8da28d006415866bf48f9a6e06b5242129195?sort=total-citations)

# GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding

## Abstract

Human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.

## Paper References

1. Recurrent Neural Network-Based Sentence Encoder with Gated Attention for Natural Language Inference
2. A Joint Many-Task Model - Growing a Neural Network for Multiple NLP Tasks
3. [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](2018-a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference)
4. Sluice networks - Learning what to share between loosely related tasks
5. [Towards AI-Complete Question Answering - A Set of Prerequisite Toy Tasks](2016-towards-ai-complete-question-answering-a-set-of-prerequisite-toy-tasks)
6. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference)
7. One billion word benchmark for measuring progress in statistical language modeling
8. AllenNLP - A Deep Semantic Natural Language Processing Platform
9. DisSent - Sentence Representation Learning from Explicit Discourse Relations
10. Reasoning about Entailment with Neural Attention
11. [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](2017-supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data)
12. Annotation Artifacts in Natural Language Inference Data
13. [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](2017-semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation)
14. Towards Linguistically Generalizable NLP Systems - A Workshop and Shared Task
15. Transforming Question Answering Datasets Into Natural Language Inference Datasets
16. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
17. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
18. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
19. [Skip-Thought Vectors](2015-skip-thought-vectors)
20. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
21. [Learned in Translation - Contextualized Word Vectors](2017-learned-in-translation-contextualized-word-vectors)
22. [Bidirectional Attention Flow for Machine Comprehension](2017-bidirectional-attention-flow-for-machine-comprehension)
23. Hypothesis Only Baselines in Natural Language Inference
24. ParlAI - A Dialog Research Software Platform
25. SentEval - An Evaluation Toolkit for Universal Sentence Representations
26. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
27. Non-entailed subsequences as a challenge for natural language inference
28. [Attention is All you Need](2017-transformer.md)
29. Inference is Everything - Recasting Semantic Resources into a Unified Evaluation Framework
30. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
31. [Learning Distributed Representations of Sentences from Unlabelled Data](2016-learning-distributed-representations-of-sentences-from-unlabelled-data)
32. [Distributed Representations of Sentences and Documents](2014-distributed-representations-of-sentences-and-documents)
33. Deep multi-task learning with low level tasks supervised at lower layers
34. [Neural Network Acceptability Judgments](2019-neural-network-acceptability-judgments)
35. The Sixth PASCAL Recognizing Textual Entailment Challenge
36. The Effect of Different Writing Tasks on Linguistic Style - A Case Study of the ROC Story Cloze Task
37. Performance Impact Caused by Hidden Bias of Training Data for Recognizing Textual Entailment
38. The Seventh PASCAL Recognizing Textual Entailment Challenge
39. Automatically Constructing a Corpus of Sentential Paraphrases
40. Seeing Stars - Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales
41. The TREC-8 Question Answering Track Report
42. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
43. The PASCAL Recognising Textual Entailment Challenge
44. The Second PASCAL Recognising Textual Entailment Challenge
45. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
46. The Winograd Schema Challenge
47. Comparing two K-category assignments by a K-category correlation coefficient
48. Bag of Tricks for Efficient Text Classification
49. [A Sentimental Education - Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts](2004-a-sentimental-education-sentiment-analysis-using-subjectivity-summarization-based-on-minimum-cuts)
50. The Natural Language Decathlon - Multitask Learning as Question Answering
51. [Mining and summarizing customer reviews](2004-mining-and-summarizing-customer-reviews)
52. The empirical base of linguistics - Grammaticality judgments and linguistic methodology
53. Social Bias in Elicited Natural Language Inferences
54. Annotating Expressions of Opinions and Emotions in Language
55. Using the Framework
56. Comparison of the predicted and observed secondary structure of T4 phage lysozyme.
57. Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning
58. Natural Language Inference over Interaction Space
59. Latent Multi-Task Architecture Learning
60. The Third PASCAL Recognizing Textual Entailment Challenge
61. Dynamic Coattention Networks For Question Answering
62. Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Tectual Entailment
