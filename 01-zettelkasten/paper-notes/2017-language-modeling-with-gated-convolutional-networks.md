---
title: Language Modeling with Gated Convolutional Networks
authors:
- Yann Dauphin
- Angela Fan
- Michael Auli
- David Grangier
fieldsOfStudy:
- Computer Science
meta_key: 2017-language-modeling-with-gated-convolutional-networks
numCitedBy: 1289
reading_status: TBD
ref_count: 42
tags:
- gen-from-ref
- other-default
- paper
venue: ICML
year: 2017
---

# Language Modeling with Gated Convolutional Networks

## Abstract

The pre-dominant approach to language modeling to date is based on recurrent neural networks. Their success on this task is often linked to their ability to capture unbounded context. In this paper we develop a finite context approach through stacked convolutions, which can be more efficient since they allow parallelization over sequential tokens. We propose a novel simplified gating mechanism that outperforms Oord et al (2016) and investigate the impact of key architectural decisions. The proposed approach achieves state-of-the-art on the WikiText-103 benchmark, even though it features long-term dependencies, as well as competitive results on the Google Billion Words benchmark. Our model reduces the latency to score a sentence by an order of magnitude compared to a recurrent baseline. To our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on these large scale language tasks.

## Paper References

1. [Exploring the Limits of Language Modeling](2016-exploring-the-limits-of-language-modeling)
2. BlackOut - Speeding up Recurrent Neural Network Language Models With Very Large Vocabularies
3. Pointer Sentinel Mixture Models
4. One billion word benchmark for measuring progress in statistical language modeling
5. [Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](2017-outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer)
6. Skip-gram Language Modeling Using Sparse Non-negative Matrix Probability Estimation
7. Improving Neural Language Models with a Continuous Cache
8. Hierarchical Probabilistic Neural Network Language Model
9. Strategies for Training Large Vocabulary Neural Language Models
10. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model)
11. Efficient softmax approximation for GPUs
12. [Weight Normalization - A Simple Reparameterization to Accelerate Training of Deep Neural Networks](2016-weight-normalization-a-simple-reparameterization-to-accelerate-training-of-deep-neural-networks)
13. [On the importance of initialization and momentum in deep learning](2013-on-the-importance-of-initialization-and-momentum-in-deep-learning)
14. A Neural Probabilistic Language Model
15. [Long Short-Term Memory](1997-long-short-term-memory)
16. Improved backing-off for M-gram language modeling
17. [On the difficulty of training recurrent neural networks](2013-on-the-difficulty-of-training-recurrent-neural-networks)
18. Three new graphical models for statistical language modelling
19. Factorization tricks for LSTM networks
20. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification)
21. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition)
22. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks)
23. Predicting distributions with Linearizing Belief Networks
24. Conditional Image Generation with PixelCNN Decoders
25. Statistical Machine Translation
26. Automatic Speech Recognition - A Deep Learning Approach
27. [Torch7 - A Matlab-like Environment for Machine Learning](2011-torch7-a-matlab-like-environment-for-machine-learning)
28. [Foundations of statistical natural language processing](2002-foundations-of-statistical-natural-language-processing)
29. Noise-contrastive estimation - A new estimation principle for unnormalized statistical models
30. Syntactic Process
31. An empirical study of smoothing techniques for language modeling
32. Pixel Recurrent Neural Networks
33. genCNN - A Convolutional Architecture for Word Sequence Prediction
34. Convolutional networks for images, speech, and time series
