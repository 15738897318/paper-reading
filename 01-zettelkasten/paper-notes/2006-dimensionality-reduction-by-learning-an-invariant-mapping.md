---
title: Dimensionality Reduction by Learning an Invariant Mapping
authors:
- R. Hadsell
- S. Chopra
- Yann LeCun
fieldsOfStudy:
- Computer Science
meta_key: 2006-dimensionality-reduction-by-learning-an-invariant-mapping
numCitedBy: 3027
reading_status: TBD
ref_count: 25
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Dimensionality-Reduction-by-Learning-an-Invariant-Hadsell-Chopra/46f30e94dd3d5902141c5fbe58d0bc9189545c76?sort=total-citations
venue: 2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)
year: 2006
---

# Dimensionality Reduction by Learning an Invariant Mapping

## Abstract

Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.

## Paper References

1. [Nonlinear dimensionality reduction by locally linear embedding.](2000-nonlinear-dimensionality-reduction-by-locally-linear-embedding.md)
2. Learning a kernel matrix for nonlinear dimensionality reduction
3. Unsupervised Learning of Image Manifolds by Semidefinite Programming
4. [A global geometric framework for nonlinear dimensionality reduction.](2000-a-global-geometric-framework-for-nonlinear-dimensionality-reduction.md)
5. [Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering](2001-laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering.md)
6. Learning a similarity metric discriminatively, with application to face verification
7. Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering
8. Hessian eigenmaps - Locally linear embedding techniques for high-dimensional data
9. A Neural Support Vector Network architecture with adaptive kernels
10. Nonlinear Component Analysis as a Kernel Eigenvalue Problem
11. [On Spectral Clustering - Analysis and an algorithm](2001-on-spectral-clustering-analysis-and-an-algorithm.md)
12. [Normalized cuts and image segmentation](1997-normalized-cuts-and-image-segmentation.md)
13. Learning methods for generic object recognition with invariance to pose and lighting
14. [GradientBased Learning Applied to Document Recognition](2001-gradientbased-learning-applied-to-document-recognition.md)
15. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition.md)
16. Principal Component Analysis
17. Signature Verification Using A Siamese Time Delay Neural Network
