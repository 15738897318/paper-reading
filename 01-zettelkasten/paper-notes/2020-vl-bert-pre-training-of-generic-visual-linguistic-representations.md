---
title: VL-BERT - Pre-training of Generic Visual-Linguistic Representations
authors:
- Weijie Su
- Xizhou Zhu
- Yue Cao
- B. Li
- Lewei Lu
- Furu Wei
- Jifeng Dai
fieldsOfStudy:
- Computer Science
meta_key: 2020-vl-bert-pre-training-of-generic-visual-linguistic-representations
numCitedBy: 735
reading_status: TBD
ref_count: 56
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/VL-BERT:-Pre-training-of-Generic-Visual-Linguistic-Su-Zhu/2527626c11a84f15709e943fbfa2356e19930e3b?sort=total-citations
venue: ICLR
year: 2020
---

[semanticscholar url](https://www.semanticscholar.org/paper/VL-BERT:-Pre-training-of-Generic-Visual-Linguistic-Su-Zhu/2527626c11a84f15709e943fbfa2356e19930e3b?sort=total-citations)

# VL-BERT - Pre-training of Generic Visual-Linguistic Representations

## Abstract

We introduce a new pre-trainable generic representation for visual-linguistic tasks, called Visual-Linguistic BERT (VL-BERT for short). VL-BERT adopts the simple yet powerful Transformer model as the backbone, and extends it to take both visual and linguistic embedded features as input. In it, each element of the input is either of a word from the input sentence, or a region-of-interest (RoI) from the input image. It is designed to fit for most of the visual-linguistic downstream tasks. To better exploit the generic representation, we pre-train VL-BERT on the massive-scale Conceptual Captions dataset, together with text-only corpus. Extensive empirical analysis demonstrates that the pre-training procedure can better align the visual-linguistic clues and benefit the downstream tasks, such as visual commonsense reasoning, visual question answering and referring expression comprehension. It is worth noting that VL-BERT achieved the first place of single model on the leaderboard of the VCR benchmark. Code is released at \url{this https URL}.

## Paper References

1. [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks)
2. [VisualBERT - A Simple and Performant Baseline for Vision and Language](2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language)
3. [Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training](2020-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training)
4. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
5. [Visual7W - Grounded Question Answering in Images](2016-visual7w-grounded-question-answering-in-images)
6. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
7. [Language Models are Unsupervised Multitask Learners](2019-language-models-are-unsupervised-multitask-learners)
8. [Fusion of Detected Objects in Text for Visual Question Answering](2019-fusion-of-detected-objects-in-text-for-visual-question-answering)
9. [MAttNet - Modular Attention Network for Referring Expression Comprehension](2018-mattnet-modular-attention-network-for-referring-expression-comprehension)
10. [Skip-Thought Vectors](2015-skip-thought-vectors)
11. [LXMERT - Learning Cross-Modality Encoder Representations from Transformers](2019-lxmert-learning-cross-modality-encoder-representations-from-transformers)
12. [DeCAF - A Deep Convolutional Activation Feature for Generic Visual Recognition](2014-decaf-a-deep-convolutional-activation-feature-for-generic-visual-recognition)
13. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
14. [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations)
15. [Attention is All you Need](2017-transformer.md)
16. [From Recognition to Cognition - Visual Commonsense Reasoning](2019-from-recognition-to-cognition-visual-commonsense-reasoning)
17. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
18. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
19. [GQA - A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](2019-gqa-a-new-dataset-for-real-world-visual-reasoning-and-compositional-question-answering)
20. From Two Graphs to N Questions - A VQA Dataset for Compositional Reasoning on Vision and Commonsense
21. Contrastive Bidirectional Transformer for Temporal Representation Learning
22. [Cross-lingual Language Model Pretraining](2019-cross-lingual-language-model-pretraining)
23. [XLNet - Generalized Autoregressive Pretraining for Language Understanding](2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding)
24. [Fully Convolutional Networks for Semantic Segmentation](2017-fully-convolutional-networks-for-semantic-segmentation)
25. [CLEVR - A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning](2017-clevr-a-diagnostic-dataset-for-compositional-language-and-elementary-visual-reasoning)
26. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
27. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
28. [Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation](2014-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation)
29. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation)
30. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
31. [Simultaneous Detection and Segmentation](2014-simultaneous-detection-and-segmentation)
32. [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](2014-from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions)
33. [RoBERTa - A Robustly Optimized BERT Pretraining Approach](2019-roberta-a-robustly-optimized-bert-pretraining-approach)
34. A Multiscale Visualization of Attention in the Transformer Model
35. [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning)
36. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database)
37. BERT has a Mouth, and It Must Speak - BERT as a Markov Random Field Language Model
38. Learning Video Representations using Contrastive Bidirectional Transformer
39. Relation Networks for Object Detection
40. [ImageNet classification with deep convolutional neural networks](2012-alexnet.md)
41. ReferItGame - Referring to Objects in Photographs of Natural Scenes
42. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
43. [Fast R-CNN](2015-fast-r-cnn)
44. [Microsoft COCO Captions - Data Collection and Evaluation Server](2015-microsoft-coco-captions-data-collection-and-evaluation-server)
45. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn.md)
46. [VideoBERT - A Joint Model for Video and Language Representation Learning](2019-videobert-a-joint-model-for-video-and-language-representation-learning)
47. Rethinking ImageNet Pre-Training
48. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
