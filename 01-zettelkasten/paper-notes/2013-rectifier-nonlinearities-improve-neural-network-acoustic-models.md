---
title: Rectifier Nonlinearities Improve Neural Network Acoustic Models
authors:
- Andrew L. Maas
fieldsOfStudy:
- Computer Science
meta_key: 2013-rectifier-nonlinearities-improve-neural-network-acoustic-models
numCitedBy: 4428
reading_status: TBD
ref_count: 16
tags:
- gen-from-ref
- other-default
- paper
venue: ''
year: 2013
---

# Rectifier Nonlinearities Improve Neural Network Acoustic Models

## Abstract

Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.

## Paper References

1. On rectified linear units for speech processing
2. [Improving deep neural networks for LVCSR using rectified linear units and dropout](2013-improving-deep-neural-networks-for-lvcsr-using-rectified-linear-units-and-dropout)
3. [Deep Neural Networks for Acoustic Modeling in Speech Recognition](2012-deep-neural-networks-for-acoustic-modeling-in-speech-recognition)
4. Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks.
5. [Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition](2012-context-dependent-pre-trained-deep-neural-networks-for-large-vocabulary-speech-recognition)
6. Learning long-term dependencies with gradient descent is difficult
7. [ImageNet classification with deep convolutional neural networks](2012-alexnet.md)
8. Sequence-discriminative training of deep neural networks
9. [The Kaldi Speech Recognition Toolkit](2011-the-kaldi-speech-recognition-toolkit)
10. The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization
11. A Comparison of Natural-Image-Based Models of Simple-Cell Coding
12. Scalable Minimum Bayes Risk Training of Deep Neural Network Acoustic Models Using Distributed Hessian-free Optimization
13. Characterizing the sparseness of neural codes
