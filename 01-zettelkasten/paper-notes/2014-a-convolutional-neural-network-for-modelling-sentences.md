---
title: A Convolutional Neural Network for Modelling Sentences
authors:
- Nal Kalchbrenner
- Edward Grefenstette
- P. Blunsom
fieldsOfStudy:
- Computer Science
meta_key: 2014-a-convolutional-neural-network-for-modelling-sentences
numCitedBy: 3000
reading_status: TBD
ref_count: 44
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/A-Convolutional-Neural-Network-for-Modelling-Kalchbrenner-Grefenstette/27725a2d2a8cee9bf9fffc6c2167017103aba0fa?sort=total-citations
venue: ACL
year: 2014
---

[semanticscholar url](https://www.semanticscholar.org/paper/A-Convolutional-Neural-Network-for-Modelling-Kalchbrenner-Grefenstette/27725a2d2a8cee9bf9fffc6c2167017103aba0fa?sort=total-citations)

# A Convolutional Neural Network for Modelling Sentences

## Abstract

The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25% error reduction in the last task with respect to the strongest baseline.

## Paper References

1. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
2. Recurrent Convolutional Neural Networks for Discourse Compositionality
3. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
4. Context dependent recurrent neural network language model
5. Grounded Compositional Semantics for Finding and Describing Images with Sentences
6. [Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions](2011-semi-supervised-recursive-autoencoders-for-predicting-sentiment-distributions)
7. [Recurrent Continuous Translation Models](2013-recurrent-continuous-translation-models)
8. Word Representations - A Simple and General Method for Semi-Supervised Learning
9. Inductive Learning in Symbolic Domains Using Structure-Driven Recurrent Neural Networks
10. LSTM recurrent networks learn simple context-free and context-sensitive languages
11. Prior Disambiguation of Word Tensors for Constructing Sentence Vectors
12. The Role of Syntax in Vector Space Models of Compositional Semantics
13. Question Classification using Head Words and their Hypernyms
14. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors)
15. [Extensions of recurrent neural network language model](2011-extensions-of-recurrent-neural-network-language-model)
16. From symbolic to sub-symbolic information in question classification
17. Learning to Map Sentences to Logical Form - Structured Classification with Probabilistic Categorial Grammars
18. Learning Question Classifiers
19. A Structured Vector Space Model for Word Meaning in Context
20. Recursive Distributed Representations
21. Phoneme recognition using time-delay neural networks
22. Composition in Distributional Models of Semantics
23. Connectionist Learning Procedures
24. Estimating Linear Models for Compositional Distributional Semantics
25. A Context-Theoretic Framework for Compositionality in Distributional Semantics
26. [Gradient-based learning applied to document recognition](1998-lenet5.md)
27. Nouns are Vectors, Adjectives are Matrices - Representing Adjective-Noun Constructions in Semantic Space
28. Question classification with log-linear models
29. Category-theoretic quantitative compositional distributional models of natural language semantics
30. Vector-based Models of Semantic Composition
31. Mathematical Foundations for a Compositional Distributional Model of Meaning
32. Domain and Function - A Dual-Space Model of Semantic Relations and Compositions
33. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization)
34. Readings in speech recognition
35. Vector Space Models of Word Meaning and Phrase Meaning - A Survey
36. Continuous Space Translation Models for Phrase-Based Statistical Machine Translation
37. Experimental Support for a Categorical Compositional Distributional Model of Meaning
