---
title: Multi-Task Deep Neural Networks for Natural Language Understanding
authors:
- Xiaodong Liu
- Pengcheng He
- Weizhu Chen
- Jianfeng Gao
fieldsOfStudy:
- Computer Science
meta_key: 2019-multi-task-deep-neural-networks-for-natural-language-understanding
numCitedBy: 741
reading_status: TBD
ref_count: 38
tags:
- gen-from-ref
- other-default
- paper
venue: ACL
year: 2019
---

# Multi-Task Deep Neural Networks for Natural Language Understanding

## Abstract

In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.

## Paper References

1. Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding
2. [Unified Language Model Pre-training for Natural Language Understanding and Generation](2019-unified-language-model-pre-training-for-natural-language-understanding-and-generation)
3. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
4. Sentence Encoders on STILTs - Supplementary Training on Intermediate Labeled-data Tasks
5. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
6. [Attention is All you Need](2017-transformer.md)
7. Multi-task Sequence to Sequence Learning
8. The RepEval 2017 Shared Task - Multi-Genre Natural Language Inference with Sentence Representations
9. Testing the Generalization Power of Neural Network Models across NLI Benchmarks
10. Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval
11. Soft Layer-Specific Multi-Task Summarization with Entailment and Question Generation
12. [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](2017-semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation)
13. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
14. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
15. Semantic Sentence Matching with Densely-connected Recurrent and Co-attentive Information
16. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
17. A Survey on MultiTask Learning
18. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference)
19. Stochastic Answer Networks for Machine Reading Comprehension
20. A Survey on Multi-Task Learning
21. SciTaiL - A Textual Entailment Dataset from Science Question Answering
22. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
23. Stochastic Answer Networks for Natural Language Inference
24. Learning deep structured semantic models for web search using clickthrough data
25. [Neural Network Acceptability Judgments](2019-neural-network-acceptability-judgments)
26. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
27. Breaking NLI Systems with Sentences that Require Simple Lexical Inferences
28. Neural Approaches to Conversational AI
29. Automatically Constructing a Corpus of Sentential Paraphrases
30. The Winograd Schema Challenge
31. Learning to rank using gradient descent
32. Comparison of the predicted and observed secondary structure of T4 phage lysozyme.
33. Latent Multi-Task Architecture Learning
34. Multi-Task Learning for Machine Reading Comprehension
35. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
