---
title: Improving Language Understanding by Generative Pre-Training
authors:
- Alec Radford
- Karthik Narasimhan
fieldsOfStudy:
- Computer Science
meta_key: 2018-improving-language-understanding-by-generative-pre-training
numCitedBy: 3591
reading_status: TBD
ref_count: 76
tags:
- gen-from-ref
- other-default
- paper
venue: ''
year: 2018
---

# Improving Language Understanding by Generative Pre-Training

## Abstract

Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).

## Paper References

1. Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning
2. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
3. [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](2018-a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference)
4. [Universal Language Model Fine-tuning for Text Classification](2018-universal-language-model-fine-tuning-for-text-classification)
5. Semi-supervised sequence tagging with bidirectional language models
6. A Simple but Tough-to-Beat Baseline for Sentence Embeddings
7. [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](2017-supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data)
8. Unsupervised Pretraining for Sequence to Sequence Learning
9. Reasoning about Entailment with Neural Attention
10. [Skip-Thought Vectors](2015-skip-thought-vectors)
11. A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference
12. Unsupervised Machine Translation Using Monolingual Corpora Only
13. UNSUPERVISED MACHINE TRANSLATION USING MONOLINGUAL CORPORA ONLY
14. When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?
15. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference)
16. Semi-Supervised Sequential Labeling and Segmentation Using Giga-Word Scale Unlabeled Data
17. Semi-supervised Multitask Learning for Sequence Labeling
18. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
19. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
20. [Learned in Translation - Contextualized Word Vectors](2017-learned-in-translation-contextualized-word-vectors)
21. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
22. Semi-Supervised Learning for Natural Language
23. [Attention is All you Need](2017-attention-is-all-you-need)
24. An efficient framework for learning sentence representations
25. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend)
26. Constituency Parsing with a Self-Attentive Encoder
27. [Distributed Representations of Sentences and Documents](2014-distributed-representations-of-sentences-and-documents)
28. [Neural Machine Translation of Rare Words with Subword Units](2016-neural-machine-translation-of-rare-words-with-subword-units)
29. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
30. SciTaiL - A Textual Entailment Dataset from Science Question Answering
31. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
32. [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](2017-semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation)
33. Multi-range Reasoning for Machine Comprehension
34. Generating Wikipedia by Summarizing Long Sequences
35. Semi-supervised Sequence Learning
36. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
37. Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition
38. Discourse-Based Objectives for Fast Unsupervised Sentence Representation Learning
39. Semi-Supervised Text Classification Using EM
40. Discriminative Improvements to Distributional Sentence Similarity
41. Story Comprehension for Predicting What Happens Next
42. [Convolutional Neural Networks for Sentence Classification](2014-convolutional-neural-networks-for-sentence-classification)
43. The Sixth PASCAL Recognizing Textual Entailment Challenge
44. [RACE - Large-scale ReAding Comprehension Dataset From Examinations](2017-race-large-scale-reading-comprehension-dataset-from-examinations)
45. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
46. ECNU at SemEval-2017 Task 1 - Leverage Kernel-based Traditional NLP features and Neural Networks to Build a Universal Model for Multilingual and Cross-lingual Semantic Textual Similarity
47. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
48. A Simple and Effective Approach to the Story Cloze Test
49. Fixing Weight Decay Regularization in Adam
50. Learning Entity Representation for Entity Disambiguation
51. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
52. Greedy Layer-Wise Training of Deep Networks
53. Towards Human-level Machine Reading Comprehension - Reasoning and Inference with Multiple Strategies
54. [A Fast and Accurate Dependency Parser using Neural Networks](2014-a-fast-and-accurate-dependency-parser-using-neural-networks)
55. Quora Question Pairs
56. Stochastic Answer Networks for Natural Language Inference
57. Resolving Complex Cases of Definite Pronouns - The Winograd Schema Challenge
58. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
59. [Extracting and composing robust features with denoising autoencoders](2008-extracting-and-composing-robust-features-with-denoising-autoencoders)
60. Semi-Supervised Conditional Random Fields for Improved Sequence Segmentation and Labeling
61. Automatically Constructing a Corpus of Sentential Paraphrases
62. Split-Brain Autoencoders - Unsupervised Learning by Cross-Channel Prediction
63. LSDSem 2017 Shared Task - The Story Cloze Test
64. [Layer Normalization](2016-layer-normalization)
65. Efficient Learning of Sparse Representations with an Energy-Based Model
66. Semi-Supervised Text Classification Using EM
67. GPU Kernels for Block-Sparse Weights
68. [Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units](2016-bridging-nonlinearities-and-stochastic-regularizers-with-gaussian-error-linear-units)
69. [A Stochastic Approximation Method](2007-a-stochastic-approximation-method)
70. Semi-Supervised Learning Literature Survey
71. Why Does Unsupervised Pre-training Help Deep Learning?
