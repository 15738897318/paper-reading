---
title: Learning both Weights and Connections for Efficient Neural Network
authors:
- Song Han
- Jeff Pool
- J. Tran
- W. Dally
fieldsOfStudy:
- Computer Science
meta_key: 2015-learning-both-weights-and-connections-for-efficient-neural-network
numCitedBy: 4076
reading_status: TBD
ref_count: 40
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff?sort=total-citations
venue: NIPS
year: 2015
---

# Learning both Weights and Connections for Efficient Neural Network

## Abstract

Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.

## Paper References

1. [Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding](2016-deep-compression-compressing-deep-neural-network-with-pruning-trained-quantization-and-huffman-coding.md)
2. Data-free Parameter Pruning for Deep Neural Networks
3. Improving the speed of neural networks on CPUs
4. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
5. Memory Bounded Deep Convolutional Networks
6. [Network In Network](2014-network-in-network.md)
7. [How transferable are features in deep neural networks?](2014-how-transferable-are-features-in-deep-neural-networks.md)
8. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
9. Compressing Deep Convolutional Networks using Vector Quantization
10. Compressing Neural Networks with the Hashing Trick
11. Second Order Derivatives for Network Pruning - Optimal Brain Surgeon
12. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
13. Improving Generalization of Neural Networks Through Pruning
14. Deep Fried Convnets
15. [Regularization of Neural Networks using DropConnect](2013-regularization-of-neural-networks-using-dropconnect.md)
16. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
17. [Predicting Parameters in Deep Learning](2013-predicting-parameters-in-deep-learning.md)
18. [Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation](2014-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.md)
19. Deep learning with COTS HPC systems
20. Learning long-term dependencies with gradient descent is difficult
21. [Caffe - Convolutional Architecture for Fast Feature Embedding](2014-caffe-convolutional-architecture-for-fast-feature-embedding.md)
22. Comparing Biases for Minimal Network Construction with Back-Propagation
23. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition.md)
24. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch.md)
25. Optimal Brain Damage
26. [DeepFace - Closing the Gap to Human-Level Performance in Face Verification](2014-deepface-closing-the-gap-to-human-level-performance-in-face-verification.md)
27. [Framewise phoneme classification with bidirectional LSTM and other neural network architectures](2005-framewise-phoneme-classification-with-bidirectional-lstm-and-other-neural-network-architectures.md)
28. Neuronal mechanisms of developmental plasticity in the cat's visual system.
29. Neuronal mechanisms of developmental plasticity in the cat's visual system.
30. Feature hashing for large scale multitask learning
31. Hash Kernels for Structured Data
32. Nature
33. Peter Huttenlocher (1931-2013)
