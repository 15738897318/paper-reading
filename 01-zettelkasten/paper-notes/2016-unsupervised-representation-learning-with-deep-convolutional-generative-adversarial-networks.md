---
title: Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks
authors:
- Alec Radford
- Luke Metz
- Soumith Chintala
fieldsOfStudy:
- Computer Science
meta_key: 2016-unsupervised-representation-learning-with-deep-convolutional-generative-adversarial-networks
numCitedBy: 9978
reading_status: TBD
ref_count: 59
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Unsupervised-Representation-Learning-with-Deep-Radford-Metz/8388f1be26329fa45e5807e968a641ce170ea078?sort=total-citations
venue: ICLR
year: 2016
---

# Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks

## Abstract

In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.

## Paper References

1. Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks
2. Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks
3. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations
4. [Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks](2014-learning-and-transferring-mid-level-image-representations-using-convolutional-neural-networks.md)
5. [Striving for Simplicity - The All Convolutional Net](2015-striving-for-simplicity-the-all-convolutional-net.md)
6. [Stacked Denoising Autoencoders - Learning Useful Representations in a Deep Network with a Local Denoising Criterion](2010-stacked-denoising-autoencoders-learning-useful-representations-in-a-deep-network-with-a-local-denoising-criterion.md)
7. Improved Techniques for Training GANs
8. LSUN - Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop
9. Selecting Receptive Fields in Deep Networks
10. Generative Adversarial Nets
11. [Visualizing and Understanding Convolutional Networks](2014-visualizing-and-understanding-convolutional-networks.md)
12. Semi-supervised Learning with Ladder Networks
13. Dreaming More Data - Class-dependent Distributions over Diffeomorphisms for Learned Data Augmentation
14. Semi-Supervised Learning with Ladder Network
15. [DRAW - A Recurrent Neural Network For Image Generation](2015-draw-a-recurrent-neural-network-for-image-generation.md)
16. Stacked What-Where Auto-encoders
17. Synthesizing the preferred inputs for neurons in neural networks via deep generator networks
18. Empirical Evaluation of Rectified Activations in Convolutional Network
19. Learning to generate chairs with convolutional neural networks
20. Understanding Locally Competitive Networks
21. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
22. Learning to Generate Chairs, Tables and Cars with Convolutional Networks
23. A note on the evaluation of generative models
24. [Reading Digits in Natural Images with Unsupervised Feature Learning](2011-reading-digits-in-natural-images-with-unsupervised-feature-learning.md)
25. [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](2016-under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
26. Learning Feature Representations with K-Means
27. Unrolled Generative Adversarial Networks
28. Deep Unsupervised Learning using Nonequilibrium Thermodynamics
29. [Auto-Encoding Variational Bayes](2014-auto-encoding-variational-bayes.md)
30. [Rectifier Nonlinearities Improve Neural Network Acoustic Models](2013-rectifier-nonlinearities-improve-neural-network-acoustic-models.md)
31. Train faster, generalize better - Stability of stochastic gradient descent
32. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
33. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines.md)
34. Scene completion using millions of photographs
35. Sparse probabilistic projections
36. Propagation Algorithms for Variational Bayesian Learning
37. Variational Learning for Switching State-Space Models
38. Hierarchical Mixtures of Experts and the EM Algorithm
39. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality.md)
40. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database.md)
41. Example-Based Super-Resolution
42. A Parametric Texture Model Based on Joint Statistics of Complex Wavelet Coefficients
43. Factorial Hidden Markov Models
44. Variational EM Algorithms for Non-Gaussian Latent Variable Models
45. Texture synthesis by non-parametric sampling
46. Training Invariant Support Vector Machines using Selective Sampling
47. An Input Output HMM Architecture
48. [Random Search for Hyper-Parameter Optimization](2012-random-search-for-hyper-parameter-optimization.md)
49. A Probabilistic Interpretation of Canonical Correlation Analysis
50. Inceptionism - Going Deeper into Neural Networks
