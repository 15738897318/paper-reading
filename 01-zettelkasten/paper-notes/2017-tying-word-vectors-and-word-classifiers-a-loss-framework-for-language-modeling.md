---
title: Tying Word Vectors and Word Classifiers - A Loss Framework for Language Modeling
authors:
- Hakan Inan
- Khashayar Khosravi
- R. Socher
fieldsOfStudy:
- Computer Science
meta_key: 2017-tying-word-vectors-and-word-classifiers-a-loss-framework-for-language-modeling
numCitedBy: 337
reading_status: TBD
ref_count: 34
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Tying-Word-Vectors-and-Word-Classifiers:-A-Loss-for-Inan-Khosravi/424aef7340ee618132cc3314669400e23ad910ba?sort=total-citations
venue: ICLR
year: 2017
---

[semanticscholar url](https://www.semanticscholar.org/paper/Tying-Word-Vectors-and-Word-Classifiers:-A-Loss-for-Inan-Khosravi/424aef7340ee618132cc3314669400e23ad910ba?sort=total-citations)

# Tying Word Vectors and Word Classifiers - A Loss Framework for Language Modeling

## Abstract

Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.

## Paper References

1. Context dependent recurrent neural network language model
2. [Using the Output Embedding to Improve Language Models](2017-using-the-output-embedding-to-improve-language-models.md)
3. Improved Learning through Augmenting the Loss
4. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation.md)
5. [Character-Aware Neural Language Models](2016-character-aware-neural-language-models.md)
6. [Pointer Sentinel Mixture Models](2017-pointer-sentinel-mixture-models.md)
7. A Neural Probabilistic Language Model
8. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank.md)
9. LSTM, GRU, Highway and a Bit of Attention - An Empirical Overview for Language Modeling in Speech Recognition
10. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model.md)
11. [Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond](2016-abstractive-text-summarization-using-sequence-to-sequence-rnns-and-beyond.md)
12. [Recurrent Highway Networks](2017-recurrent-highway-networks.md)
13. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality.md)
14. Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism
15. Language modeling with sum-product networks
16. Three new graphical models for statistical language modelling
17. [A Neural Attention Model for Abstractive Sentence Summarization](2015-a-neural-attention-model-for-abstractive-sentence-summarization.md)
18. [Distilling the Knowledge in a Neural Network](2015-distilling-the-knowledge-in-a-neural-network.md)
19. [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](2016-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.md)
20. [On the Properties of Neural Machine Translation - Encoder-Decoder Approaches](2014-on-the-properties-of-neural-machine-translation-encoder-decoder-approaches.md)
21. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization.md)
22. How to Construct Deep Recurrent Neural Networks
23. Building a Large Annotated Corpus of English - The Penn Treebank
24. [Long Short-Term Memory](1997-long-short-term-memory.md)
25. Learning with a Wasserstein Loss
26. [On the difficulty of training recurrent neural networks](2013-on-the-difficulty-of-training-recurrent-neural-networks.md)
27. [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](2018-published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks.md)
28. [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](2016-under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
29. Numerical methods for computing angles between linear subspaces
30. Bulletin de la Société Mathématique de France
31. Advances in Neural Information Processing
