---
title: On the Optimality of the Simple Bayesian Classifier under Zero-One Loss
authors:
- Pedro M. Domingos
- M. Pazzani
fieldsOfStudy:
- Computer Science
meta_key: 2004-on-the-optimality-of-the-simple-bayesian-classifier-under-zero-one-loss
numCitedBy: 3137
reading_status: TBD
ref_count: 60
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/On-the-Optimality-of-the-Simple-Bayesian-Classifier-Domingos-Pazzani/700666f0c59a4fedc8b08294424c47cb99a8e2ff?sort=total-citations
venue: Machine Learning
year: 2004
---

[semanticscholar url](https://www.semanticscholar.org/paper/On-the-Optimality-of-the-Simple-Bayesian-Classifier-Domingos-Pazzani/700666f0c59a4fedc8b08294424c47cb99a8e2ff?sort=total-citations)

# On the Optimality of the Simple Bayesian Classifier under Zero-One Loss

## Abstract

The simple Bayesian classifier is known to be optimal when attributes are independent given the class, but the question of whether other sufficient conditions for its optimality exist has so far not been explored. Empirical results showing that it performs surprisingly well in many domains containing clear attribute dependences suggest that the answer to this question may be positive. This article shows that, although the Bayesian classifier's probability estimates are only optimal under quadratic loss if the independence assumption holds, the classifier itself can be optimal under zero-one loss (misclassification rate) even when this assumption is violated by a wide margin. The region of quadratic-loss optimality of the Bayesian classifier is in fact a second-order infinitesimal fraction of the region of zero-one optimality. This implies that the Bayesian classifier has a much greater range of applicability than previously thought. For example, in this article it is shown to be optimal for learning conjunctions and disjunctions, even though they violate the independence assumption. Further, studies in artificial domains show that it will often outperform more powerful classifiers for common training set sizes and numbers of attributes, even if its bias is a priori much less appropriate to the domain. This article's results also imply that detecting attribute dependence is not necessarily the best way to extend the Bayesian classifier, and this is also verified empirically.

## Paper References

1. Beyond Independence - Conditions for the Optimality of the Simple Bayesian Classifier
2. Bayesian Network Classifiers
3. Learning Limited Dependence Bayesian Classifiers
4. An Analysis of Bayesian Classiiers
5. On Bias, Variance, 0/1-Loss, and the Curse-of-Dimensionality
6. An Analysis of Bayesian Classifiers
7. Sensitivity Analysis in Bayesian Classification Models - Multiplicative Deviations
8. Searching for Dependencies in Bayesian Classifiers
9. Induction of Selective Bayesian Classifiers
10. Eecient Learning of Selective Bayesian Network Classiiers
11. A Comparison of Induction Algorithms for Selective and non-Selective Bayesian Classifiers
12. Scaling Up the Accuracy of Naive-Bayes Classifiers - A Decision-Tree Hybrid
13. Estimating Continuous Distributions in Bayesian Classifiers
14. Induction of Recursive Bayesian Classifiers
15. The effect of assuming independence in applying Bayes' theorem to risk estimation and classification in diagnosis.
16. Error rates in quadratic discrimination with constraints on the covariance matrices
17. Bias, Variance , And Arcing Classifiers
18. Efficient Learning of Selective Bayesian Network Classifiers
19. Semi-Naive Bayesian Classifier
20. Quantifying Inductive Bias - AI Learning Algorithms and Valiant's Learning Framework
21. Wrappers for Performance Enhancements and Oblivious Decision Graphs.
22. Supervised and Unsupervised Discretization of Continuous Features
23. Bias Plus Variance Decomposition for Zero-One Loss Functions
24. A Weighted Nearest Neighbor Algorithm for Learning with Symbolic Features
25. [C4.5 - Programs for Machine Learning](1992-c4-5-programs-for-machine-learning.md)
26. A framework for average case analysis of conjunctive learning algorithms
27. Probability and Statistics
28. Syskill & Webert - Identifying Interesting Web Sites
29. Error-Correcting Output Coding Corrects Bias and Variance
30. Bias, Variance and Prediction Error for Classification Rules
31. Learning from Data - Artificial Intelligence and Statistics V
32. Rule Induction with CN2 - Some Recent Improvements
33. Pattern classification and scene analysis
34. The CN2 Induction Algorithm
35. [UCI Repository of machine learning databases](1998-uci-repository-of-machine-learning-databases.md)
36. Discovering Patterns in EEG-Signals - Comparative Study of a Few Methods
37. Improving simple Bayes
38. Estimating Probabilities - A Crucial Task in Machine Learning
39. Constructing Decision Trees in Noisy Domains
