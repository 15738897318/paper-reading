---
title: Distributed Representations of Sentences and Documents
authors:
- Quoc V. Le
- Tomas Mikolov
fieldsOfStudy:
- Computer Science
meta_key: 2014-distributed-representations-of-sentences-and-documents
numCitedBy: 7075
reading_status: TBD
ref_count: 49
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Distributed-Representations-of-Sentences-and-Le-Mikolov/f527bcfb09f32e6a4a8afc0b37504941c1ba2cee?sort=total-citations
venue: ICML
year: 2014
---

[semanticscholar url](https://www.semanticscholar.org/paper/Distributed-Representations-of-Sentences-and-Le-Mikolov/f527bcfb09f32e6a4a8afc0b37504941c1ba2cee?sort=total-citations)

# Distributed Representations of Sentences and Documents

## Abstract

Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.

## Paper References

1. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
2. A Neural Probabilistic Language Model
3. Neural Probabilistic Language Models
4. A Neural Autoregressive Topic Model
5. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
6. [Linguistic Regularities in Continuous Space Word Representations](2013-linguistic-regularities-in-continuous-space-word-representations)
7. [Learning Word Vectors for Sentiment Analysis](2011-learning-word-vectors-for-sentiment-analysis)
8. Word Representations - A Simple and General Method for Semi-Supervised Learning
9. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
10. Training Restricted Boltzmann Machines on Word Observations
11. Hierarchical Probabilistic Neural Network Language Model
12. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
13. Statistical Language Models Based on Neural Networks
14. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
15. Modeling documents with a Deep Boltzmann Machine
16. Reasoning With Neural Tensor Networks for Knowledge Base Completion
17. [DeViSE - A Deep Visual-Semantic Embedding Model](2013-devise-a-deep-visual-semantic-embedding-model)
18. Modeling Documents with Deep Boltzmann Machines
19. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
20. From Frequency to Meaning - Vector Space Models of Semantics
21. Large-scale image retrieval with compressed Fisher vectors
22. [Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions](2011-semi-supervised-recursive-autoencoders-for-predicting-sentiment-distributions)
23. Compositional Matrix-Space Models for Sentiment Analysis
24. Seeing Stars - Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales
25. Exploiting Similarities among Languages for Machine Translation
26. Composition in Distributional Models of Semantics
27. [Baselines and Bigrams - Simple, Good Sentiment and Topic Classification](2012-baselines-and-bigrams-simple-good-sentiment-and-topic-classification)
28. A Scalable Hierarchical Distributed Language Model
29. Distributional Structure
30. Fisher Kernels on Visual Vocabularies for Image Categorization
31. [Parsing Natural Scenes and Natural Language with Recursive Neural Networks](2011-parsing-natural-scenes-and-natural-language-with-recursive-neural-networks)
32. Finding Structure in Time
33. Exploiting Generative Models in Discriminative Classifiers
34. Learning representations by back-propagating errors
35. Bilingual Word Embeddings for Phrase-Based Machine Translation
36. Estimating Linear Models for Compositional Distributional Semantics
37. Combining Heterogeneous Models for Measuring Relational Similarity
38. Learning representations by backpropagating errors
39. [Accurate Unlexicalized Parsing](2003-accurate-unlexicalized-parsing)
40. Learning representations by back-propagation errors, nature
41. [Improving Word Representations via Global Context and Multiple Word Prototypes](2012-improving-word-representations-via-global-context-and-multiple-word-prototypes)
42. Multi-Step Regression Learning for Compositional Distributional Semantics
