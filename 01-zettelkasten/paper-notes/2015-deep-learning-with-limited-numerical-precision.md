---
title: Deep Learning with Limited Numerical Precision
authors:
- Suyog Gupta
- A. Agrawal
- K. Gopalakrishnan
- P. Narayanan
fieldsOfStudy:
- Computer Science
meta_key: 2015-deep-learning-with-limited-numerical-precision
numCitedBy: 1510
reading_status: TBD
ref_count: 38
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Deep-Learning-with-Limited-Numerical-Precision-Gupta-Agrawal/b7cf49e30355633af2db19f35189410c8515e91f?sort=total-citations
venue: ICML
year: 2015
---

# Deep Learning with Limited Numerical Precision

## Abstract

Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding.

## Paper References

1. Probabilistic rounding in neural network learning with limited precision
2. Training deep neural networks with low precision multiplications
3. Low precision arithmetic for deep learning
4. Improving the speed of neural networks on CPUs
5. Finite Precision Error Analysis of Neural Network Hardware Implementations
6. Learning Machines Implemented on Non-Deterministic Hardware
7. Deep learning with COTS HPC systems
8. [DaDianNao - A Machine-Learning Supercomputer](2014-dadiannao-a-machine-learning-supercomputer.md)
9. [Large Scale Distributed Deep Networks](2012-large-scale-distributed-deep-networks.md)
10. A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks
11. Project Adam - Building an Efficient and Scalable Deep Learning Training System
12. X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks
13. Deep Image - Scaling up Image Recognition
14. Accelerating scientific computations with mixed precision algorithms
15. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
16. Enhanced MLP performance and fault tolerance resulting from synaptic weight noise during training
17. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
18. Deep, Big, Simple Neural Nets for Handwritten Digit Recognition
19. A VLSI architecture for high-performance, low-cost, on-chip learning
20. An artificial neural network accelerator using general purpose 24 bit floating point digital signal processors
21. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
22. [Hogwild - A Lock-Free Approach to Parallelizing Stochastic Gradient Descent](2011-hogwild-a-lock-free-approach-to-parallelizing-stochastic-gradient-descent.md)
23. Training with Noise is Equivalent to Tikhonov Regularization
24. Why systolic architectures?
25. Noise benefits in backpropagation and deep bidirectional pre-training
26. NeuFlow - A runtime reconfigurable dataflow processor for vision
27. A million spiking-neuron integrated circuit with a scalable communication network and interface
28. The Effects of Adding Noise During Backpropagation Training on a Generalization Performance
29. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition.md)
30. [GradientBased Learning Applied to Document Recognition](2001-gradientbased-learning-applied-to-document-recognition.md)
31. The Tradeoffs of Large Scale Learning
32. [The mnist database of handwritten digits](2005-the-mnist-database-of-handwritten-digits.md)
