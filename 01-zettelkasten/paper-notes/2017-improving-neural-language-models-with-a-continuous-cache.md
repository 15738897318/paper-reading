---
title: Improving Neural Language Models with a Continuous Cache
authors:
- Edouard Grave
- Armand Joulin
- Nicolas Usunier
fieldsOfStudy:
- Computer Science
meta_key: 2017-improving-neural-language-models-with-a-continuous-cache
numCitedBy: 241
reading_status: TBD
ref_count: 53
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Improving-Neural-Language-Models-with-a-Continuous-Grave-Joulin/2d7782c225e0fc123d6e227f2cb253e58279ac73?sort=total-citations
venue: ICLR
year: 2017
---

[semanticscholar url](https://www.semanticscholar.org/paper/Improving-Neural-Language-Models-with-a-Continuous-Grave-Joulin/2d7782c225e0fc123d6e227f2cb253e58279ac73?sort=total-citations)

# Improving Neural Language Models with a Continuous Cache

## Abstract

We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.

## Paper References

1. [Exploring the Limits of Language Modeling](2016-exploring-the-limits-of-language-modeling.md)
2. Empirical Evaluation and Combination of Advanced Language Modeling Techniques
3. [Pointer Sentinel Mixture Models](2017-pointer-sentinel-mixture-models.md)
4. [End-To-End Memory Networks](2015-end-to-end-memory-networks.md)
5. Context dependent recurrent neural network language model
6. Efficient softmax approximation for GPUs
7. Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
8. Learning Longer Memory in Recurrent Neural Networks
9. A Cache-Based Natural Language Model for Speech Recognition
10. Learning to Transduce with Unbounded Memory
11. Modeling long distance dependence in language - topic mixtures versus dynamic cache models
12. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](2014-empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling.md)
13. Towards better integration of semantic predictors in statistical language modeling
14. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization.md)
15. Pointing the Unknown Words
16. [Recurrent Highway Networks](2017-recurrent-highway-networks.md)
17. On the dynamic adaptation of stochastic language models
18. Larger-Context Language Modelling
19. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model.md)
20. A bit of progress in language modeling
21. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend.md)
22. [Text Understanding with the Attention Sum Reader Network](2016-text-understanding-with-the-attention-sum-reader-network.md)
23. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
24. A Dynamic Language Model for Speech Recognition
25. [Neural Turing Machines](2014-neural-turing-machines.md)
26. A Neural Probabilistic Language Model
27. [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](2016-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks.md)
28. [Long Short-Term Memory](1997-long-short-term-memory.md)
29. Finding Structure in Time
30. Adaptive Language Modeling Using Minimum Discriminant Estimation
31. [Speech recognition with deep recurrent neural networks](2013-speech-recognition-with-deep-recurrent-neural-networks.md)
32. Exploiting latent semantic information in statistical language modeling
33. Trigger-based language models - a maximum entropy approach
34. Probabilistic Models of Short and Long Distance Word Dependencies in Running Text
35. Maximum entropy techniques for exploiting syntactic, semantic and collocational dependencies in language modeling
36. Improved backing-off for M-gram language modeling
37. The LAMBADA dataset - Word prediction requiring a broad discourse context
38. Evaluating Prerequisite Qualities for Learning End-to-End Dialog Systems
39. [A Thorough Examination of the CNN/Daily Mail Reading Comprehension Task](2016-a-thorough-examination-of-the-cnn-daily-mail-reading-comprehension-task.md)
40. Estimation of probabilities from sparse data for the language model component of a speech recognizer
41. A Maximum Entropy Approach to Adaptive Statistical Language Modeling
42. Speech Recognition and the Frequency of Recently Used Words - A Modified Markov Model for Natural Language
43. Building a Large Annotated Corpus of English - The Penn Treebank
44. A maximum entropy approach to adaptive statistical language modelling
45. An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories
46. The Mathematics of Statistical Machine Translation - Parameter Estimation
47. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization.md)
48. [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](2016-under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
49. Backpropagation Through Time - What It Does and How to Do It
50. Dialogue act modeling for automatic tagging and recognition of conversational speech
51. A Maximum Likelihood Approach to Continuous Speech Recognition
