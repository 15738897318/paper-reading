---
title: LSTM - A Search Space Odyssey
authors:
- Klaus Greff
- R. Srivastava
- "J. Koutn\xEDk"
- Bas R. Steunebrink
- J. Schmidhuber
fieldsOfStudy:
- Computer Science
meta_key: 2017-lstm-a-search-space-odyssey
numCitedBy: 3313
reading_status: TBD
ref_count: 64
tags:
- gen-from-ref
- paper
venue: IEEE Transactions on Neural Networks and Learning Systems
year: 2017
---

# LSTM - A Search Space Odyssey

## Abstract

Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ( $\approx 15$  years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.

## Paper References

1. An Empirical Exploration of Recurrent Network Architectures
2. Long short-term memory recurrent neural network architectures for large scale acoustic modeling
3. Training Recurrent Networks by Evolino
4. [Speech recognition with deep recurrent neural networks](2013-speech-recognition-with-deep-recurrent-neural-networks)
5. Learning to Forget - Continual Prediction with LSTM
6. [Framewise phoneme classification with bidirectional LSTM and other neural network architectures](2005-framewise-phoneme-classification-with-bidirectional-lstm-and-other-neural-network-architectures)
7. Dropout Improves Recurrent Neural Networks for Handwriting Recognition
8. Multi-resolution linear prediction based features for audio onset detection with bidirectional LSTM neural networks
9. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](2014-empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling)
10. Dynamic Cortex Memory - Enhancing Recurrent Neural Networks for Gradient-Based Sequence Learning
11. [Long Short-Term Memory](1997-long-short-term-memory)
12. TTS synthesis with bidirectional LSTM based recurrent neural networks
13. Fast and Robust Training of Recurrent Neural Networks for Offline Handwriting Recognition
14. [Connectionist temporal classification - labelling unsegmented sequence data with recurrent neural networks](2006-connectionist-temporal-classification-labelling-unsegmented-sequence-data-with-recurrent-neural-networks)
15. [On the importance of initialization and momentum in deep learning](2013-on-the-importance-of-initialization-and-momentum-in-deep-learning)
16. Recurrent nets that time and count
17. Learning long-term dependencies with gradient descent is difficult
18. Evolving Memory Cell Structures for Sequence Learning
19. Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks
20. [Long-term recurrent convolutional networks for visual recognition and description](2015-long-term-recurrent-convolutional-networks-for-visual-recognition-and-description)
21. Addressing the Rare Word Problem in Neural Machine Translation
22. Supervised Sequence Labelling with Recurrent Neural Networks
23. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
24. [Practical Bayesian Optimization of Machine Learning Algorithms](2012-practical-bayesian-optimization-of-machine-learning-algorithms)
25. Generalization of backpropagation with application to a recurrent gas market model
26. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
27. An Efficient Approach for Assessing Hyperparameter Importance
28. Protein Secondary Structure Prediction with Long Short Term Memory Networks
29. Modeling Temporal Dependencies in High-Dimensional Sequences - Application to Polyphonic Music Generation and Transcription
30. IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard
31. Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
32. Heterogeneous acoustic measurements and multiple classifiers for speech recognition
33. Sequential Model-Based Optimization for General Algorithm Configuration
34. Random Search for Hyper-Parameter Optimization
35. Generalized Functional ANOVA Diagnostics for High-Dimensional Functions of Dependent Variables
36. Association
37. Harmonising Chorales by Probabilistic Inference
38. References
39. Minimization by Random Search Techniques
40. A dictionary of linguistics and phonetics
41. DARPA TIMIT - - acoustic-phonetic continuous speech corpus CD-ROM, NIST speech disc 1-1.1
42. Distance measures for speech recognition, psychological and instrumental
43. RECENT ADVANCES IN FINDING BEST OPERATING CONDITIONS
44. Darpa Timit Acoustic-Phonetic Continuous Speech Corpus CD-ROM {TIMIT} | NIST
45. Untersuchungen zu dynamischen neuronalen Netzen
46. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization)
47. A Novel Connectionist System for Unconstrained Handwriting Recognition
