---
title: RoBERTa - A Robustly Optimized BERT Pretraining Approach
authors:
- Yinhan Liu
- Myle Ott
- Naman Goyal
- Jingfei Du
- Mandar Joshi
- Danqi Chen
- Omer Levy
- M. Lewis
- Luke Zettlemoyer
- Veselin Stoyanov
fieldsOfStudy:
- Computer Science
meta_key: 2019-roberta-a-robustly-optimized-bert-pretraining-approach
numCitedBy: 7554
reading_status: TBD
ref_count: 58
tags:
- gen-from-ref
- other-default
- paper
venue: ArXiv
year: 2019
---

# RoBERTa - A Robustly Optimized BERT Pretraining Approach

## Abstract

Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.

## Paper References

1. SuperGLUE - A Stickier Benchmark for General-Purpose Language Understanding Systems
2. [XLNet - Generalized Autoregressive Pretraining for Language Understanding](2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding)
3. Cloze-driven Pretraining of Self-attention Networks
4. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
5. Reducing BERT Pre-Training Time from 3 Days to 76 Minutes
6. [SpanBERT - Improving Pre-training by Representing and Predicting Spans](2020-spanbert-improving-pre-training-by-representing-and-predicting-spans)
7. [Automatic differentiation in PyTorch](2017-automatic-differentiation-in-pytorch)
8. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
9. Scaling Neural Machine Translation
10. A Surprisingly Robust Trick for the Winograd Schema Challenge
11. [Universal Language Model Fine-tuning for Text Classification](2018-universal-language-model-fine-tuning-for-text-classification)
12. [Attention is All you Need](2017-attention-is-all-you-need.md)
13. [Language Models are Unsupervised Multitask Learners](2019-language-models-are-unsupervised-multitask-learners)
14. A Simple Method for Commonsense Reasoning
15. [Multi-Task Deep Neural Networks for Natural Language Understanding](2019-multi-task-deep-neural-networks-for-natural-language-understanding)
16. Semi-supervised Sequence Learning
17. [Cross-lingual Language Model Pretraining](2019-cross-lingual-language-model-pretraining)
18. Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding
19. Mixed Precision Training
20. [Unified Language Model Pre-training for Natural Language Understanding and Generation](2019-unified-language-model-pre-training-for-natural-language-understanding-and-generation)
21. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
22. MASS - Masked Sequence to Sequence Pre-training for Language Generation
23. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
24. KERMIT - Generative Insertion-Based Modeling for Sequences
25. [Learned in Translation - Contextualized Word Vectors](2017-learned-in-translation-contextualized-word-vectors)
26. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
27. fairseq - A Fast, Extensible Toolkit for Sequence Modeling
28. [Know What You Don't Know - Unanswerable Questions for SQuAD](2018-know-what-you-don-t-know-unanswerable-questions-for-squad)
29. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
30. [A large annotated corpus for learning natural language inference](2015-a-large-annotated-corpus-for-learning-natural-language-inference)
31. [RACE - Large-scale ReAding Comprehension Dataset From Examinations](2017-race-large-scale-reading-comprehension-dataset-from-examinations)
32. [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](2018-a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference)
33. The Sixth PASCAL Recognizing Textual Entailment Challenge
34. [Neural Network Acceptability Judgments](2019-neural-network-acceptability-judgments)
35. ERNIE - Enhanced Representation through Knowledge Integration
36. [Neural Machine Translation of Rare Words with Subword Units](2016-neural-machine-translation-of-rare-words-with-subword-units)
37. The Second PASCAL Recognising Textual Entailment Challenge
38. Defending Against Neural Fake News
39. The Seventh PASCAL Recognizing Textual Entailment Challenge
40. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
41. The PASCAL Recognising Textual Entailment Challenge
42. [Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units](2016-bridging-nonlinearities-and-stochastic-regularizers-with-gaussian-error-linear-units)
43. The Winograd Schema Challenge
44. Gaussian Error Linear Units (GELUs)
45. Automatically Constructing a Corpus of Sentential Paraphrases
46. news-please - A Generic News Crawler and Extractor
47. The Third PASCAL Recognizing Textual Entailment Challenge
48. Fine-tuned Language Models for Text Classification
