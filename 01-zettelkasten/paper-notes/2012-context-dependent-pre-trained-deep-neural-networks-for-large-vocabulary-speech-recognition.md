---
title: Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition
authors:
- George E. Dahl
- Dong Yu
- L. Deng
- A. Acero
fieldsOfStudy:
- Computer Science
meta_key: 2012-context-dependent-pre-trained-deep-neural-networks-for-large-vocabulary-speech-recognition
numCitedBy: 2679
reading_status: TBD
ref_count: 93
tags:
- gen-from-ref
- other-default
- paper
venue: IEEE Transactions on Audio, Speech, and Language Processing
year: 2012
---

# Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition

## Abstract

We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8% and 9.2% (or relative error reduction of 16.0% and 23.2%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.

## Paper References

1. Roles of Pre-Training and Fine-Tuning in Context-Dependent DBN-HMMs for Real-World Speech Recognition
2. Context-dependent connectionist probability estimation in a hybrid hidden Markov model-neural net speech recognition system
3. Large vocabulary continuous speech recognition with context-dependent DBN-HMMS
4. CDNN - a context dependent neural network for continuous speech recognition
5. Investigation of full-sequence training of deep belief networks for speech recognition
6. Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine
7. A segmental CRF approach to large vocabulary continuous speech recognition
8. Speech Recognition Using Augmented Conditional Random Fields
9. Deep Belief Networks for phone recognition
10. Discriminative Training for Large-Vocabulary Speech Recognition Using Minimum Classification Error
11. Deep-structured hidden conditional random fields for phonetic recognition
12. Using MLP features in SRI's conversational speech recognition system
13. [Acoustic Modeling Using Deep Belief Networks](2012-acoustic-modeling-using-deep-belief-networks)
14. A log-linear discriminative modeling framework for speech recognition
15. Large-Margin Discriminative Training of Hidden Markov Models for Speech Recognition
16. Large margin HMMs for speech recognition
17. Connectionist Speech Recognition - A Hybrid Approach
18. SCARF - a segmental conditional random field toolkit for speech recognition
19. Tandem connectionist feature extraction for conventional HMM systems
20. A Novel Framework and Training Algorithm for Variable-Parameter Hidden Markov Models
21. On adaptive decision rules and decision parameter adaptation for automatic speech recognition
22. Large-Margin Minimum Classification Error Training for Large-Scale Speech Recognition Tasks
23. A survey of hybrid ANN/HMM models for automatic speech recognition
24. Structured speech modeling
25. A unified framework of HMM adaptation with joint compensation of additive and convolutive distortions
26. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
27. Transcribing broadcast data using MLP features
28. Continuous speech recognition using multilayer perceptrons with hidden Markov models
29. Boosted MMI for model and feature-space discriminative training
30. Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system
31. Speaker-adaptation in a hybrid HMM-MLP recognizer
32. Minimum Phone Error and I-smoothing for improved discriminative training
33. Mean and variance adaptation within the MLLR framework
34. Hidden conditional random fields for phone classification
35. A bidirectional target-filtering model of speech coarticulation and reduction - two-stage implementation for phonetic recognition
36. Why Does Unsupervised Pre-training Help Deep Learning?
37. A comparative large scale study of MLP features for Mandarin ASR
38. Minimum classification error rate methods for speech recognition
39. A dynamic, feature-based approach to the interface between phonology and phonetics for speech modeling and recognition
40. Shared-distribution hidden Markov models for speech recognition
41. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
42. Optimizing bottle-neck features for lvcsr
43. Incorporating Training Errors for Large Margin HMMS Under Semi-Definite Programming Framework
44. Continuous speech recognition by connectionist statistical methods
45. Large Margin Gaussian Mixture Modeling for Phonetic Classification and Recognition
46. MMI training for continuous phoneme recognition on the TIMIT database
47. Computational Models for Speech Production
48. Probabilistic and Bottle-Neck Features for LVCSR of Meetings
49. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks)
50. Connectionist probability estimators in HMM speech recognition
51. 3D Object Recognition with Deep Belief Nets
52. Automated directory assistance system - from theory to practice
53. Pushing the envelope - aside [speech recognition
54. Connectionist speech recognition of Broadcast News
55. [Learning Deep Architectures for AI](2007-learning-deep-architectures-for-ai)
56. [Extracting and composing robust features with denoising autoencoders](2008-extracting-and-composing-robust-features-with-denoising-autoencoders)
57. Use of incrementally regulated discriminative margins in MCE training for speech recognition
58. The Recurrent Temporal Restricted Boltzmann Machine
59. Speech recognition using neural networks with forward-backward probability generated targets
60. Semantic hashing
61. Modeling pixel means and covariances using factorized third-order boltzmann machines
62. Developments and directions in speech recognition and understanding, Part 1 [DSP Education
63. Large-margin minimum classification error training - A theoretical risk minimization perspective
64. Scaling learning algorithms towards AI
65. Discriminative learning in sequential pattern recognition
66. What is the best multi-stage architecture for object recognition?
67. [Reducing the Dimensionality of Data with Neural Networks](2006-reducing-the-dimensionality-of-data-with-neural-networks)
68. Learning representations by back-propagating errors
69. [Training Products of Experts by Minimizing Contrastive Divergence](2002-training-products-of-experts-by-minimizing-contrastive-divergence)
70. Estimation of global posteriors and forward-backward training of hybrid HMM/ANN systems
71. Updated MINDS report on speech recognition and understanding, Part 2 [DSP Education
72. DECISION TREES DO NOT GENERALIZE TO NEW VARIATIONS
73. Development of the SRI/nightingale Arabic ASR system
74. The Curse of Highly Variable Functions for Local Kernel Machines
75. Deep learning via Hessian-free optimization
76. Exponential Family Harmoniums with an Application to Information Retrieval
77. Learning to Detect Roads in High-Resolution Aerial Images
78. Live search for mobile - Web services by voice on the cellphone
79. CUDAMat - a CUDA-based matrix class for Python
80. Information processing in dynamical systems - foundations of harmony theory
81. Research Developments and Directions in Speech Recognition and Understanding, Part 1
82. Scaling Learning Algorithms toward AI
83. An investigation of segmental hidden dynamic models of speech coarticulation for automatic speech recognition
84. A Bidirectional Target Filtering Model of Speech Coarticulation - two-stage Implementation for Phonetic Recognition
85. The Difficulty of Training Deep Architectures and the Effect of Unsupervised Pre-Training
86. Combining phonetic attributes using conditional random fields
