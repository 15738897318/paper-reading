---
title: Unified Language Model Pre-training for Natural Language Understanding and Generation
authors:
- Li Dong
- Nan Yang
- Wenhui Wang
- Furu Wei
- Xiaodong Liu
- Yu Wang
- Jianfeng Gao
- M. Zhou
- H. Hon
fieldsOfStudy:
- Computer Science
meta_key: 2019-unified-language-model-pre-training-for-natural-language-understanding-and-generation
numCitedBy: 743
reading_status: TBD
ref_count: 61
tags:
- gen-from-ref
- other-default
- paper
venue: NeurIPS
year: 2019
---

# Unified Language Model Pre-training for Natural Language Understanding and Generation

## Abstract

This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at this https URL.

## Paper References

1. Cross-Lingual Natural Language Generation via Pre-Training
2. Pre-trained language model representations for language generation
3. [Multi-Task Deep Neural Networks for Natural Language Understanding](2019-multi-task-deep-neural-networks-for-natural-language-understanding)
4. [Language Models are Unsupervised Multitask Learners](2019-language-models-are-unsupervised-multitask-learners)
5. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding)
6. MASS - Masked Sequence to Sequence Pre-training for Language Generation
7. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
8. Text Summarization with Pretrained Encoders
9. [Attention is All you Need](2017-attention-is-all-you-need)
10. A Deep Reinforced Model for Abstractive Summarization
11. [Universal Language Model Fine-tuning for Text Classification](2018-universal-language-model-fine-tuning-for-text-classification)
12. Paragraph-level Neural Question Generation with Maxout Pointer and Gated Self-attention Networks
13. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
14. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
15. Learning to Ask - Neural Question Generation for Reading Comprehension
16. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation)
17. [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](2018-a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference)
18. Get To The Point - Summarization with Pointer-Generator Networks
19. [A Neural Attention Model for Abstractive Sentence Summarization](2015-a-neural-attention-model-for-abstractive-sentence-summarization)
20. Addressing Semantic Drift in Question Generation for Semi-Supervised Question Answering
21. Bottom-Up Abstractive Summarization
22. [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](2017-semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation)
23. Cloze-driven Pretraining of Self-attention Networks
24. Harvesting Paragraph-level Question-Answer Pairs from Wikipedia
25. Neural Question Generation from Text - A Preliminary Study
26. Semi-supervised Sequence Learning
27. Conversing by Reading - Contentful Neural Conversation with On-demand Machine Reading
28. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
29. CoQA - A Conversational Question Answering Challenge
30. Neural Document Summarization by Jointly Learning to Score and Select Sentences
31. Fine-tune BERT for Extractive Summarization
32. [Know What You Don't Know - Unanswerable Questions for SQuAD](2018-know-what-you-don-t-know-unanswerable-questions-for-squad)
33. The Sixth PASCAL Recognizing Textual Entailment Challenge
34. Read + Verify - Machine Reading Comprehension with Unanswerable Questions
35. Cluster-based beam search for pointer-generator chatbot grounded by knowledge
36. Grounded Response Generation Task at DSTC7
37. Retrieve, Rerank and Rewrite - Soft Template Based Neural Summarization
38. [Rethinking the Inception Architecture for Computer Vision](2016-rethinking-the-inception-architecture-for-computer-vision)
39. OpenNMT - Open-Source Toolkit for Neural Machine Translation
40. The Seventh PASCAL Recognizing Textual Entailment Challenge
41. [Neural Network Acceptability Judgments](2019-neural-network-acceptability-judgments)
42. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
43. The Second PASCAL Recognising Textual Entailment Challenge
44. Neural Approaches to Conversational AI
45. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
46. [Long Short-Term Memory](1997-long-short-term-memory)
47. [ROUGE - A Package for Automatic Evaluation of Summaries](2004-rouge-a-package-for-automatic-evaluation-of-summaries)
48. Automatically Constructing a Corpus of Sentential Paraphrases
49. The PASCAL Recognising Textual Entailment Challenge
50. BERT has a Mouth, and It Must Speak - BERT as a Markov Random Field Language Model
51. The Winograd Schema Challenge
52. Gaussian Error Linear Units (GELUs)
53. “Cloze Procedure” - A New Tool for Measuring Readability
54. Learning and Evaluating General Linguistic Intelligence
55. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
56. In Advances in Neural Information Processing Systems
