---
title: ShuffleNet V2 - Practical Guidelines for Efficient CNN Architecture Design
authors:
- Ningning Ma
- X. Zhang
- Haitao Zheng
- Jian Sun
fieldsOfStudy:
- Computer Science
meta_key: 2018-shufflenet-v2-practical-guidelines-for-efficient-cnn-architecture-design
numCitedBy: 1910
reading_status: TBD
ref_count: 48
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/ShuffleNet-V2:-Practical-Guidelines-for-Efficient-Ma-Zhang/c02b909a514af6b9255315e2d50112845ca5ed0e?sort=total-citations
venue: ECCV
year: 2018
---

# ShuffleNet V2 - Practical Guidelines for Efficient CNN Architecture Design

## Abstract

Currently, the neural network architecture design is mostly guided by the indirect metric of computation complexity, i.e., FLOPs. However, the direct metric, e.g., speed, also depends on the other factors such as memory access cost and platform characterics. Thus, this work proposes to evaluate the direct metric on the target platform, beyond only considering FLOPs. Based on a series of controlled experiments, this work derives several practical guidelines for efficient network design. Accordingly, a new architecture is presented, called ShuffleNet V2. Comprehensive ablation experiments verify that our model is the state-of-the-art in terms of speed and accuracy tradeoff.

## Paper References

1. CondenseNet - An Efficient DenseNet Using Learned Group Convolutions
2. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
3. [Learning Efficient Convolutional Networks through Network Slimming](2017-learning-efficient-convolutional-networks-through-network-slimming.md)
4. [Progressive Neural Architecture Search](2018-progressive-neural-architecture-search.md)
5. Deep Roots - Improving CNN Efficiency with Hierarchical Filter Groups
6. [Channel Pruning for Accelerating Very Deep Neural Networks](2017-channel-pruning-for-accelerating-very-deep-neural-networks.md)
7. [Aggregated Residual Transformations for Deep Neural Networks](2017-aggregated-residual-transformations-for-deep-neural-networks.md)
8. [ShuffleNet - An Extremely Efficient Convolutional Neural Network for Mobile Devices](2018-shufflenet-an-extremely-efficient-convolutional-neural-network-for-mobile-devices.md)
9. IGCV3 - Interleaved Low-Rank Group Convolutions for Efficient Deep Neural Networks
10. [Speeding up Convolutional Neural Networks with Low Rank Expansions](2014-speeding-up-convolutional-neural-networks-with-low-rank-expansions.md)
11. Large Kernel Matters - Improve Semantic Segmentation by Global Convolutional Network
12. [Rethinking the Inception Architecture for Computer Vision](2016-rethinking-the-inception-architecture-for-computer-vision.md)
13. [Squeeze-and-Excitation Networks](2020-squeeze-and-excitation-networks.md)
14. [Learning Transferable Architectures for Scalable Image Recognition](2018-learning-transferable-architectures-for-scalable-image-recognition.md)
15. [Genetic CNN](2017-genetic-cnn.md)
16. Efficient and accurate approximations of nonlinear convolutional networks
17. [Accelerating Very Deep Convolutional Networks for Classification and Detection](2016-accelerating-very-deep-convolutional-networks-for-classification-and-detection.md)
18. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
19. Learning Structured Sparsity in Deep Neural Networks
20. Distributed Deep Learning Using Synchronous Stochastic Gradient Descent
21. [MobileNetV2 - Inverted Residuals and Linear Bottlenecks](2018-mobilenetv2-inverted-residuals-and-linear-bottlenecks.md)
22. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](2017-inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning.md)
23. [Large-Scale Evolution of Image Classifiers](2017-large-scale-evolution-of-image-classifiers.md)
24. IGCV$2$ - Interleaved Structured Sparse Convolutional Neural Networks
25. [Xception - Deep Learning with Depthwise Separable Convolutions](2017-xception-deep-learning-with-depthwise-separable-convolutions.md)
26. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
27. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks.md)
28. [MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications](2017-mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications.md)
29. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks.md)
30. Interleaved Structured Sparse Convolutional Neural Networks
31. Light-Head R-CNN - In Defense of Two-Stage Object Detector
32. Interleaved Group Convolutions for Deep Neural Networks
33. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
34. [Regularized Evolution for Image Classifier Architecture Search](2019-regularized-evolution-for-image-classifier-architecture-search.md)
35. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
36. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
37. [cuDNN - Efficient Primitives for Deep Learning](2014-cudnn-efficient-primitives-for-deep-learning.md)
38. Interleaved Group Convolutions
39. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge.md)
40. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context.md)
41. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database.md)
42. [Inverted Residuals and Linear Bottlenecks - Mobile Networks for Classification, Detection and Segmentation](2018-inverted-residuals-and-linear-bottlenecks-mobile-networks-for-classification-detection-and-segmentation.md)
