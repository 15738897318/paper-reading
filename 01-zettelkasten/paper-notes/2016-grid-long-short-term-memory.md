---
title: Grid Long Short-Term Memory
authors:
- Nal Kalchbrenner
- Ivo Danihelka
- A. Graves
fieldsOfStudy:
- Computer Science
meta_key: 2016-grid-long-short-term-memory
numCitedBy: 315
reading_status: TBD
ref_count: 52
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Grid-Long-Short-Term-Memory-Kalchbrenner-Danihelka/5b791cd374c7109693aaddee2c12d659ae4e3ec0?sort=total-citations
venue: ICLR
year: 2016
---

# Grid Long Short-Term Memory

## Abstract

This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to significantly outperform the standard LSTM. We then give results for two empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task.

## Paper References

1. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks.md)
2. [Long Short-Term Memory](1997-long-short-term-memory.md)
3. [LSTM - A Search Space Odyssey](2017-lstm-a-search-space-odyssey.md)
4. Learning to Execute
5. [Generating Text with Recurrent Neural Networks](2011-generating-text-with-recurrent-neural-networks.md)
6. [Speech recognition with deep recurrent neural networks](2013-speech-recognition-with-deep-recurrent-neural-networks.md)
7. Gated Feedback Recurrent Neural Networks
8. [Network In Network](2014-network-in-network.md)
9. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator.md)
10. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks.md)
11. [Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](2014-unifying-visual-semantic-embeddings-with-multimodal-neural-language-models.md)
12. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
13. Multi-dimensional Recurrent Neural Networks
14. Supervised Sequence Labelling with Recurrent Neural Networks
15. Learning long-term dependencies with gradient descent is difficult
16. Convolutional Kernel Networks
17. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
18. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
19. [Multi-column deep neural networks for image classification](2012-multi-column-deep-neural-networks-for-image-classification.md)
20. Spatially-sparse convolutional neural networks
21. [Recurrent Continuous Translation Models](2013-recurrent-continuous-translation-models.md)
22. Fractional Max-Pooling
23. cdec - A Decoder, Alignment, and Learning Framework for Finite- State and Context-Free Translation Models
24. Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks
25. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition.md)
26. [Deeply-Supervised Nets](2015-deeply-supervised-nets.md)
27. ReNet - A Recurrent Neural Network Based Alternative to Convolutional Networks
28. Best practices for convolutional neural networks applied to visual document analysis
29. [Regularization of Neural Networks using DropConnect](2013-regularization-of-neural-networks-using-dropconnect.md)
30. [Maxout Networks](2013-maxout-networks.md)
31. Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
32. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization.md)
33. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines.md)
34. Artificial Neural Networks
35. [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](2016-under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks.md)
36. Perceptrons - an introduction to computational geometry
37. Untersuchungen zu dynamischen neuronalen Netzen
38. Solving the N-bit parity problem using neural networks
