---
title: Exploring the Limits of Language Modeling
authors:
- R. Józefowicz
- Oriol Vinyals
- M. Schuster
- Noam M. Shazeer
- Yonghui Wu
fieldsOfStudy:
- Computer Science
meta_key: 2016-exploring-the-limits-of-language-modeling
numCitedBy: 955
reading_status: TBD
ref_count: 59
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Language-Modeling-Józefowicz-Vinyals/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa?sort=total-citations
venue: ArXiv
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Language-Modeling-Józefowicz-Vinyals/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa?sort=total-citations)

# Exploring the Limits of Language Modeling

## Abstract

In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.

## Paper References

1. Context dependent recurrent neural network language model
2. One billion word benchmark for measuring progress in statistical language modeling
3. Larger-Context Language Modelling
4. Hierarchical Probabilistic Neural Network Language Model
5. Decoding with Large-Scale Neural Language Models Improves Translation
6. [A Convolutional Neural Network for Modelling Sentences](2014-a-convolutional-neural-network-for-modelling-sentences)
7. Deep Neural Network Language Models
8. A Scalable Hierarchical Distributed Language Model
9. [Character-Aware Neural Language Models](2016-character-aware-neural-language-models)
10. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
11. Sparse non-negative matrix language modeling for skip-grams
12. Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation
13. [Generating Text with Recurrent Neural Networks](2011-generating-text-with-recurrent-neural-networks)
14. Adaptive Importance Sampling to Accelerate Training of a Neural Probabilistic Language Model
15. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model)
16. An Empirical Exploration of Recurrent Network Architectures
17. Multilingual Language Processing From Bytes
18. Neural Probabilistic Language Models
19. Empirical Evaluation and Combination of Advanced Language Modeling Techniques
20. A Neural Conversational Model
21. Statistical Language Models Based on Neural Networks
22. Long short-term memory recurrent neural network architectures for large scale acoustic modeling
23. Finding Function in Form - Compositional Character Models for Open Vocabulary Word Representation
24. Learning word embeddings efficiently with noise-contrastive estimation
25. Addressing the Rare Word Problem in Neural Machine Translation
26. Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs
27. Sentence Compression by Deletion with LSTMs
28. Improved backing-off for M-gram language modeling
29. Hierarchical Neural Network Generative Models for Movie Dialogues
30. Document Context Language Models
31. Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets
32. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
33. [Long Short-Term Memory](1997-long-short-term-memory)
34. Learning to Forget - Continual Prediction with LSTM
35. [Framewise phoneme classification with bidirectional LSTM and other neural network architectures](2005-framewise-phoneme-classification-with-bidirectional-lstm-and-other-neural-network-architectures)
36. [A Neural Attention Model for Abstractive Sentence Summarization](2015-a-neural-attention-model-for-abstractive-sentence-summarization)
37. Bidirectional recurrent neural networks
38. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
39. [On the difficulty of training recurrent neural networks](2013-on-the-difficulty-of-training-recurrent-neural-networks)
40. Unsupervised Learning of Video Representations using LSTMs
41. [Training Very Deep Networks](2015-training-very-deep-networks)
42. Improving Neural Networks with Dropout
43. Building a Large Annotated Corpus of English - The Penn Treebank
44. [ImageNet classification with deep convolutional neural networks](2012-alexnet.md)
45. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database)
46. Noise-contrastive estimation - A new estimation principle for unnormalized statistical models
47. An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories
48. Handwritten Digit Recognition with a Back-Propagation Network
49. Scaling recurrent neural network language models
50. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization)
51. Quick Training of Probabilistic Neural Nets by Importance Sampling
