---
title: Stacked Denoising Autoencoders - Learning Useful Representations in a Deep Network with a Local Denoising Criterion
authors:
- Pascal Vincent
- H. Larochelle
- Isabelle Lajoie
- Yoshua Bengio
- Pierre-Antoine Manzagol
fieldsOfStudy:
- Computer Science
meta_key: 2010-stacked-denoising-autoencoders-learning-useful-representations-in-a-deep-network-with-a-local-denoising-criterion
numCitedBy: 5633
reading_status: TBD
ref_count: 62
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Stacked-Denoising-Autoencoders:-Learning-Useful-in-Vincent-Larochelle/e2b7f37cd97a7907b1b8a41138721ed06a0b76cd?sort=total-citations
venue: J. Mach. Learn. Res.
year: 2010
---

# Stacked Denoising Autoencoders - Learning Useful Representations in a Deep Network with a Local Denoising Criterion

## Abstract

We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.

## Paper References

1. [Extracting and composing robust features with denoising autoencoders](2008-extracting-and-composing-robust-features-with-denoising-autoencoders)
2. Why Does Unsupervised Pre-training Help Deep Learning?
3. Sparse Feature Learning for Deep Belief Networks
4. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
5. Deep Learning using Robust Interdependent Codes
6. Exploring Strategies for Training Deep Neural Networks
7. [Greedy Layer-Wise Training of Deep Networks](2006-greedy-layer-wise-training-of-deep-networks)
8. Sparse deep belief net model for visual area V2
9. Efficient Learning of Sparse Representations with an Energy-Based Model
10. Deep learning via semi-supervised embedding
11. Training with Noise is Equivalent to Tikhonov Regularization
12. An empirical evaluation of deep architectures on problems with many factors of variation
13. [Learning Deep Architectures for AI](2007-learning-deep-architectures-for-ai)
14. An Information-Maximization Approach to Blind Separation and Blind Deconvolution
15. [Reducing the Dimensionality of Data with Neural Networks](2006-reducing-the-dimensionality-of-data-with-neural-networks)
16. Neural networks and principal component analysis - Learning from examples without local minima
17. Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network
18. Emergence of simple-cell receptive field properties by learning a sparse code for natural images
19. Scaling learning algorithms towards AI
20. Sparse coding with an overcomplete basis set - A strategy employed by V1?
21. Creating artificial neural networks that generalize
22. Kernel Methods for Deep Learning
23. The Effects of Adding Noise During Backpropagation Training on a Generalization Performance
24. [Training Products of Experts by Minimizing Contrastive Divergence](2002-training-products-of-experts-by-minimizing-contrastive-divergence)
25. The “independent components” of natural scenes are edge filters
26. Connectionist Learning Procedures
27. Noise Injection - Theoretical Prospects
28. Nonlinear Autoassociation Is Not Equivalent to PCA
29. Justifying and Generalizing Contrastive Divergence
30. Semi-Supervised Learning
31. Learning Continuous Attractors in Recurrent Networks
32. Backpropagation Applied to Handwritten Zip Code Recognition
33. Large-scale kernel machines
34. Many-Layered Learning
35. Factors influencing learning by backpropagation
36. An Application of the Principle of Maximum Information Preservation to Linear Systems
37. Incorporating Invariances in Support Vector Learning Machines
38. Recognition and Structure from one 2D Model View - Observations on Prototypes, Object Classes and Symmetries
39. Receptive fields of single neurones in the cat's striate cortex
40. Document image defect models
41. Neural networks and physical systems with emergent collective computational abilities.
42. Information processing in dynamical systems - foundations of harmony theory
43. Parallel distributed processing - explorations in the microstructure of cognition, vol. 1 - foundations
44. Statistical Analysis of Non-Lattice Data
45. Parallel Distributed Processing - Explorations in the Microstructure of Cognition, vol 1 - Foundations, vol 2 - Psychological and Biological Models
46. Dependency Networks for Inference, Collaborative Filtering, and Data Visualization
47. Almost optimal lower bounds for small depth circuits
48. On the power of small-depth threshold circuits
49. PhD thesis - Modeles connexionnistes de l'apprentissage (connectionist learning models)
50. Memoires associatives distribuees - Une comparaison (Distributed associative memories - A comparison)
51. Emergence of grandmother memory in feed forward networks - learning with noise and forgetfulness
52. Using additive noise in back-propagation training
53. Auto-association by multilayer perceptrons and singular value decomposition
54. Algorithms for Classifying Recorded Music by Genre
55. Natural Image Denoising with Convolutional Networks
