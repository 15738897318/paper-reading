---
title: Image-to-Markup Generation with Coarse-to-Fine Attention
authors:
- Yuntian Deng
- Anssi Kanervisto
- Jeffrey Ling
- Alexander M. Rush
fieldsOfStudy:
- Computer Science
meta_key: 2017-image-to-markup-generation-with-coarse-to-fine-attention
numCitedBy: 133
reading_status: TBD
ref_count: 48
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Image-to-Markup-Generation-with-Coarse-to-Fine-Deng-Kanervisto/071c0b3ec700758dd9b4164ede08b714ea7e3c38?sort=total-citations
venue: ICML
year: 2017
---

[semanticscholar url](https://www.semanticscholar.org/paper/Image-to-Markup-Generation-with-Coarse-to-Fine-Deng-Kanervisto/071c0b3ec700758dd9b4164ede08b714ea7e3c38?sort=total-citations)

# Image-to-Markup Generation with Coarse-to-Fine Attention

## Abstract

We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention.

## Paper References

1. End-to-end text recognition with convolutional neural networks
2. [Recursive Recurrent Nets with Attention Modeling for OCR in the Wild](2016-recursive-recurrent-nets-with-attention-modeling-for-ocr-in-the-wild)
3. What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment
4. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
5. Reading Text in the Wild with Convolutional Neural Networks
6. Watch, attend and parse - An end-to-end neural network based approach to handwritten mathematical expression recognition
7. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator)
8. [An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition](2017-an-end-to-end-trainable-neural-network-for-image-based-sequence-recognition-and-its-application-to-scene-text-recognition)
9. Deep Structured Output Learning for Unconstrained Text Recognition
10. [Effective Approaches to Attention-based Neural Machine Translation](2015-effective-approaches-to-attention-based-neural-machine-translation)
11. Learning Efficient Algorithms with Hierarchical Attentive Memory
12. [Recurrent Models of Visual Attention](2014-recurrent-models-of-visual-attention)
13. Scene Text Recognition using Higher Order Language Priors
14. [Deep Visual-Semantic Alignments for Generating Image Descriptions](2017-deep-visual-semantic-alignments-for-generating-image-descriptions)
15. [Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](2017-outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer)
16. Multiple Object Recognition with Visual Attention
17. Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes
18. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
19. Tracing and straightening the baseline in handwritten persian/arabic text-line - A new approach based on painting-technique
20. From Softmax to Sparsemax - A Sparse Model of Attention and Multi-Label Classification
21. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift)
22. ICFHR 2014 Competition on Recognition of On-Line Handwritten Mathematical Expressions (CROHME 2014)
23. [Connectionist temporal classification - labelling unsegmented sequence data with recurrent neural networks](2006-connectionist-temporal-classification-labelling-unsegmented-sequence-data-with-recurrent-neural-networks)
24. Conditional Computation in Neural Networks for faster models
25. Visualizing and Understanding Recurrent Networks
26. [Long Short-Term Memory](1997-long-short-term-memory)
27. Grammar as a Foreign Language
28. Ambiguity and Constraint in Mathematical Expression Recognition
29. Introduction to the Special Issue on Natural Language Generation
30. ICDAR 2013 CROHME - Third International Competition on Recognition of Online Handwritten Mathematical Expressions
31. ICFHR 2012 Competition on Recognition of On-Line Mathematical Expressions (CROHME 2012)
32. Coarse-to-Fine Dynamic Programming
33. A Syntactic Approach for Handwritten Mathematical Formula Recognition
34. Syntax-directed recognition of hand-printed two-dimensional mathematics
35. Gradient Estimation Using Stochastic Computation Graphs
36. OpenNMT - Open-Source Toolkit for Neural Machine Translation
37. Mathematical expression recognition - a survey
38. [Torch7 - A Matlab-like Environment for Machine Learning](2011-torch7-a-matlab-like-environment-for-machine-learning)
39. [Bleu - a Method for Automatic Evaluation of Machine Translation](2002-bleu-a-method-for-automatic-evaluation-of-machine-translation)
40. INFTY - an integrated OCR system for mathematical documents
41. Deep, Big, Simple Neural Nets for Handwritten Digit Recognition
42. [Simple statistical gradient-following algorithms for connectionist reinforcement learning](2004-simple-statistical-gradient-following-algorithms-for-connectionist-reinforcement-learning)
43. Overview of the 2003 KDD Cup
