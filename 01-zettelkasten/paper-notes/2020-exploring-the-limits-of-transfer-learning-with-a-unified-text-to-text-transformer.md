---
title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
authors:
- Colin Raffel
- Noam M. Shazeer
- Adam Roberts
- Katherine Lee
- Sharan Narang
- Michael Matena
- Yanqi Zhou
- Wei Li
- Peter J. Liu
fieldsOfStudy:
- Computer Science
meta_key: 2020-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer
numCitedBy: 3907
reading_status: TBD
ref_count: 139
tags:
- gen-from-ref
- other-default
- paper
venue: J. Mach. Learn. Res.
year: 2020
---

# Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer

## Abstract

Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code.

## Paper References

1. [Universal Language Model Fine-tuning for Text Classification](2018-universal-language-model-fine-tuning-for-text-classification)
2. StructBERT - Incorporating Language Structures into Pre-training for Deep Language Understanding
3. Transfer Learning in Natural Language Processing
4. [Language Models are Unsupervised Multitask Learners](2019-language-models-are-unsupervised-multitask-learners)
5. [Unified Language Model Pre-training for Natural Language Understanding and Generation](2019-unified-language-model-pre-training-for-natural-language-understanding-and-generation)
6. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
7. [Multi-Task Deep Neural Networks for Natural Language Understanding](2019-multi-task-deep-neural-networks-for-natural-language-understanding)
8. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
9. Unsupervised Pretraining for Sequence to Sequence Learning
10. Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning
11. SciBERT - A Pretrained Language Model for Scientific Text
12. MASS - Masked Sequence to Sequence Pre-training for Language Generation
13. A Simple Method for Commonsense Reasoning
14. DistilBERT, a distilled version of BERT - smaller, faster, cheaper and lighter
15. Parameter-Efficient Transfer Learning for NLP
16. Sentence Encoders on STILTs - Supplementary Training on Intermediate Labeled-data Tasks
17. [Cross-lingual Language Model Pretraining](2019-cross-lingual-language-model-pretraining)
18. TinyBERT - Distilling BERT for Natural Language Understanding
19. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
20. [Supervised Learning of Universal Sentence Representations from Natural Language Inference Data](2017-supervised-learning-of-universal-sentence-representations-from-natural-language-inference-data)
21. Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling
22. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
23. The Bottom-up Evolution of Representations in the Transformer - A Study with Machine Translation and Language Modeling Objectives
24. Simple, Scalable Adaptation for Neural Machine Translation
25. Get To The Point - Summarization with Pointer-Generator Networks
26. BoolQ - Exploring the Surprising Difficulty of Natural Yes/No Questions
27. [Exploring the Limits of Language Modeling](2016-exploring-the-limits-of-language-modeling)
28. SummAE - Zero-Shot Abstractive Text Summarization using Length-Agnostic Auto-Encoders
29. A Deep Reinforced Model for Abstractive Summarization
30. [Skip-Thought Vectors](2015-skip-thought-vectors)
31. Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond
32. Massively Multilingual Neural Machine Translation in the Wild - Findings and Challenges
33. Unifying Question Answering and Text Classification via Span Extraction
34. [ALBERT - A Lite BERT for Self-supervised Learning of Language Representations](2020-albert-a-lite-bert-for-self-supervised-learning-of-language-representations)
35. [SemEval-2017 Task 1 - Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation](2017-semeval-2017-task-1-semantic-textual-similarity-multilingual-and-crosslingual-focused-evaluation)
36. Understanding Back-Translation at Scale
37. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
38. Semi-supervised Sequence Learning
39. QANet - Combining Local Convolution with Global Self-Attention for Reading Comprehension
40. Representation Learning Using Multi-Task Deep Neural Networks for Semantic Classification and Information Retrieval
41. To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks
42. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
43. CTRL - A Conditional Transformer Language Model for Controllable Generation
44. [Attention is All you Need](2017-attention-is-all-you-need)
45. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation)
46. Snorkel MeTaL - Weak Supervision for Multi-Task Learning
47. WiC - 10, 000 Example Pairs for Evaluating Context-Sensitive Representations
48. [XLNet - Generalized Autoregressive Pretraining for Language Understanding](2019-xlnet-generalized-autoregressive-pretraining-for-language-understanding)
49. [A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference](2018-a-broad-coverage-challenge-corpus-for-sentence-understanding-through-inference)
50. [A Convolutional Neural Network for Modelling Sentences](2014-a-convolutional-neural-network-for-modelling-sentences)
51. Exploring the Limits of Weakly Supervised Pretraining
52. An efficient framework for learning sentence representations
53. [Learning Distributed Representations of Sentences from Unlabelled Data](2016-learning-distributed-representations-of-sentences-from-unlabelled-data)
54. SentencePiece - A simple and language independent subword tokenizer and detokenizer for Neural Text Processing
55. Looking Beyond the Surface - A Challenge Set for Reading Comprehension over Multiple Sentences
56. Learning Word Vectors for 157 Languages
57. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend)
58. Cloze-driven Pretraining of Self-attention Networks
59. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
60. [TriviaQA - A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension](2017-triviaqa-a-large-scale-distantly-supervised-challenge-dataset-for-reading-comprehension)
61. [Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](2017-outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer)
62. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
63. [Neural Machine Translation of Rare Words with Subword Units](2016-neural-machine-translation-of-rare-words-with-subword-units)
64. SuperGLUE - A Stickier Benchmark for General-Purpose Language Understanding Systems
65. [Character-Level Language Modeling with Deeper Self-Attention](2019-character-level-language-modeling-with-deeper-self-attention)
66. Fine-tune BERT for Extractive Summarization
67. A Call for Clarity in Reporting BLEU Scores
68. N-gram Counts and Language Models from the Common Crawl
69. FreeLB - Enhanced Adversarial Training for Language Understanding
70. [NewsQA - A Machine Comprehension Dataset](2017-newsqa-a-machine-comprehension-dataset)
71. Subword Regularization - Improving Neural Network Translation Models with Multiple Subword Candidates
72. A Surprisingly Robust Trick for the Winograd Schema Challenge
73. [SpanBERT - Improving Pre-training by Representing and Predicting Spans](2020-spanbert-improving-pre-training-by-representing-and-predicting-spans)
74. [Self-Attention with Relative Position Representations](2018-self-attention-with-relative-position-representations)
75. Generating Sentences from a Continuous Space
76. C4Corpus - Multilingual Web-size Corpus with Free License
77. SentEval - An Evaluation Toolkit for Universal Sentence Representations
78. Deep Learning Scaling is Predictable, Empirically
79. Long Short-Term Memory-Networks for Machine Reading
80. Distilling the Knowledge in a Neural Network
81. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
82. Findings of the 2015 Workshop on Statistical Machine Translation
83. A Hybrid Neural Network Model for Commonsense Reasoning
84. An Overview of Multi-Task Learning in Deep Neural Networks
85. What makes ImageNet good for transfer learning?
86. Findings of the 2016 Conference on Machine Translation
87. Generating Wikipedia by Summarizing Long Sequences
88. Dirt Cheap Web-Scale Parallel Text from the Common Crawl
89. Rethinking ImageNet Pre-Training
90. [Deep Residual Learning for Image Recognition](2015-resnet.md)
91. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
92. Do Better ImageNet Models Transfer Better?
93. Findings of the 2014 Workshop on Statistical Machine Translation
94. An Overview of Multi-task Learning
95. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting)
96. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
97. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
98. Automatically Constructing a Corpus of Sentential Paraphrases
99. Defending Against Neural Fake News
100. Multitask Learning
101. Resolving Complex Cases of Definite Pronouns - The Winograd Schema Challenge
102. [Neural Network Acceptability Judgments](2019-neural-network-acceptability-judgments)
103. How transferable are features in deep neural networks?
104. Music Transformer - Generating Music with Long-Term Structure
105. Adafactor - Adaptive Learning Rates with Sublinear Memory Cost
106. The PASCAL Recognising Textual Entailment Challenge
107. [ROUGE - A Package for Automatic Evaluation of Summaries](2004-rouge-a-package-for-automatic-evaluation-of-summaries)
108. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database)
109. [Learning and Transferring Mid-level Image Representations Using Convolutional Neural Networks](2014-learning-and-transferring-mid-level-image-representations-using-convolutional-neural-networks)
110. [Bleu - a Method for Automatic Evaluation of Machine Translation](2002-bleu-a-method-for-automatic-evaluation-of-machine-translation)
111. Measuring the Effects of Data Parallelism on Neural Network Training
112. Mesh-TensorFlow - Deep Learning for Supercomputers
113. [Caffe - Convolutional Architecture for Fast Feature Embedding](2014-caffe-convolutional-architecture-for-fast-feature-embedding)
114. Memory-Efficient Adaptive Optimization for Large-Scale Learning
115. ReCoRD - Bridging the Gap between Human and Machine Commonsense Reading Comprehension
116. GPipe - Efficient Training of Giant Neural Networks using Pipeline Parallelism
117. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
118. Federated Learning - Strategies for Improving Communication Efficiency
119. The CommitmentBank - Investigating projection in naturally occurring discourse
120. The Winograd Schema Challenge
121. The Natural Language Decathlon - Multitask Learning as Question Answering
122. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge)
123. Federated Optimization - Distributed Optimization Beyond the Datacenter
124. SemEval-2012 Task 7 - Choice of Plausible Alternatives - An Evaluation of Commonsense Causal Reasoning
125. A Learning Algorithm for Continually Running Fully Recurrent Neural Networks
126. One weird trick for parallelizing convolutional neural networks
127. “Cloze Procedure” - A New Tool for Measuring Readability
128. [RoBERTa - A Robustly Optimized BERT Pretraining Approach](2019-roberta-a-robustly-optimized-bert-pretraining-approach)
129. ELECTRA - Pre-training Text Encoders as Discriminators Rather Than Generators
130. Neural transfer learning for natural language processing
131. Low-field SQUID MRI - To tune or not to tune?
132. A bitter lesson.
