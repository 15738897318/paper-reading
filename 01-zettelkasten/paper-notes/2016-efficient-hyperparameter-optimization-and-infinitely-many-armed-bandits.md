---
title: Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits
authors:
- Lisha Li
- Kevin G. Jamieson
- Giulia DeSalvo
- Afshin Rostamizadeh
- Ameet S. Talwalkar
fieldsOfStudy:
- Computer Science
meta_key: 2016-efficient-hyperparameter-optimization-and-infinitely-many-armed-bandits
numCitedBy: 122
reading_status: TBD
ref_count: 22
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Efficient-Hyperparameter-Optimization-and-Many-Li-Jamieson/b6670222f0cae0ce4a886405e8c8f5d273feeded?sort=total-citations
venue: ArXiv
year: 2016
---

# Efficient Hyperparameter Optimization and Infinitely Many Armed Bandits

## Abstract

Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While current methods offer efficiencies by adaptively choosing new configurations to train, an alternative strategy is to adaptively allocate resources across the selected configurations. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinitely many armed bandit problem where allocation of additional resources to an arm corresponds to training a configuration on larger subsets of the data. We introduce Hyperband for this framework and analyze its theoretical properties, providing several desirable guarantees. We compare Hyperband with state-of-the-art Bayesian optimization methods and a random search baseline on a comprehensive benchmark including 117 datasets. Our results on this benchmark demonstrate that while Bayesian optimization methods do not outperform random search trained for twice as long, Hyperband in favorable settings offers valuable speedups.

## Paper References

1. Non-stochastic Best Arm Identification and Hyperparameter Optimization
2. Multi-Task Bayesian Optimization
3. [Practical Bayesian Optimization of Machine Learning Algorithms](2012-practical-bayesian-optimization-of-machine-learning-algorithms.md)
4. Almost Optimal Exploration in Multi-Armed Bandits
5. [Algorithms for Hyper-Parameter Optimization](2011-algorithms-for-hyper-parameter-optimization.md)
6. Scalable Bayesian Optimization Using Deep Neural Networks
7. Simple regret for infinitely many armed bandits
8. Oracle inequalities for computationally budgeted model selection
9. Freeze-Thaw Bayesian Optimization
10. Efficient Multi-Start Strategies for Local Search Algorithms
11. Towards an Empirical Foundation for Assessing Bayesian Optimization of Hyperparameters
12. Automating model search for large scale machine learning
13. [Sequential Model-Based Optimization for General Algorithm Configuration](2011-sequential-model-based-optimization-for-general-algorithm-configuration.md)
14. Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems
15. Efficient and Robust Automated Machine Learning
16. Fast cross-validation via sequential testing
17. Sharp analysis of low-rank kernel matrix approximations
18. On the Impact of Kernel Approximation on Learning Accuracy
19. Action Elimination and Stopping Conditions for Reinforcement Learning
20. Fast rates for support vector machines using Gaussian kernels
21. ESTIMATING THE APPROXIMATION ERROR IN LEARNING THEORY
