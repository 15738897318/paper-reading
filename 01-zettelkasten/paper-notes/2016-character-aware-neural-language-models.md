---
title: Character-Aware Neural Language Models
authors:
- Yoon Kim
- Yacine Jernite
- D. Sontag
- Alexander M. Rush
fieldsOfStudy:
- Computer Science
meta_key: 2016-character-aware-neural-language-models
numCitedBy: 1429
reading_status: TBD
ref_count: 83
tags:
- gen-from-ref
- other-default
- paper
venue: AAAI
year: 2016
---

# Character-Aware Neural Language Models

## Abstract

We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway net work over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60% fewer parameters. On languages with rich morphology (Arabic, Czech, French, German, Spanish, Russian), the model outperforms word-level/morpheme-level LSTM baselines, again with fewer parameters. The results suggest that on many languages, character inputs are sufficient for language modeling. Analysis of word representations obtained from the character composition part of the model reveals that the model is able to encode, from characters only, both semantic and orthographic information.

## Paper References

1. SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
2. Learning Character-level Representations for Part-of-Speech Tagging
3. Boosting Named Entity Recognition with Neural Character Embeddings
4. [LSTM Neural Networks for Language Modeling](2012-lstm-neural-networks-for-language-modeling)
5. Context dependent recurrent neural network language model
6. Co-learning of Word Representations and Morpheme Representations
7. Better Word Representations with Recursive Neural Networks for Morphology
8. [A Convolutional Neural Network for Modelling Sentences](2014-a-convolutional-neural-network-for-modelling-sentences)
9. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
10. Improved Transition-based Parsing by Modeling Characters instead of Words with LSTMs
11. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model)
12. Hierarchical Probabilistic Neural Network Language Model
13. Finding Function in Form - Compositional Character Models for Open Vocabulary Word Representation
14. Probabilistic modelling of morphologically rich languages
15. [Generating Text with Recurrent Neural Networks](2011-generating-text-with-recurrent-neural-networks)
16. [Linguistic Regularities in Continuous Space Word Representations](2013-linguistic-regularities-in-continuous-space-word-representations)
17. Compositional Morphology for Word Representations and Language Modelling
18. A Neural Probabilistic Language Model
19. Factored Language Models and Generalized Parallel Backoff
20. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
21. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
22. [Convolutional Neural Networks for Sentence Classification](2014-convolutional-neural-networks-for-sentence-classification)
23. A Latent Semantic Model with Convolutional-Pooling Structure for Information Retrieval
24. Factored Neural Language Models
25. [Character-level Convolutional Networks for Text Classification](2015-character-level-convolutional-networks-for-text-classification)
26. Language modeling with sum-product networks
27. How to Construct Deep Recurrent Neural Networks
28. The Fixed-Size Ordinally-Forgetting Encoding Method for Neural Network Language Models
29. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
30. Text Understanding from Scratch
31. Empirical Evaluation and Combination of Advanced Language Modeling Techniques
32. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks)
33. Unsupervised models for morpheme segmentation and morphology learning
34. Three new graphical models for statistical language modelling
35. Molding CNNs for text - non-linear, non-consecutive convolutions
36. N-gram Counts and Language Models from the Common Crawl
37. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
38. [Long Short-Term Memory](1997-long-short-term-memory)
39. Building a Large Annotated Corpus of English - The Penn Treebank
40. Learning long-term dependencies with gradient descent is difficult
41. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors)
42. [Training Very Deep Networks](2015-training-very-deep-networks)
43. Backpropagation Through Time - What It Does and How to Do It
44. Handwritten Digit Recognition with a Back-Propagation Network
45. Indexing by Latent Semantic Analysis
46. An empirical study of smoothing techniques for language modeling
47. genCNN - A Convolutional Architecture for Word Sequence Prediction
48. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization)
