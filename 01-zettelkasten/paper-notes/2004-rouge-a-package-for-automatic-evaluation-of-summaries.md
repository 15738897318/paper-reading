---
title: ROUGE - A Package for Automatic Evaluation of Summaries
authors:
- Chin-Yew Lin
fieldsOfStudy:
- Computer Science
meta_key: 2004-rouge-a-package-for-automatic-evaluation-of-summaries
numCitedBy: 7019
reading_status: TBD
ref_count: 16
tags:
- gen-from-ref
- paper
venue: ACL 2004
year: 2004
---

# ROUGE - A Package for Automatic Evaluation of Summaries

## Abstract

ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation. It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans. This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluations. Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.

## Paper References

1. Looking for a Few Good Metrics - ROUGE and its Evaluation
2. Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics
3. Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence and Skip-Bigram Statistics
4. Automatic Evaluation and Uniform Filter Cascades for Inducing N-Best Translation Lexicons
5. Automatic Summarization
6. [Bleu - a Method for Automatic Evaluation of Machine Translation](2002-bleu-a-method-for-automatic-evaluation-of-machine-translation)
7. Meta-evaluation of Summaries in a Cross-lingual Environment using Content-based Metrics
8. Precision and Recall of Machine Translation
9. Bootstrap Methods and Their Application
10. Intrinsic Evaluation of Generic News Text Summarization Systems
