---
title: Squeeze-and-Excitation Networks
authors:
- Jie Hu
- Li Shen
- Samuel Albanie
- Gang Sun
- E. Wu
fieldsOfStudy:
- Computer Science
meta_key: 2020-squeeze-and-excitation-networks
numCitedBy: 7692
reading_status: TBD
ref_count: 90
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Squeeze-and-Excitation-Networks-Hu-Shen/df67d46e78aae0d2fccfb6212d101a342259c01b?sort=total-citations
venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
year: 2020
---

[semanticscholar url](https://www.semanticscholar.org/paper/Squeeze-and-Excitation-Networks-Hu-Shen/df67d46e78aae0d2fccfb6212d101a342259c01b?sort=total-citations)

# Squeeze-and-Excitation Networks

## Abstract

The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the “Squeeze-and-Excitation” (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of <inline-formula><tex-math notation="LaTeX">${\sim }$</tex-math><alternatives><mml:math><mml:mo>∼</mml:mo></mml:math><inline-graphic xlink:href="shen-ieq1-2913372.gif"/></alternatives></inline-formula>25 percent. Models and code are available at <uri>https://github.com/hujie-frank/SENet</uri>.

## Paper References

1. Gather-Excite - Exploiting Feature Context in Convolutional Neural Networks
2. [Deep Pyramidal Residual Networks](2017-deep-pyramidal-residual-networks.md)
3. [Improved Regularization of Convolutional Neural Networks with Cutout](2017-improved-regularization-of-convolutional-neural-networks-with-cutout.md)
4. [Aggregated Residual Transformations for Deep Neural Networks](2017-aggregated-residual-transformations-for-deep-neural-networks.md)
5. [Network In Network](2014-network-in-network.md)
6. CBAM - Convolutional Block Attention Module
7. Dual Path Networks
8. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks.md)
9. SCA-CNN - Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning
10. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
11. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
12. [Fully Convolutional Networks for Semantic Segmentation](2017-fully-convolutional-networks-for-semantic-segmentation.md)
13. [Convolutional Neural Fabrics](2016-convolutional-neural-fabrics.md)
14. Deep Roots - Improving CNN Efficiency with Hierarchical Filter Groups
15. [How transferable are features in deep neural networks?](2014-how-transferable-are-features-in-deep-neural-networks.md)
16. Residual Attention Network for Image Classification
17. [Xception - Deep Learning with Depthwise Separable Convolutions](2017-xception-deep-learning-with-depthwise-separable-convolutions.md)
18. [Spatial Transformer Networks](2015-spatial-transformer-networks.md)
19. PolyNet - A Pursuit of Structural Diversity in Very Deep Networks
20. Deep Networks with Internal Selective Attention through Feedback Connections
21. [Rethinking the Inception Architecture for Computer Vision](2016-rethinking-the-inception-architecture-for-computer-vision.md)
22. [Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning](2017-inception-v4-inception-resnet-and-the-impact-of-residual-connections-on-learning.md)
23. [Learning Transferable Architectures for Scalable Image Recognition](2018-learning-transferable-architectures-for-scalable-image-recognition.md)
24. [Genetic CNN](2017-genetic-cnn.md)
25. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks.md)
26. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.md)
27. [Deep Networks with Stochastic Depth](2016-deep-networks-with-stochastic-depth.md)
28. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
29. [Inside-Outside Net - Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks](2016-inside-outside-net-detecting-objects-in-context-with-skip-pooling-and-recurrent-neural-networks.md)
30. [Wide Residual Networks](2016-wide-residual-networks.md)
31. On the importance of single directions for generalization
32. [Attention is All you Need](2017-attention-is-all-you-need.md)
33. Look and Think Twice - Capturing Top-Down Visual Attention with Feedback Convolutional Neural Networks
34. [Speeding up Convolutional Neural Networks with Low Rank Expansions](2014-speeding-up-convolutional-neural-networks-with-low-rank-expansions.md)
35. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
36. [Stacked Hourglass Networks for Human Pose Estimation](2016-stacked-hourglass-networks-for-human-pose-estimation.md)
37. Learnable pooling with Context Gating for video classification
38. Exploring the Limits of Weakly Supervised Pretraining
39. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
40. Relay Backpropagation for Effective Learning of Deep Convolutional Neural Networks
41. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
42. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
43. [Progressive Neural Architecture Search](2018-progressive-neural-architecture-search.md)
44. [Recurrent Models of Visual Attention](2014-recurrent-models-of-visual-attention.md)
45. [MnasNet - Platform-Aware Neural Architecture Search for Mobile](2019-mnasnet-platform-aware-neural-architecture-search-for-mobile.md)
46. [SMASH - One-Shot Model Architecture Search through HyperNetworks](2018-smash-one-shot-model-architecture-search-through-hypernetworks.md)
47. Shake-Shake regularization
48. [Large-Scale Evolution of Image Classifiers](2017-large-scale-evolution-of-image-classifiers.md)
49. AutoAugment - Learning Augmentation Policies from Data
50. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
51. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations
52. [Designing Neural Network Architectures using Reinforcement Learning](2017-designing-neural-network-architectures-using-reinforcement-learning.md)
53. [Training Very Deep Networks](2015-training-very-deep-networks.md)
54. [DARTS - Differentiable Architecture Search](2019-darts-differentiable-architecture-search.md)
55. How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)
56. Multi-objective Architecture Search for CNNs
57. [DeepArchitect - Automatically Designing and Training Deep Architectures](2017-deeparchitect-automatically-designing-and-training-deep-architectures.md)
58. [Image Classification with the Fisher Vector - Theory and Practice](2013-image-classification-with-the-fisher-vector-theory-and-practice.md)
59. [An Empirical Exploration of Recurrent Network Architectures](2015-an-empirical-exploration-of-recurrent-network-architectures.md)
60. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention.md)
61. Places - A 10 Million Image Database for Scene Recognition
62. [Regularized Evolution for Image Classifier Architecture Search](2019-regularized-evolution-for-image-classifier-architecture-search.md)
63. [ShuffleNet - An Extremely Efficient Convolutional Neural Network for Mobile Devices](2018-shufflenet-an-extremely-efficient-convolutional-neural-network-for-mobile-devices.md)
64. [MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications](2017-mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications.md)
65. Multi-Level Discriminative Dictionary Learning With Application to Large Scale Image Classification
66. Learning to combine foveal glimpses with a third-order Boltzmann machine
67. Linear spatial pyramid matching using sparse coding for image classification
68. Efficient Multi-Objective Neural Architecture Search via Lamarckian Evolution
69. [Efficient Neural Architecture Search via Parameter Sharing](2018-efficient-neural-architecture-search-via-parameter-sharing.md)
70. Joint Line Segmentation and Transcription for End-to-End Handwritten Paragraph Recognition
71. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context.md)
72. [DeepPose - Human Pose Estimation via Deep Neural Networks](2014-deeppose-human-pose-estimation-via-deep-neural-networks.md)
73. A neurobiological model of visual attention and invariant pattern recognition based on dynamic routing of information
74. Lip Reading Sentences in the Wild
75. Computational modelling of visual attention
76. Accelerating Neural Architecture Search using Performance Prediction
77. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge.md)
78. [Long Short-Term Memory](1997-long-short-term-memory.md)
79. [Hierarchical Representations for Efficient Architecture Search](2018-hierarchical-representations-for-efficient-architecture-search.md)
80. Evolving Neural Networks through Augmenting Topologies
81. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines.md)
82. [A Model of Saliency-Based Visual Attention for Rapid Scene Analysis](2009-a-model-of-saliency-based-visual-attention-for-rapid-scene-analysis.md)
83. Evolving Memory Cell Structures for Sequence Learning
84. Designing Neural Networks using Genetic Algorithms
85. [Random Search for Hyper-Parameter Optimization](2012-random-search-for-hyper-parameter-optimization.md)
