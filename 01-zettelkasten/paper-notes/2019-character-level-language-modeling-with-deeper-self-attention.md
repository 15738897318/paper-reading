---
title: Character-Level Language Modeling with Deeper Self-Attention
authors:
- Rami Al-Rfou
- Dokook Choe
- Noah Constant
- Mandy Guo
- Llion Jones
fieldsOfStudy:
- Computer Science
meta_key: 2019-character-level-language-modeling-with-deeper-self-attention
numCitedBy: 219
reading_status: TBD
ref_count: 61
tags:
- gen-from-ref
- other-default
- paper
venue: AAAI
year: 2019
---

# Character-Level Language Modeling with Deeper Self-Attention

## Abstract

LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model (Vaswani et al. 2017) with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.

## Paper References

1. [Language Modeling with Gated Convolutional Networks](2017-language-modeling-with-gated-convolutional-networks)
2. Regularizing and Optimizing LSTM Language Models
3. Multiplicative LSTM for sequence modelling
4. SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
5. [End-To-End Memory Networks](2015-end-to-end-memory-networks)
6. [Exploring the Limits of Language Modeling](2016-exploring-the-limits-of-language-modeling)
7. [LSTM Neural Networks for Language Modeling](2012-lstm-neural-networks-for-language-modeling)
8. MuFuRU - The Multi-Function Recurrent Unit
9. Memory Architectures in Recurrent Neural Network Language Models
10. Zoneout - Regularizing RNNs by Randomly Preserving Hidden Activations
11. [Attention is All you Need](2017-transformer.md)
12. Very Deep Convolutional Networks for Natural Language Processing
13. Recurrent Highway Networks
14. Fast-Slow Recurrent Neural Networks
15. Dynamic Evaluation of Neural Sequence Models
16. Sharp Nearby, Fuzzy Far Away - How Neural Language Models Use Context
17. Recurrent Batch Normalization
18. One billion word benchmark for measuring progress in statistical language modeling
19. Independently Recurrent Neural Network (IndRNN) - Building A Longer and Deeper RNN
20. Frustratingly Short Attention Spans in Neural Language Modeling
21. Learning to Generate Reviews and Discovering Sentiment
22. [Character-level Convolutional Networks for Text Classification](2015-character-level-convolutional-networks-for-text-classification)
23. [Outrageously Large Neural Networks - The Sparsely-Gated Mixture-of-Experts Layer](2017-outrageously-large-neural-networks-the-sparsely-gated-mixture-of-experts-layer)
24. Neural Architecture Search with Reinforcement Learning
25. Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
26. Surprisal-Driven Feedback in Recurrent Networks
27. [Recurrent neural network based language model](2010-recurrent-neural-network-based-language-model)
28. Gated Feedback Recurrent Neural Networks
29. A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
30. [Extensions of recurrent neural network language model](2011-extensions-of-recurrent-neural-network-language-model)
31. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
32. Improving Neural Language Models with a Continuous Cache
33. On Multiplicative Integration with Recurrent Neural Networks
34. [Long Short-Term Memory](1997-long-short-term-memory)
35. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift)
36. Sparse Attentive Backtracking - Long-Range Credit Assignment in Recurrent Networks
37. Unitary Evolution Recurrent Neural Networks
38. Byte-Level Machine Reading Across Morphologically Varied Languages
39. Hierarchical Multiscale Recurrent Neural Networks
40. [Layer Normalization](2016-layer-normalization)
41. A Neural Probabilistic Language Model
42. Architectural Complexity Measures of Recurrent Neural Networks
43. Unbiasing Truncated Backpropagation Through Time
44. Improving GANs Using Optimal Transport
45. Gradient Flow in Recurrent Nets - the Difficulty of Learning Long-Term Dependencies
46. Backpropagation Through Time - What It Does and How to Do It
47. Neural Machine Translation in Linear Time
48. [Memory Networks](2015-memory-networks)
49. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization)
