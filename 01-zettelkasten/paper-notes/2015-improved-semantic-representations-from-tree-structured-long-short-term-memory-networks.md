---
title: Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks
authors:
- Kai Sheng Tai
- R. Socher
- Christopher D. Manning
fieldsOfStudy:
- Computer Science
meta_key: 2015-improved-semantic-representations-from-tree-structured-long-short-term-memory-networks
numCitedBy: 2517
reading_status: TBD
ref_count: 43
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Improved-Semantic-Representations-From-Long-Memory-Tai-Socher/32de44f01a96d4473d21099d15e25bc2b9f08e2f?sort=total-citations
venue: ACL
year: 2015
---

# Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks

## Abstract

Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).

## Paper References

1. [Semantic Compositionality through Recursive Matrix-Vector Spaces](2012-semantic-compositionality-through-recursive-matrix-vector-spaces)
2. [A Convolutional Neural Network for Modelling Sentences](2014-a-convolutional-neural-network-for-modelling-sentences)
3. [Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank](2013-recursive-deep-models-for-semantic-compositionality-over-a-sentiment-treebank)
4. Deep Recursive Neural Networks for Compositionality in Language
5. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
6. Grounded Compositional Semantics for Finding and Describing Images with Sentences
7. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
8. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
9. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
10. Learning task-dependent distributed representations by backpropagation through structure
11. Word Representations - A Simple and General Method for Semi-Supervised Learning
12. SemEval-2014 Task 1 - Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment
13. [Parsing Natural Scenes and Natural Language with Recursive Neural Networks](2011-parsing-natural-scenes-and-natural-language-with-recursive-neural-networks)
14. [Long Short-Term Memory](1997-long-short-term-memory)
15. [A Fast and Accurate Dependency Parser using Neural Networks](2014-a-fast-and-accurate-dependency-parser-using-neural-networks)
16. [Distributed Representations of Sentences and Documents](2014-distributed-representations-of-sentences-and-documents)
17. Learning to Execute
18. Learning long-term dependencies with gradient descent is difficult
19. Statistical Language Models Based on Neural Networks
20. [Convolutional Neural Networks for Sentence Classification](2014-convolutional-neural-networks-for-sentence-classification)
21. Composition in Distributional Models of Semantics
22. Compositional Matrix-Space Models for Sentiment Analysis
23. UNAL-NLP - Combining Soft Cardinality Features for Semantic Textual Similarity, Relatedness and Entailment
24. Finding Structure in Time
25. The Meaning Factory - Formal Semantics for Recognizing Textual Entailment and Determining Semantic Similarity
26. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
27. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator)
28. A Solution to Plato's Problem - The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.
29. Modeling Documents with Deep Boltzmann Machines
30. Modeling documents with a Deep Boltzmann Machine
31. Hybrid speech recognition with Deep Bidirectional LSTM
32. PPDB - The Paraphrase Database
33. ECNU - One Stone Two Birds - Ensemble of Heterogenous Measures for Semantic Relatedness and Textual Entailment
34. Learning representations by back-propagating errors
35. The Measurement of Textual Coherence with Latent Semantic Analysis.
36. [Accurate Unlexicalized Parsing](2003-accurate-unlexicalized-parsing)
37. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting)
38. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors)
39. The Vanishing Gradient Problem During Learning Recurrent Neural Nets and Problem Solutions
40. Illinois-LH - A Denotational and Distributional Approach to Semantics
41. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization)
42. [Improving Word Representations via Global Context and Multiple Word Prototypes](2012-improving-word-representations-via-global-context-and-multiple-word-prototypes)
43. Multi-Step Regression Learning for Compositional Distributional Semantics
