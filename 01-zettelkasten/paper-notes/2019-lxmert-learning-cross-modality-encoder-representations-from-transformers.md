---
title: LXMERT - Learning Cross-Modality Encoder Representations from Transformers
authors:
- Hao Hao Tan
- Mohit Bansal
fieldsOfStudy:
- Computer Science
meta_key: 2019-lxmert-learning-cross-modality-encoder-representations-from-transformers
numCitedBy: 952
reading_status: TBD
ref_count: 44
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/LXMERT:-Learning-Cross-Modality-Encoder-from-Tan-Bansal/79c93274429d6355959f1e4374c2147bb81ea649?sort=total-citations
venue: EMNLP
year: 2019
---

# LXMERT - Learning Cross-Modality Encoder Representations from Transformers

## Abstract

Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert

## Paper References

1. [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks)
2. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
3. Multimodal Unified Attention Networks for Vision-and-Language Interactions
4. [VideoBERT - A Joint Model for Video and Language Representation Learning](2019-videobert-a-joint-model-for-video-and-language-representation-learning)
5. [Attention is All you Need](2017-transformer.md)
6. [Visual7W - Grounded Question Answering in Images](2016-visual7w-grounded-question-answering-in-images)
7. Multi-Modality Latent Interaction Network for Visual Question Answering
8. [VisualBERT - A Simple and Performant Baseline for Vision and Language](2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language)
9. [Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering](2019-dynamic-fusion-with-intra-and-inter-modality-attention-flow-for-visual-question-answering)
10. [Deep Modular Co-Attention Networks for Visual Question Answering](2019-deep-modular-co-attention-networks-for-visual-question-answering)
11. exBERT - A Visual Analysis Tool to Explore Learned Representations in Transformer Models
12. Beyond Bilinear - Generalized Multimodal Factorized High-Order Pooling for Visual Question Answering
13. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
14. [Bilinear Attention Networks](2018-bilinear-attention-networks)
15. [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations)
16. [FiLM - Visual Reasoning with a General Conditioning Layer](2018-film-visual-reasoning-with-a-general-conditioning-layer)
17. [Hierarchical Question-Image Co-Attention for Visual Question Answering](2016-hierarchical-question-image-co-attention-for-visual-question-answering)
18. [GLUE - A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding](2018-glue-a-multi-task-benchmark-and-analysis-platform-for-natural-language-understanding)
19. [Cross-lingual Language Model Pretraining](2019-cross-lingual-language-model-pretraining)
20. [Learning to Reason - End-to-End Module Networks for Visual Question Answering](2017-learning-to-reason-end-to-end-module-networks-for-visual-question-answering)
21. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
22. GQA - a new dataset for compositional question answering over real-world images
23. Cycle-Consistency for Robust Visual Question Answering
24. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
25. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation)
26. [Bidirectional Attention Flow for Machine Comprehension](2017-bidirectional-attention-flow-for-machine-comprehension)
27. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
28. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
29. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
30. [Deep Residual Learning for Image Recognition](2015-resnet.md)
31. [Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation](2014-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation)
32. [A Corpus for Reasoning about Natural Language Grounded in Photographs](2019-a-corpus-for-reasoning-about-natural-language-grounded-in-photographs)
33. [Going deeper with convolutions](2015-going-deeper-with-convolutions)
34. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2014-vggnet.md)
35. Pythia v0.1 - the Winning Entry to the VQA Challenge 2018
36. Gated Feedback Recurrent Neural Networks
37. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn.md)
38. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization)
39. [SQuAD - 100,000+ Questions for Machine Comprehension of Text](2016-squad-100-000-questions-for-machine-comprehension-of-text)
40. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
41. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database)
42. [Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units](2016-bridging-nonlinearities-and-stochastic-regularizers-with-gaussian-error-linear-units)
43. [Improving Language Understanding by Generative Pre-Training](2018-improving-language-understanding-by-generative-pre-training)
