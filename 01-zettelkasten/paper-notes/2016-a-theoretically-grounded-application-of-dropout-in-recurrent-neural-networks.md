---
title: A Theoretically Grounded Application of Dropout in Recurrent Neural Networks
authors:
- Y. Gal
- Zoubin Ghahramani
fieldsOfStudy:
- Computer Science
meta_key: 2016-a-theoretically-grounded-application-of-dropout-in-recurrent-neural-networks
numCitedBy: 1320
reading_status: TBD
ref_count: 36
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/A-Theoretically-Grounded-Application-of-Dropout-in-Gal-Ghahramani/0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652?sort=total-citations
venue: NIPS
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/A-Theoretically-Grounded-Application-of-Dropout-in-Gal-Ghahramani/0c1f9ca23f4f09ecfc44bcc8ca1c2736624f4652?sort=total-citations)

# A Theoretically Grounded Application of Dropout in Recurrent Neural Networks

## Abstract

Recurrent neural networks (RNNs) stand at the forefront of many recent developments in deep learning. Yet a major difficulty with these models is their tendency to overfit, with dropout shown to fail when applied to recurrent layers. Recent results at the intersection of Bayesian modelling and deep learning offer a Bayesian interpretation of common deep learning techniques such as dropout. This grounding of dropout in approximate Bayesian inference suggests an extension of the theoretical results, offering insights into the use of dropout with RNN models. We apply this new variational inference based dropout technique in LSTM and GRU models, assessing it on language modelling and sentiment analysis tasks. The new approach outperforms existing techniques, and to the best of our knowledge improves on the single model state-of-the-art in language modelling with the Penn Treebank (73.4 test perplexity). This extends our arsenal of variational tools in deep learning.

## Paper References

1. On Fast Dropout and its Applicability to Recurrent Networks
2. Dropout as a Bayesian Approximation - Representing Model Uncertainty in Deep Learning
3. [Dropout - a simple way to prevent neural networks from overfitting](2014-dropout-a-simple-way-to-prevent-neural-networks-from-overfitting.md)
4. Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference
5. [LSTM Neural Networks for Language Modeling](2012-lstm-neural-networks-for-language-modeling.md)
6. Bayesian recurrent neural network language model
7. Regularization and nonlinearities for neural language models - when are they needed?
8. [Speech recognition with deep recurrent neural networks](2013-speech-recognition-with-deep-recurrent-neural-networks.md)
9. RNNDROP - A novel dropout for RNNS in ASR
10. [Practical Variational Inference for Neural Networks](2011-practical-variational-inference-for-neural-networks.md)
11. Recognizing recurrent neural networks (rRNN) - Bayesian inference for recurrent neural networks
12. Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks
13. [Stochastic Backpropagation and Approximate Inference in Deep Generative Models](2014-stochastic-backpropagation-and-approximate-inference-in-deep-generative-models.md)
14. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks.md)
15. Bayesian dark knowledge
16. Bayesian Dark Knowledge
17. [Recurrent Neural Network Regularization](2014-recurrent-neural-network-regularization.md)
18. Where to apply dropout in recurrent neural networks for handwriting recognition?
19. Variational Dropout and the Local Reparameterization Trick
20. Ensemble learning in Bayesian neural networks
21. Dropout Improves Recurrent Neural Networks for Handwriting Recognition
22. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation.md)
23. Weight Uncertainty in Neural Networks
24. Weight Uncertainty in Neural Network
25. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
26. [Long Short-Term Memory](1997-long-short-term-memory.md)
27. Bayesian Learning for Neural Networks
28. A Practical Bayesian Framework for Backpropagation Networks
29. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
30. [Recurrent Continuous Translation Models](2013-recurrent-continuous-translation-models.md)
31. Keeping the neural networks simple by minimizing the description length of the weights
32. Seeing Stars - Exploiting Class Relationships for Sentiment Categorization with Respect to Rating Scales
33. Building a Large Annotated Corpus of English - The Penn Treebank
