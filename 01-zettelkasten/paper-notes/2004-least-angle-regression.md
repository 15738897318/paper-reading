---
title: Least angle regression
authors:
- B. Efron
- T. Hastie
- I. Johnstone
- R. Tibshirani
fieldsOfStudy:
- Computer Science
meta_key: 2004-least-angle-regression
numCitedBy: 7911
reading_status: TBD
ref_count: 38
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Least-angle-regression-Efron-Hastie/1c7c5595dc7a1f5d360acf5c360ca1ca49536ba5?sort=total-citations
venue: ''
year: 2004
---

[semanticscholar url](https://www.semanticscholar.org/paper/Least-angle-regression-Efron-Hastie/1c7c5595dc7a1f5d360acf5c360ca1ca49536ba5?sort=total-citations)

# Least angle regression

## Abstract

The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps us understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as ordinary least squares applied to the full set of covariates.

## Paper References

1. The Little Bootstrap and other Methods for Dimensionality Selection in Regression - X-Fixed Prediction Error
2. Gaussian model selection
3. [Regression Shrinkage and Selection via the Lasso](1996-regression-shrinkage-and-selection-via-the-lasso.md)
4. A new approach to variable selection in least squares problems
5. On the LASSO and its Dual
6. Linear Model Selection by Cross-validation
7. [Greedy function approximation - A gradient boosting machine.](2001-greedy-function-approximation-a-gradient-boosting-machine.md)
8. The risk inflation criterion for multiple regression
9. On Measuring and Correcting the Effects of Data Mining and Model Selection
10. Matrix computations
11. ON THE DEGREES OF FREEDOM IN SHAPE-RESTRICTED REGRESSION
12. An Information Theoretic Comparison of Model Selection Criteria
13. How Biased is the Apparent Error Rate of a Prediction Rule
14. Better subset regression using the nonnegative garrote
15. Boosting as a Regularized Path to a Maximum Margin Classifier
16. Prediction Games and Arcing Algorithms
17. Solving least squares problems
18. Detecting Differentially Expressed Genes in Microarrays Using Bayesian Model Selection
19. Matrix computations
20. Linear statistical inference and its applications
21. Adapting to unknown sparsity by controlling the false discovery rate
22. Bagging Predictors
23. Maximum likelihood identification of Gaussian autoregressive moving average models
24. Controlling the false discovery rate - a practical and powerful approach to multiple testing
25. A Statistic for Allocating C p to Individual Cases
26. Matrix computations
27. Linear statistical inference and its applications
28. Applied Linear Regression
29. Some Comments on Cp
30. Graphs in Statistical Analysis - Is the Medium the Message?
31. Some Comments on Cp
32. Sliced Inverse Regression for Dimension Reduction
33. Estimation of the Mean of a Multivariate Normal Distribution
34. The Elements of Statistical Learning - Data Mining, Inference, and Prediction, 2nd Edition
35. Calibration and empirical Bayes variable selection
36. [A decision-theoretic generalization of on-line learning and an application to boosting](1995-a-decision-theoretic-generalization-of-on-line-learning-and-an-application-to-boosting.md)
37. Classification and Regression Trees
38. [UCI Repository of machine learning databases](1998-uci-repository-of-machine-learning-databases.md)
