---
title: Attention-Based Models for Speech Recognition
authors:
- J. Chorowski
- Dzmitry Bahdanau
- Dmitriy Serdyuk
- Kyunghyun Cho
- Yoshua Bengio
fieldsOfStudy:
- Computer Science
meta_key: 2015-attention-based-models-for-speech-recognition
numCitedBy: 1886
reading_status: TBD
ref_count: 43
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Attention-Based-Models-for-Speech-Recognition-Chorowski-Bahdanau/b624504240fa52ab76167acfe3156150ca01cf3b?sort=total-citations
venue: NIPS
year: 2015
---

# Attention-Based Models for Speech Recognition

## Abstract

Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis [1,2] and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in [2] reaches a competitive 18.7% phoneme error rate (PER) on the TIMET phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames, which further reduces PER to 17.6% level.

## Paper References

1. [Speech recognition with deep recurrent neural networks](2013-speech-recognition-with-deep-recurrent-neural-networks)
2. Deep Speech - Scaling up end-to-end speech recognition
3. Combining time- and frequency-domain convolution in convolutional neural network-based phone recognition
4. [Sequence to Sequence Learning with Neural Networks](2014-sequence-to-sequence-learning-with-neural-networks)
5. [Towards End-To-End Speech Recognition with Recurrent Neural Networks](2014-towards-end-to-end-speech-recognition-with-recurrent-neural-networks)
6. [End-To-End Memory Networks](2015-end-to-end-memory-networks)
7. End-to-end Continuous Speech Recognition using Attention-based Recurrent NN - First Results
8. Sequence Transduction with Recurrent Neural Networks
9. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate)
10. The Application of Hidden Markov Models in Speech Recognition
11. On Using Monolingual Corpora in Neural Machine Translation
12. [Deep Neural Networks for Acoustic Modeling in Speech Recognition - The Shared Views of Four Research Groups](2012-deep-neural-networks-for-acoustic-modeling-in-speech-recognition-the-shared-views-of-four-research-groups)
13. [Connectionist temporal classification - labelling unsegmented sequence data with recurrent neural networks](2006-connectionist-temporal-classification-labelling-unsegmented-sequence-data-with-recurrent-neural-networks)
14. [Long Short-Term Memory](1997-long-short-term-memory)
15. [Recurrent Models of Visual Attention](2014-recurrent-models-of-visual-attention)
16. Weakly Supervised Memory Networks
17. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
18. [Gradient-based learning applied to document recognition](1998-lenet5.md)
19. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
20. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors)
21. [The Kaldi Speech Recognition Toolkit](2011-the-kaldi-speech-recognition-toolkit)
22. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks)
23. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method)
24. [Practical Variational Inference for Neural Networks](2011-practical-variational-inference-for-neural-networks)
25. [Theano - new features and speed improvements](2012-theano-new-features-and-speed-improvements)
26. [Neural Turing Machines](2014-neural-turing-machines)
27. Theano - A CPU and GPU Math Compiler in Python
28. Blocks and Fuel - Frameworks for deep learning
29. Et al
30. Darpa Timit Acoustic-Phonetic Continuous Speech Corpus CD-ROM {TIMIT} | NIST
31. DARPA TIMIT - - acoustic-phonetic continuous speech corpus CD-ROM, NIST speech disc 1-1.1
32. [Memory Networks](2015-memory-networks)
33. Pylearn2 - a machine learning research library
