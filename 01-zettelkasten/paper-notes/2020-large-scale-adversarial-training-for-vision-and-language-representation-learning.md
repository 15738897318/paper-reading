---
title: Large-Scale Adversarial Training for Vision-and-Language Representation Learning
authors:
- Zhe Gan
- Yen-Chun Chen
- Linjie Li
- Chen Zhu
- Yu Cheng
- Jingjing Liu
fieldsOfStudy:
- Computer Science
meta_key: 2020-large-scale-adversarial-training-for-vision-and-language-representation-learning
numCitedBy: 173
reading_status: TBD
ref_count: 93
tags:
- gen-from-ref
- paper
venue: NeurIPS
year: 2020
---

# Large-Scale Adversarial Training for Vision-and-Language Representation Learning

## Abstract

We present VILLA, the first known effort on large-scale adversarial training for vision-and-language (V+L) representation learning. VILLA consists of two training stages: (i) task-agnostic adversarial pre-training; followed by (ii) task-specific adversarial finetuning. Instead of adding adversarial perturbations on image pixels and textual tokens, we propose to perform adversarial training in the embedding space of each modality. To enable large-scale training, we adopt the "free" adversarial training strategy, and combine it with KL-divergence-based regularization to promote higher invariance in the embedding space. We apply VILLA to current best-performing V+L models, and achieve new state of the art on a wide range of tasks, including Visual Question Answering, Visual Commonsense Reasoning, Image-Text Retrieval, Referring Expression Comprehension, Visual Entailment, and NLVR2.

## Paper References

1. Adversarial Training for Large Neural Language Models
2. FreeLB - Enhanced Adversarial Training for Language Understanding
3. Behind the Scene - Revealing the Secrets of Pre-trained Vision-and-Language Models
4. Improving Neural Language Modeling via Adversarial Training
5. Overcoming Language Priors in Visual Question Answering with Adversarial Regularization
6. Adversarial Training Methods for Semi-Supervised Text Classification
7. [UNITER - Learning UNiversal Image-TExt Representations](2019-uniter-learning-universal-image-text-representations)
8. Using Pre-Training Can Improve Model Robustness and Uncertainty
9. You Only Propagate Once - Accelerating Adversarial Training via Maximal Principle
10. Virtual Adversarial Training - A Regularization Method for Supervised and Semi-Supervised Learning
11. Attacking Visual Language Grounding with Adversarial Examples - A Case Study on Neural Image Captioning
12. [Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training](2020-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training)
13. Adversarial Robustness - From Self-Supervised Pre-Training to Fine-Tuning
14. [ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks](2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks)
15. [Unified Vision-Language Pre-Training for Image Captioning and VQA](2020-unified-vision-language-pre-training-for-image-captioning-and-vqa)
16. Adversarial Training for Free!
17. [VideoBERT - A Joint Model for Video and Language Representation Learning](2019-videobert-a-joint-model-for-video-and-language-representation-learning)
18. Adversarial Examples Improve Image Recognition
19. Boosting Adversarial Training with Hypersphere Embedding
20. [LXMERT - Learning Cross-Modality Encoder Representations from Transformers](2019-lxmert-learning-cross-modality-encoder-representations-from-transformers)
21. Large-scale Pretraining for Visual Dialog - A Simple State-of-the-Art Baseline
22. Metric Learning for Adversarial Robustness
23. Improving the Robustness of Deep Neural Networks via Adversarial Training with Triplet Loss
24. [12-in-1 - Multi-Task Vision and Language Representation Learning](2020-12-in-1-multi-task-vision-and-language-representation-learning)
25. Feature Denoising for Improving Adversarial Robustness
26. [Oscar - Object-Semantics Aligned Pre-training for Vision-Language Tasks](2020-oscar-object-semantics-aligned-pre-training-for-vision-language-tasks)
27. Fast is better than free - Revisiting adversarial training
28. Are Labels Required for Improving Adversarial Robustness?
29. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding)
30. Exact Adversarial Attack to Image Captioning via Structured Output Learning With Latent Variables
31. Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering
32. Ensemble Adversarial Training - Attacks and Defenses
33. Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training
34. VD-BERT - A Unified Vision and Dialog Transformer with BERT
35. Explaining and Harnessing Adversarial Examples
36. [From Recognition to Cognition - Visual Commonsense Reasoning](2019-from-recognition-to-cognition-visual-commonsense-reasoning)
37. Unlabeled Data Improves Adversarial Robustness
38. [VisualBERT - A Simple and Performant Baseline for Vision and Language](2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language)
39. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
40. Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training
41. [VL-BERT - Pre-training of Generic Visual-Linguistic Representations](2020-vl-bert-pre-training-of-generic-visual-linguistic-representations)
42. Visual Entailment - A Novel Task for Fine-Grained Image Understanding
43. XGPT - Cross-modal Generative Pre-Training for Image Captioning
44. Hero - Hierarchical Encoder for Video+Language Omni-representation Pre-training
45. Towards Deep Learning Models Resistant to Adversarial Attacks
46. [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](2016-multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding)
47. [Deep Modular Co-Attention Networks for Visual Question Answering](2019-deep-modular-co-attention-networks-for-visual-question-answering)
48. Learning Conditioned Graph Structures for Interpretable Visual Question Answering
49. [Attention is All you Need](2017-attention-is-all-you-need)
50. Unsupervised Data Augmentation for Consistency Training
51. [Neural Module Networks](2016-neural-module-networks)
52. Contrastive Bidirectional Transformer for Temporal Representation Learning
53. SMART - Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization
54. Pixel-BERT - Aligning Image Pixels with Text by Deep Multi-Modal Transformers
55. [Hierarchical Question-Image Co-Attention for Visual Question Answering](2016-hierarchical-question-image-co-attention-for-visual-question-answering)
56. UniViLM - A Unified Video and Language Pre-Training Model for Multimodal Understanding and Generation
57. MUREL - Multimodal Relational Reasoning for Visual Question Answering
58. [Fusion of Detected Objects in Text for Visual Question Answering](2019-fusion-of-detected-objects-in-text-for-visual-question-answering)
59. Semantically Equivalent Adversarial Rules for Debugging NLP models
60. 5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. - BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
61. [GQA - A New Dataset for Real-World Visual Reasoning and Compositional Question Answering](2019-gqa-a-new-dataset-for-real-world-visual-reasoning-and-compositional-question-answering)
62. [Bilinear Attention Networks](2018-bilinear-attention-networks)
63. [Learning to Reason - End-to-End Module Networks for Visual Question Answering](2017-learning-to-reason-end-to-end-module-networks-for-visual-question-answering)
64. [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations)
65. Relation-Aware Graph Attention Network for Visual Question Answering
66. Intriguing properties of neural networks
67. [Compositional Attention Networks for Machine Reasoning](2018-compositional-attention-networks-for-machine-reasoning)
68. Improving Vision-and-Language Navigation with Image-Text Pairs from the Web
69. Flickr30k Entities - Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models
70. 5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. - BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
71. [A simple neural network module for relational reasoning](2017-a-simple-neural-network-module-for-relational-reasoning)
72. [Stacked Cross Attention for Image-Text Matching](2018-stacked-cross-attention-for-image-text-matching)
73. [Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering](2019-dynamic-fusion-with-intra-and-inter-modality-attention-flow-for-visual-question-answering)
74. [Stacked Attention Networks for Image Question Answering](2016-stacked-attention-networks-for-image-question-answering)
75. [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning)
76. [Modeling Context in Referring Expressions](2016-modeling-context-in-referring-expressions)
77. Multi-step Reasoning via Recurrent Dual Attention for Visual Dialog
78. Meta Module Network for Compositional Visual Reasoning
79. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
80. 5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. - BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
81. Im2Text - Describing Images Using 1 Million Captioned Photographs
82. [Inferring and Executing Programs for Visual Reasoning](2017-inferring-and-executing-programs-for-visual-reasoning)
83. Obfuscated Gradients Give a False Sense of Security - Circumventing Defenses to Adversarial Examples
84. 5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. - BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
85. [UNITER - UNiversal Image-TExt Representation Learning](2020-uniter-universal-image-text-representation-learning)
86. Learning Video Representations using Contrastive Bidirectional Transformer
87. FreeLB - Enhanced Adversarial Training for Natural Language Understanding
88. Cycle-Consistency for Robust Visual Question Answering
89. Show-and-Fool - Crafting Adversarial Examples for Neural Image Captioning
