---
title: Speech recognition with deep recurrent neural networks
authors:
- A. Graves
- Abdel-rahman Mohamed
- Geoffrey E. Hinton
fieldsOfStudy:
- Computer Science
meta_key: 2013-speech-recognition-with-deep-recurrent-neural-networks
numCitedBy: 6931
reading_status: TBD
ref_count: 35
tags:
- gen-from-ref
- paper
venue: 2013 IEEE International Conference on Acoustics, Speech and Signal Processing
year: 2013
---

# Speech recognition with deep recurrent neural networks

## Abstract

Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.

## Paper References

1. Revisiting Recurrent Neural Networks for robust ASR
2. Sequence Transduction with Recurrent Neural Networks
3. [Connectionist temporal classification - labelling unsegmented sequence data with recurrent neural networks](2006-connectionist-temporal-classification-labelling-unsegmented-sequence-data-with-recurrent-neural-networks)
4. [Deep Neural Networks for Acoustic Modeling in Speech Recognition](2012-deep-neural-networks-for-acoustic-modeling-in-speech-recognition)
5. Bidirectional recurrent neural networks
6. Recurrent Neural Networks for Noise Reduction in Robust ASR
7. Applying Convolutional Neural Networks concepts to hybrid NN-HMM model for speech recognition
8. [Deep Neural Networks for Acoustic Modeling in Speech Recognition - The Shared Views of Four Research Groups](2012-deep-neural-networks-for-acoustic-modeling-in-speech-recognition-the-shared-views-of-four-research-groups)
9. Tandem Connectionist Feature Extraction for Conversational Speech Recognition
10. Connectionist Speech Recognition - A Hybrid Approach
11. Investigation of full-sequence training of deep belief networks for speech recognition
12. Supervised Sequence Labelling with Recurrent Neural Networks
13. SCARF - A Segmental CRF Speech Recognition System
14. [Framewise phoneme classification with bidirectional LSTM and other neural network architectures](2005-framewise-phoneme-classification-with-bidirectional-lstm-and-other-neural-network-architectures)
15. Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks
16. [Long Short-Term Memory](1997-long-short-term-memory)
17. Unconstrained Online Handwriting Recognition with Recurrent Neural Networks
18. Discriminatively estimated joint acoustic, duration, and language model for speech recognition
19. Offline Handwriting Recognition with Multidimensional Recurrent Neural Networks
20. Learning Precise Timing with LSTM Recurrent Networks
21. An analysis of noise in recurrent neural networks - convergence and generalization
22. An application of recurrent nets to phone probability estimation
23. [Acoustic Modeling Using Deep Belief Networks](2012-acoustic-modeling-using-deep-belief-networks)
24. Speaker-independent phone recognition using hidden Markov models
25. Learning representations by back-propagating errors
26. Keeping the neural networks simple by minimizing the description length of the weights
27. [Practical Variational Inference for Neural Networks](2011-practical-variational-inference-for-neural-networks)
28. Learning representations by backpropagating errors
