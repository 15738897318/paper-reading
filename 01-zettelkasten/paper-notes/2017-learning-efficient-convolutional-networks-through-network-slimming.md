---
title: Learning Efficient Convolutional Networks through Network Slimming
authors:
- Zhuang Liu
- Jianguo Li
- Zhiqiang Shen
- Gao Huang
- Shoumeng Yan
- Changshui Zhang
fieldsOfStudy:
- Computer Science
meta_key: 2017-learning-efficient-convolutional-networks-through-network-slimming
numCitedBy: 1257
reading_status: TBD
ref_count: 44
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Learning-Efficient-Convolutional-Networks-through-Liu-Li/90a16f34d109b63d95ab4da2d491cbe3a1c8b656?sort=total-citations
venue: 2017 IEEE International Conference on Computer Vision (ICCV)
year: 2017
---

[semanticscholar url](https://www.semanticscholar.org/paper/Learning-Efficient-Convolutional-Networks-through-Liu-Li/90a16f34d109b63d95ab4da2d491cbe3a1c8b656?sort=total-citations)

# Learning Efficient Convolutional Networks through Network Slimming

## Abstract

The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20× reduction in model size and a 5× reduction in computing operations.

## Paper References

1. Less Is More - Towards Compact CNNs
2. The Power of Sparsity in Convolutional Neural Networks
3. [Deep Networks with Stochastic Depth](2016-deep-networks-with-stochastic-depth.md)
4. [Pruning Filters for Efficient ConvNets](2017-pruning-filters-for-efficient-convnets.md)
5. Learning Structured Sparsity in Deep Neural Networks
6. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
7. [Exploiting Linear Structure Within Convolutional Networks for Efficient Evaluation](2014-exploiting-linear-structure-within-convolutional-networks-for-efficient-evaluation.md)
8. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks.md)
9. Multi-Scale Dense Convolutional Networks for Efficient Prediction
10. [Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding](2016-deep-compression-compressing-deep-neural-network-with-pruning-trained-quantization-and-huffman-coding.md)
11. [Learning both Weights and Connections for Efficient Neural Network](2015-learning-both-weights-and-connections-for-efficient-neural-network.md)
12. [Network In Network](2014-network-in-network.md)
13. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
14. Multi-Scale Dense Networks for Resource Efficient Image Classification
15. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
16. [Binarized Neural Networks - Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](2016-binarized-neural-networks-training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or-1.md)
17. Sparse Convolutional Neural Networks
18. [Fully Convolutional Networks for Semantic Segmentation](2017-fully-convolutional-networks-for-semantic-segmentation.md)
19. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
20. [Designing Neural Network Architectures using Reinforcement Learning](2017-designing-neural-network-architectures-using-reinforcement-learning.md)
21. [Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation](2014-rich-feature-hierarchies-for-accurate-object-detection-and-semantic-segmentation.md)
22. Training Sparse Neural Networks
23. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
24. [BinaryNet - Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1](2016-binarynet-training-deep-neural-networks-with-weights-and-activations-constrained-to-1-or-1.md)
25. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
26. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
27. Group sparse regularization for deep neural networks
28. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
29. Compressing Neural Networks with the Hashing Trick
30. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks.md)
31. [On the importance of initialization and momentum in deep learning](2013-on-the-importance-of-initialization-and-momentum-in-deep-learning.md)
32. Neural Network Architecture Optimization through Submodularity and Supermodularity
33. [Maxout Networks](2013-maxout-networks.md)
34. [Reading Digits in Natural Images with Unsupervised Feature Learning](2011-reading-digits-in-natural-images-with-unsupervised-feature-learning.md)
35. Fast Optimization Methods for L1 Regularization - A Comparative Study and Two New Approaches
36. [Torch7 - A Matlab-like Environment for Machine Learning](2011-torch7-a-matlab-like-environment-for-machine-learning.md)
37. Et al
