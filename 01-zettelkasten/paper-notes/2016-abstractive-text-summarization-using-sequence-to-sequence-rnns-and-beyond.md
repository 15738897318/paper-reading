---
title: Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond
authors:
- Ramesh Nallapati
- Bowen Zhou
- C. D. Santos
- Çaglar Gülçehre
- Bing Xiang
fieldsOfStudy:
- Computer Science
meta_key: 2016-abstractive-text-summarization-using-sequence-to-sequence-rnns-and-beyond
numCitedBy: 1569
reading_status: TBD
ref_count: 42
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Abstractive-Text-Summarization-using-RNNs-and-Nallapati-Zhou/f37076f426023241f19cdc2fb0a0fd733a6fa7fa?sort=total-citations
venue: CoNLL
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/Abstractive-Text-Summarization-using-RNNs-and-Nallapati-Zhou/f37076f426023241f19cdc2fb0a0fd733a6fa7fa?sort=total-citations)

# Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond

## Abstract

In this work, we model abstractive text summarization using Attentional Encoder-Decoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-to-word structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.

## Paper References

1. [A Neural Attention Model for Abstractive Sentence Summarization](2015-a-neural-attention-model-for-abstractive-sentence-summarization.md)
2. Sequence-to-Sequence RNNs for Text Summarization
3. Neural Summarization by Extracting Sentences and Words
4. Abstractive Sentence Summarization with Attentive Recurrent Neural Networks
5. HEADS - Headline Generation as Sequence Prediction Using an Abstract Feature-Rich Space
6. A Hierarchical Neural Autoencoder for Paragraphs and Documents
7. Long story short - Global unsupervised models for keyphrase based meeting summarization
8. Extractive Summarization Using Supervised and Semi-Supervised Learning
9. Temporal Attention Model for Neural Machine Translation
10. LCSTS - A Large Scale Chinese Short Text Summarization Dataset
11. Headline Generation Based on Statistical Translation
12. Automatic Text Summarization Using a Machine Learning Approach
13. Pointing the Unknown Words
14. [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](2014-empirical-evaluation-of-gated-recurrent-neural-networks-on-sequence-modeling.md)
15. Long Short-Term Memory-Networks for Machine Reading
16. Sequence to Sequence -- Video to Text
17. Overcoming the Lack of Parallel Data in Sentence Compression
18. Title Generation with Quasi-Synchronous Grammar
19. Self reinforcement for important passage retrieval
20. End-to-end attention-based large vocabulary speech recognition
21. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch.md)
22. Sentence Compression Beyond Word Deletion
23. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend.md)
24. Addressing the Rare Word Problem in Neural Machine Translation
25. Graph-Based Keyword Extraction for Single-Document Summarization
26. On Using Very Large Target Vocabulary for Neural Machine Translation
27. [Neural Machine Translation by Jointly Learning to Align and Translate](2015-neural-machine-translation-by-jointly-learning-to-align-and-translate.md)
28. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality.md)
29. BBN/UMD at DUC-2004 - Topiary
30. LexRank - Graph-based Lexical Centrality as Salience in Text Summarization
31. [Pointer Networks](2015-pointer-networks.md)
32. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method.md)
