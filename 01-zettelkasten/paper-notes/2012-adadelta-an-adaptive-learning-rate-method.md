---
title: ADADELTA - An Adaptive Learning Rate Method
authors:
- Matthew D. Zeiler
fieldsOfStudy:
- Computer Science
meta_key: 2012-adadelta-an-adaptive-learning-rate-method
numCitedBy: 5487
reading_status: TBD
ref_count: 8
tags:
- gen-from-ref
- paper
venue: ArXiv
year: 2012
---

# ADADELTA - An Adaptive Learning Rate Method

## Abstract

We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.

## Paper References

1. No more pesky learning rates
2. Learning representations by back-propagating errors
3. [Adaptive Subgradient Methods for Online Learning and Stochastic Optimization](2010-adaptive-subgradient-methods-for-online-learning-and-stochastic-optimization)
4. [Large Scale Distributed Deep Networks](2012-large-scale-distributed-deep-networks)
5. Application of Pretrained Deep Neural Networks to Large Vocabulary Speech Recognition
6. [Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition](2012-context-dependent-pre-trained-deep-neural-networks-for-large-vocabulary-speech-recognition)
7. Improving the convergence of back-propagation learning with second-order methods
8. A Stochastic Approximation Method for Optimization Problems
