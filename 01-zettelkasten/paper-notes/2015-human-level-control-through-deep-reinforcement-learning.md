---
title: Human-level control through deep reinforcement learning
authors:
- Volodymyr Mnih
- K. Kavukcuoglu
- David Silver
- Andrei A. Rusu
- J. Veness
- Marc G. Bellemare
- A. Graves
- Martin A. Riedmiller
- A. Fidjeland
- Georg Ostrovski
- Stig Petersen
- Charlie Beattie
- A. Sadik
- Ioannis Antonoglou
- Helen King
- D. Kumaran
- Daan Wierstra
- S. Legg
- D. Hassabis
fieldsOfStudy:
- Computer Science
meta_key: 2015-human-level-control-through-deep-reinforcement-learning
numCitedBy: 16393
reading_status: TBD
ref_count: 36
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Human-level-control-through-deep-reinforcement-Mnih-Kavukcuoglu/e0e9a94c4a6ba219e768b4e59f72c18f0a22e23d?sort=total-citations
venue: Nature
year: 2015
---

# Human-level control through deep reinforcement learning

## Abstract

The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.

## Paper References

1. Reinforcement learning for robots using neural networks
2. [Reinforcement Learning - An Introduction](2005-reinforcement-learning-an-introduction.md)
3. Deep auto-encoder neural networks in reinforcement learning
4. Reinforcement learning can account for associative and perceptual learning on a visual decision task
5. The Arcade Learning Environment - An Evaluation Platform for General Agents (Extended Abstract)
6. [Learning Deep Architectures for AI](2007-learning-deep-architectures-for-ai.md)
7. Reinforcement learning for robot soccer
8. An object-oriented representation for efficient reinforcement learning
9. A Neural Substrate of Prediction and Reward
10. Neural Fitted Q Iteration - First Experiences with a Data Efficient Neural Reinforcement Learning Method
11. Animal Intelligence; Experimental Studies
12. Why there are complementary learning systems in the hippocampus and neocortex - insights from the successes and failures of connectionist models of learning and memory.
13. Technical Note - Q-Learning
14. Visual categorization shapes feature selectivity in the primate temporal cortex
15. [Neocognitron - A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position](2004-neocognitron-a-self-organizing-neural-network-model-for-a-mechanism-of-pattern-recognition-unaffected-by-shift-in-position.md)
16. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
17. Investigating Contingency Awareness Using Atari 2600 Games
18. Prioritized Sweeping - Reinforcement Learning with Less Data and Less Time
19. Biasing the content of hippocampal replay during sleep
20. Planning and Acting in Partially Observable Stochastic Domains
21. [Reducing the Dimensionality of Data with Neural Networks](2006-reducing-the-dimensionality-of-data-with-neural-networks.md)
22. Parallel distributed processing - explorations in the microstructure of cognition, vol. 1 - foundations
23. Play it again - reactivation of waking experience and memory
24. Object recognition with features inspired by visual cortex
25. Temporal difference learning and TD-Gammon
26. Analysis of Temporal-Diffference Learning with Function Approximation
27. Shape and arrangement of columns in cat's striate cortex
28. Universal Intelligence - A Definition of Machine Intelligence
29. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition.md)
30. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines.md)
31. What is the best multi-stage architecture for object recognition?
32. [Visualizing Data using t-SNE](2008-visualizing-data-using-t-sne.md)
33. General Game Playing - Overview of the AAAI Competition
