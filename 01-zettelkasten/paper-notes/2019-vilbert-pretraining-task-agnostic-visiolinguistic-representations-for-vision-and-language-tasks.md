---
title: ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks
authors:
- Jiasen Lu
- Dhruv Batra
- Devi Parikh
- Stefan Lee
fieldsOfStudy:
- Computer Science
meta_key: 2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks
numCitedBy: 1319
reading_status: TBD
ref_count: 53
tags:
- detection
- gen-from-ref
- paper
urls:
- https://www.semanticscholar.org/paper/ViLBERT:-Pretraining-Task-Agnostic-Visiolinguistic-Lu-Batra/65a9c7b0800c86a196bc14e7621ff895cc6ab287?sort=total-citations
venue: NeurIPS
year: 2019
---

# ViLBERT - Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks

## Abstract

We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.

## Paper References

1. [VisualBERT - A Simple and Performant Baseline for Vision and Language](2019-visualbert-a-simple-and-performant-baseline-for-vision-and-language)
2. [VL-BERT - Pre-training of Generic Visual-Linguistic Representations](2020-vl-bert-pre-training-of-generic-visual-linguistic-representations)
3. [Unified Vision-Language Pre-Training for Image Captioning and VQA](2020-unified-vision-language-pre-training-for-image-captioning-and-vqa)
4. [Unicoder-VL - A Universal Encoder for Vision and Language by Cross-modal Pre-training](2020-unicoder-vl-a-universal-encoder-for-vision-and-language-by-cross-modal-pre-training)
5. [LXMERT - Learning Cross-Modality Encoder Representations from Transformers](2019-lxmert-learning-cross-modality-encoder-representations-from-transformers)
6. [BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding](2019-bert.md)
7. Modulating early visual processing by language
8. [MAttNet - Modular Attention Network for Referring Expression Comprehension](2018-mattnet-modular-attention-network-for-referring-expression-comprehension)
9. [VideoBERT - A Joint Model for Video and Language Representation Learning](2019-videobert-a-joint-model-for-video-and-language-representation-learning)
10. Visual Dialog
11. [From Recognition to Cognition - Visual Commonsense Reasoning](2019-from-recognition-to-cognition-visual-commonsense-reasoning)
12. FOIL it! Find One mismatch between Image and Language caption
13. [Visual Genome - Connecting Language and Vision Using Crowdsourced Dense Image Annotations](2016-visual-genome-connecting-language-and-vision-using-crowdsourced-dense-image-annotations)
14. [Attention is All you Need](2017-transformer.md)
15. [Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering](2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering)
16. Vision-and-Language Navigation - Interpreting Visually-Grounded Navigation Instructions in Real Environments
17. Unsupervised Visual Representation Learning by Context Prediction
18. Look, Listen and Learn
19. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
20. Colorization as a Proxy Task for Visual Understanding
21. Embodied Question Answering
22. Context Encoders - Feature Learning by Inpainting
23. Shuffle and Learn - Unsupervised Learning Using Temporal Order Verification
24. Learning Image Representations Tied to Ego-Motion
25. Don't Just Assume; Look and Answer - Overcoming Priors for Visual Question Answering
26. [Cross-lingual Language Model Pretraining](2019-cross-lingual-language-model-pretraining)
27. [Stacked Cross Attention for Image-Text Matching](2018-stacked-cross-attention-for-image-text-matching)
28. [Deep Contextualized Word Representations](2018-deep-contextualized-word-representations)
29. Learning Features by Watching Objects Move
30. nocaps - novel object captioning at scale
31. [Dynamic Fusion With Intra- and Inter-Modality Attention Flow for Visual Question Answering](2019-dynamic-fusion-with-intra-and-inter-modality-attention-flow-for-visual-question-answering)
32. ShapeCodes - Self-supervised Feature Learning by Lifting Views to Viewgrids
33. [Aligning Books and Movies - Towards Story-Like Visual Explanations by Watching Movies and Reading Books](2015-aligning-books-and-movies-towards-story-like-visual-explanations-by-watching-movies-and-reading-books)
34. [Conceptual Captions - A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning](2018-conceptual-captions-a-cleaned-hypernymed-image-alt-text-dataset-for-automatic-image-captioning)
35. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation)
36. [Deep Residual Learning for Image Recognition](2015-resnet.md)
37. [Mask R-CNN](2017-mask-r-cnn.md)
38. One billion word benchmark for measuring progress in statistical language modeling
39. Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks
40. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks)
41. Colorful Image Colorization
42. [From image descriptions to visual denotations - New similarity metrics for semantic inference over event descriptions](2014-from-image-descriptions-to-visual-denotations-new-similarity-metrics-for-semantic-inference-over-event-descriptions)
43. BERT has a Mouth, and It Must Speak - BERT as a Markov Random Field Language Model
44. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge)
45. Book Review - Mind as machine - a history of cognitive science
46. [Microsoft COCO Captions - Data Collection and Evaluation Server](2015-microsoft-coco-captions-data-collection-and-evaluation-server)
47. ReferItGame - Referring to Objects in Photographs of Natural Scenes
