---
title: The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations
authors:
- Felix Hill
- Antoine Bordes
- S. Chopra
- J. Weston
fieldsOfStudy:
- Computer Science
meta_key: 2016-the-goldilocks-principle-reading-children-s-books-with-explicit-memory-representations
numCitedBy: 536
reading_status: TBD
ref_count: 40
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/The-Goldilocks-Principle:-Reading-Children's-Books-Hill-Bordes/35b91b365ceb016fb3e022577cec96fb9b445dc5?sort=total-citations
venue: ICLR
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/The-Goldilocks-Principle:-Reading-Children's-Books-Hill-Bordes/35b91b365ceb016fb3e022577cec96fb9b445dc5?sort=total-citations)

# The Goldilocks Principle - Reading Children's Books with Explicit Memory Representations

## Abstract

We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.

## Paper References

1. The Microsoft Research Sentence Completion Challenge
2. A Cache-Based Natural Language Model for Speech Recognition
3. Interaction with context during human sentence processing
4. MCTest - A Challenge Dataset for the Open-Domain Machine Comprehension of Text
5. The neurobiology of semantic memory
6. [Towards AI-Complete Question Answering - A Set of Prerequisite Toy Tasks](2016-towards-ai-complete-question-answering-a-set-of-prerequisite-toy-tasks)
7. Large-scale Simple Question Answering with Memory Networks
8. [Teaching Machines to Read and Comprehend](2015-teaching-machines-to-read-and-comprehend)
9. Context dependent recurrent neural network language model
10. [End-To-End Memory Networks](2015-end-to-end-memory-networks)
11. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
12. Large scale image annotation - learning to rank with joint word-image embeddings
13. [A Neural Attention Model for Abstractive Sentence Summarization](2015-a-neural-attention-model-for-abstractive-sentence-summarization)
14. Learning to Transduce with Unbounded Memory
15. Scalable Modified Kneser-Ney Language Model Estimation
16. Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets
17. Word frequency distributions and lexical semantics
18. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors)
19. Learning long-term dependencies with gradient descent is difficult
20. Unconstrained On-line Handwriting Recognition with Recurrent Neural Networks
21. [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](2018-published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks)
22. Transition-Based Dependency Parsing with Stack Long Short-Term Memory
23. [Simple statistical gradient-following algorithms for connectionist reinforcement learning](2004-simple-statistical-gradient-following-algorithms-for-connectionist-reinforcement-learning)
24. [The Stanford CoreNLP Natural Language Processing Toolkit](2014-the-stanford-corenlp-natural-language-processing-toolkit)
25. [Regularization of Neural Networks using DropConnect](2013-regularization-of-neural-networks-using-dropconnect)
26. Goldilocks and the Three Bears
27. [Under Review as a Conference Paper at Iclr 2017 Delving into Transferable Adversarial Ex- Amples and Black-box Attacks](2016-under-review-as-a-conference-paper-at-iclr-2017-delving-into-transferable-adversarial-ex-amples-and-black-box-attacks)
28. [Ask Me Anything - Dynamic Memory Networks for Natural Language Processing](2016-ask-me-anything-dynamic-memory-networks-for-natural-language-processing)
29. [Memory Networks](2015-memory-networks)
30. [Improving Word Representations via Global Context and Multiple Word Prototypes](2012-improving-word-representations-via-global-context-and-multiple-word-prototypes)
