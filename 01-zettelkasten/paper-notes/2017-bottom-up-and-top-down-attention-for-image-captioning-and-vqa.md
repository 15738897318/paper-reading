---
title: Bottom-Up and Top-Down Attention for Image Captioning and VQA
authors:
- Peter Anderson
- Xiaodong He
- Chris Buehler
- Damien Teney
- Mark Johnson
- Stephen Gould
- Lei Zhang
fieldsOfStudy:
- Computer Science
meta_key: 2017-bottom-up-and-top-down-attention-for-image-captioning-and-vqa
numCitedBy: 293
reading_status: TBD
ref_count: 58
tags:
- gen-from-ref
- other-default
- paper
venue: ArXiv
year: 2017
---

# Bottom-Up and Top-Down Attention for Image Captioning and VQA

## Abstract

Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and topdown attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, improving the best published result in terms of CIDEr score from 114.7 to 117.9 and BLEU-4 from 35.2 to 36.9. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain a new state-of-the-art on the VQA v2.0 dataset with 70.2% overall accuracy.

## Paper References

1. [Hierarchical Question-Image Co-Attention for Visual Question Answering](2016-hierarchical-question-image-co-attention-for-visual-question-answering)
2. Areas of Attention for Image Captioning
3. [Knowing When to Look - Adaptive Attention via a Visual Sentinel for Image Captioning](2017-knowing-when-to-look-adaptive-attention-via-a-visual-sentinel-for-image-captioning)
4. [What Value Do Explicit High Level Concepts Have in Vision to Language Problems?](2016-what-value-do-explicit-high-level-concepts-have-in-vision-to-language-problems)
5. [Image Captioning with Semantic Attention](2016-image-captioning-with-semantic-attention)
6. Aligning where to see and what to tell - image caption with region-based attention and scene factorization
7. [Visual7W - Grounded Question Answering in Images](2016-visual7w-grounded-question-answering-in-images)
8. Zero-Shot Visual Question Answering
9. Self-Critical Sequence Training for Image Captioning
10. [Making the V in VQA Matter - Elevating the Role of Image Understanding in Visual Question Answering](2017-making-the-v-in-vqa-matter-elevating-the-role-of-image-understanding-in-visual-question-answering)
11. Show, Ask, Attend, and Answer - A Strong Baseline For Visual Question Answering
12. [Boosting Image Captioning with Attributes](2017-boosting-image-captioning-with-attributes)
13. [DenseCap - Fully Convolutional Localization Networks for Dense Captioning](2016-densecap-fully-convolutional-localization-networks-for-dense-captioning)
14. [Ask, Attend and Answer - Exploring Question-Guided Spatial Attention for Visual Question Answering](2016-ask-attend-and-answer-exploring-question-guided-spatial-attention-for-visual-question-answering)
15. [From captions to visual concepts and back](2015-from-captions-to-visual-concepts-and-back)
16. End-to-End Concept Word Detection for Video Captioning, Retrieval, and Question Answering
17. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator)
18. [Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)](2015-deep-captioning-with-multimodal-recurrent-neural-networks-m-rnn)
19. [Stacked Attention Networks for Image Question Answering](2016-stacked-attention-networks-for-image-question-answering)
20. [Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding](2016-multimodal-compact-bilinear-pooling-for-visual-question-answering-and-visual-grounding)
21. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
22. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
23. [Faster R-CNN - Towards Real-Time Object Detection with Region Proposal Networks](2015-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks)
24. [CIDEr - Consensus-based image description evaluation](2015-cider-consensus-based-image-description-evaluation)
25. Optimization of image description metrics using policy gradient methods
26. [SPICE - Semantic Propositional Image Caption Evaluation](2016-spice-semantic-propositional-image-caption-evaluation)
27. [SSD - Single Shot MultiBox Detector](2016-ssd-single-shot-multibox-detector)
28. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition)
29. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
30. [Review Networks for Caption Generation](2016-review-networks-for-caption-generation)
31. [Long-term recurrent convolutional networks for visual recognition and description](2015-long-term-recurrent-convolutional-networks-for-visual-recognition-and-description)
32. [Order-Embeddings of Images and Language](2016-order-embeddings-of-images-and-language)
33. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge)
34. [Language Modeling with Gated Convolutional Networks](2017-language-modeling-with-gated-convolutional-networks)
35. [Selective Search for Object Recognition](2013-selective-search-for-object-recognition)
36. Maximum Expected BLEU Training of Phrase and Lexicon Translation Models
37. Objects and attention - the state of the art
38. [GloVe - Global Vectors for Word Representation](2014-glove-global-vectors-for-word-representation)
39. Perceptual grouping and attention in visual search for features and for objects.
40. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks)
41. [ROUGE - A Package for Automatic Evaluation of Summaries](2004-rouge-a-package-for-automatic-evaluation-of-summaries)
42. [Microsoft COCO Captions - Data Collection and Evaluation Server](2015-microsoft-coco-captions-data-collection-and-evaluation-server)
43. [Spatial Transformer Networks](2015-spatial-transformer-networks)
44. [Long Short-Term Memory](1997-long-short-term-memory)
45. Highway Networks
46. A feature-integration theory of attention
47. Control of goal-directed and stimulus-driven attention in the brain
48. [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](2014-learning-phrase-representations-using-rnn-encoder-decoder-for-statistical-machine-translation)
49. Shifting visual attention between objects and locations - evidence from normal and parietal lesion subjects.
50. [Bleu - a Method for Automatic Evaluation of Machine Translation](2002-bleu-a-method-for-automatic-evaluation-of-machine-translation)
51. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method)
52. [Meteor Universal - Language Specific Translation Evaluation for Any Target Language](2014-meteor-universal-language-specific-translation-evaluation-for-any-target-language)
53. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning
54. Response to Comment on Top-Down Versus Bottom-Up Control of Attention in the Prefrontal and Posterior Parietal Cortices
55. [Revisiting Visual Question Answering Baselines](2016-revisiting-visual-question-answering-baselines)
56. [You Only Look Once - Unified, Real-Time Object Detection](2016-you-only-look-once-unified-real-time-object-detection)
