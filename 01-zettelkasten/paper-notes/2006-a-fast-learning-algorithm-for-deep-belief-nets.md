---
title: A Fast Learning Algorithm for Deep Belief Nets
authors:
- Geoffrey E. Hinton
- Simon Osindero
- Y. Teh
fieldsOfStudy:
- Computer Science
meta_key: 2006-a-fast-learning-algorithm-for-deep-belief-nets
numCitedBy: 13458
reading_status: TBD
ref_count: 33
tags:
- gen-from-ref
- paper
venue: Neural Computation
year: 2006
---

# A Fast Learning Algorithm for Deep Belief Nets

## Abstract

We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.

## Paper References

1. Visual Recognition and Inference Using Dynamic Overcomplete Sparse Learning
2. Connectionist Learning of Belief Networks
3. Rate-coded Restricted Boltzmann Machines for Face Recognition
4. On Contrastive Divergence Learning
5. Knowledge Transfer in Deep convolutional Neural Nets
6. Recognizing Hand-written Digits Using Hierarchical Products of Experts
7. Optimal unsupervised learning in a single-layer linear feedforward neural network
8. Boosting a weak learning algorithm by majority
9. Energy-Based Models for Sparse Overcomplete Representations
10. Learning Sparse Topographic Representations with Products of Student-t Distributions
11. Diffusion Networks, Products of Experts, and Factor Analysis
12. DIFFUSION NETWORKS , PRODUCT OF EXPERTS , AND FACTOR ANALYSIS
13. Best practices for convolutional neural networks applied to visual document analysis
14. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition)
15. Learning multiple layers of representation
16. [Training Products of Experts by Minimizing Contrastive Divergence](2002-training-products-of-experts-by-minimizing-contrastive-divergence)
17. Fields of Experts - a framework for learning image priors
18. Deep, Narrow Sigmoid Belief Networks Are Universal Approximators
19. Exponential Family Harmoniums with an Application to Information Retrieval
20. A View of the Em Algorithm that Justifies Incremental, Sparse, and other Variants
21. Intuition, Insight, Imagination and Creativity
22. The wake-sleep algorithm for unsupervised neural networks.
23. Bayesian independent component analysis - Variational methods and non-negative decompositions
24. Probabilistic reasoning in intelligent systems - networks of plausible inference
25. Choosing search heuristics by non-stationary reinforcement learning
26. The Bayesian revolution approaches psychological development.
27. Projection Pursuit Regression
28. Training Invariant Support Vector Machines
29. Toward automatic phenotyping of developing embryos from videos
30. Hierarchical Bayesian inference in the visual cortex.
