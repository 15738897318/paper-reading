---
title: HyperNetworks
authors:
- David Ha
- Andrew M. Dai
- Quoc V. Le
fieldsOfStudy:
- Computer Science
meta_key: 2017-hypernetworks
numCitedBy: 383
reading_status: TBD
ref_count: 62
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/HyperNetworks-Ha-Dai/563783de03452683a9206e85fe6d661714436686?sort=total-citations
venue: ICLR
year: 2017
---

[semanticscholar url](https://www.semanticscholar.org/paper/HyperNetworks-Ha-Dai/563783de03452683a9206e85fe6d661714436686?sort=total-citations)

# HyperNetworks

## Abstract

This work explores hypernetworks: an approach of using one network, also known as a hypernetwork, to generate the weights for another network. We apply hypernetworks to generate adaptive weights for recurrent networks. In this case, hypernetworks can be viewed as a relaxed form of weight-sharing across layers. In our implementation, hypernetworks are are trained jointly with the main network in an end-to-end fashion. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks.

## Paper References

1. [Zoneout - Regularizing RNNs by Randomly Preserving Hidden Activations](2017-zoneout-regularizing-rnns-by-randomly-preserving-hidden-activations.md)
2. SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS
3. [Generating Text with Recurrent Neural Networks](2011-generating-text-with-recurrent-neural-networks.md)
4. [Recurrent Highway Networks](2017-recurrent-highway-networks.md)
5. [On Multiplicative Integration with Recurrent Neural Networks](2016-on-multiplicative-integration-with-recurrent-neural-networks.md)
6. [Grid Long Short-Term Memory](2016-grid-long-short-term-memory.md)
7. [Recurrent Batch Normalization](2017-recurrent-batch-normalization.md)
8. Orthogonal RNNs and Long-Memory Tasks
9. Recurrent Dropout without Memory Loss
10. [Hierarchical Multiscale Recurrent Neural Networks](2017-hierarchical-multiscale-recurrent-neural-networks.md)
11. Dynamic Filter Networks
12. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks.md)
13. Gated Feedback Recurrent Neural Networks
14. [FitNets - Hints for Thin Deep Nets](2015-fitnets-hints-for-thin-deep-nets.md)
15. A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks
16. Learning to Control Fast-Weight Memories - An Alternative to Dynamic Recurrent Networks
17. [Generating Sequences With Recurrent Neural Networks](2013-generating-sequences-with-recurrent-neural-networks.md)
18. [Network In Network](2014-network-in-network.md)
19. [Wide Residual Networks](2016-wide-residual-networks.md)
20. Decoupled Neural Interfaces using Synthetic Gradients
21. Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation
22. [Training Very Deep Networks](2015-training-very-deep-networks.md)
23. Evolving neural networks in compressed weight space
24. Convolution by Evolution - Differentiable Pattern Producing Networks
25. Surprisal-Driven Feedback in Recurrent Networks
26. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks.md)
27. Recurrent Memory Array Structures
28. Residual Networks of Residual Networks - Multilevel Residual Networks
29. [Improving neural networks by preventing co-adaptation of feature detectors](2012-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors.md)
30. Regularizing Recurrent Networks - On Injected Noise and Norm-based Methods
31. [Long Short-Term Memory](1997-long-short-term-memory.md)
32. A â€˜Self-Referential' Weight Matrix
33. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
34. [Learning to learn by gradient descent by gradient descent](2016-learning-to-learn-by-gradient-descent-by-gradient-descent.md)
35. [Deep Networks with Stochastic Depth](2016-deep-networks-with-stochastic-depth.md)
36. Learning feed-forward one-shot learners
37. First-order versus second-order single-layer recurrent neural networks
38. ACDC - A Structured Efficient Linear Layer
39. [Predicting Parameters in Deep Learning](2013-predicting-parameters-in-deep-learning.md)
40. Evolving Modular Fast-Weight Networks for Control
41. [TensorFlow - Large-Scale Machine Learning on Heterogeneous Distributed Systems](2016-tensorflow-large-scale-machine-learning-on-heterogeneous-distributed-systems.md)
42. [Google's Neural Machine Translation System - Bridging the Gap between Human and Machine Translation](2016-google-s-neural-machine-translation-system-bridging-the-gap-between-human-and-machine-translation.md)
43. [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](2016-fast-and-accurate-deep-network-learning-by-exponential-linear-units-elus.md)
44. Deep Fried Convnets
45. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
46. Mixture Density Networks
47. Building a Large Annotated Corpus of English - The Penn Treebank
48. IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard
49. Handwritten Digit Recognition with a Back-Propagation Network
50. Fast algorithms for hierarchically semiseparable matrices
