---
title: SGDR - Stochastic Gradient Descent with Warm Restarts
authors:
- I. Loshchilov
- F. Hutter
fieldsOfStudy:
- Computer Science
meta_key: 2017-sgdr-stochastic-gradient-descent-with-warm-restarts
numCitedBy: 2822
reading_status: TBD
ref_count: 45
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/SGDR:-Stochastic-Gradient-Descent-with-Warm-Loshchilov-Hutter/b022f2a277a4bf5f42382e86e4380b96340b9e86?sort=total-citations
venue: ICLR
year: 2017
---

# SGDR - Stochastic Gradient Descent with Warm Restarts

## Abstract

Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at this https URL

## Paper References

1. SGDR - Stochastic Gradient Descent with Restarts
2. Snapshot Ensembles - Train 1, get M for free
3. Equilibrated adaptive learning rates for non-convex optimization
4. [Adam - A Method for Stochastic Optimization](2015-adam-a-method-for-stochastic-optimization.md)
5. RMSProp and equilibrated adaptive learning rates for non-convex optimization.
6. SGD-QN - Careful Quasi-Newton Stochastic Gradient Descent
7. Adaptive Restart for Accelerated Gradient Schemes
8. [Deep Networks with Stochastic Depth](2016-deep-networks-with-stochastic-depth.md)
9. [Identity Mappings in Deep Residual Networks](2016-identity-mappings-in-deep-residual-networks.md)
10. On Restart Procedures for the Conjugate Gradient Method
11. [ADADELTA - An Adaptive Learning Rate Method](2012-adadelta-an-adaptive-learning-rate-method.md)
12. Cyclical Learning Rates for Training Neural Networks
13. [Deep Pyramidal Residual Networks](2017-deep-pyramidal-residual-networks.md)
14. [Wide Residual Networks](2016-wide-residual-networks.md)
15. No More Pesky Learning Rate Guessing Games
16. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
17. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
18. Tiny ImageNet Visual Recognition Challenge
19. [DeCAF - A Deep Convolutional Activation Feature for Generic Visual Recognition](2014-decaf-a-deep-convolutional-activation-feature-for-generic-visual-recognition.md)
20. Restart procedures for the conjugate gradient method
21. [Deep Compression - Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding](2016-deep-compression-compressing-deep-neural-network-with-pruning-trained-quantization-and-huffman-coding.md)
22. New types of deep neural network learning for speech recognition and related applications - an overview
23. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images.md)
24. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks.md)
25. Residual Networks of Residual Networks - Multilevel Residual Networks
26. Local minima and plateaus in hierarchical structures of multilayer perceptrons
27. Deep learning with convolutional neural networks for brain mapping and decoding of movement-related information from the human EEG
28. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization
29. On the limited memory BFGS method for large scale optimization
30. Evaluating the CMA Evolution Strategy on Multimodal Test Functions
31. Multimodal Optimization by Means of Evolutionary Algorithms
32. The Loss Surfaces of Multilayer Networks
33. Benchmarking a BI-population CMA-ES on the BBOB-2009 function testbed
34. Introductory Lectures on Convex Optimization - A Basic Course
35. [Published as a conference paper at ICLR 2018 S IMULATING A CTION D YNAMICS WITH N EURAL P ROCESS N ETWORKS](2018-published-as-a-conference-paper-at-iclr-2018-s-imulating-a-ction-d-ynamics-with-n-eural-p-rocess-n-etworks.md)
36. Niching Methods and Multimodal Optimization Performance
37. Alternative Restart Strategies for CMA-ES
38. Benchmarking the BFGS algorithm on the BBOB-2009 function testbed
39. Niching the CMA-ES via nearest-better clustering
40. A method for solving the convex programming problem with convergence rate O(1/k^2)
41. Function minimization by conjugate gradients
42. The Loss Surface of Multilayer Networks
