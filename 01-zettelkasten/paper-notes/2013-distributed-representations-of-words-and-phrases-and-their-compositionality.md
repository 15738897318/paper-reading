---
title: Distributed Representations of Words and Phrases and their Compositionality
authors:
- Tomas Mikolov
- Ilya Sutskever
- Kai Chen
- G. Corrado
- J. Dean
fieldsOfStudy:
- Computer Science
meta_key: 2013-distributed-representations-of-words-and-phrases-and-their-compositionality
numCitedBy: 26176
reading_status: TBD
ref_count: 32
tags:
- gen-from-ref
- paper
venue: NIPS
year: 2013
---

# Distributed Representations of Words and Phrases and their Compositionality

## Abstract

The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. 
 
An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.

## Paper References

1. [Semantic Compositionality through Recursive Matrix-Vector Spaces](2012-semantic-compositionality-through-recursive-matrix-vector-spaces)
2. [Linguistic Regularities in Continuous Space Word Representations](2013-linguistic-regularities-in-continuous-space-word-representations)
3. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
4. A Neural Probabilistic Language Model
5. A fast and simple algorithm for training neural probabilistic language models
6. Distributional Semantics Beyond Words - Supervised Learning of Analogy and Paraphrase
7. Hierarchical Probabilistic Neural Network Language Model
8. Word Representations - A Simple and General Method for Semi-Supervised Learning
9. A Scalable Hierarchical Distributed Language Model
10. Continuous space language models
11. From Frequency to Meaning - Vector Space Models of Semantics
12. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
13. [Parsing Natural Scenes and Natural Language with Recursive Neural Networks](2011-parsing-natural-scenes-and-natural-language-with-recursive-neural-networks)
14. Strategies for training large scale neural network language models
15. Statistical Language Models Based on Neural Networks
16. Learning representations by back-propagating errors
17. [Extensions of recurrent neural network language model](2011-extensions-of-recurrent-neural-network-language-model)
18. Learning representations by backpropagating errors
19. Domain Adaptation for Large-Scale Sentiment Classification - A Deep Learning Approach
20. Noise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics
21. WSABIE - Scaling Up to Large Vocabulary Image Annotation
22. and
