---
title: Representation Learning - A Review and New Perspectives
authors:
- Yoshua Bengio
- Aaron C. Courville
- Pascal Vincent
fieldsOfStudy:
- Computer Science
meta_key: 2013-representation-learning-a-review-and-new-perspectives
numCitedBy: 8798
reading_status: TBD
ref_count: 285
tags:
- gen-from-ref
- paper
venue: IEEE Transactions on Pattern Analysis and Machine Intelligence
year: 2013
---

# Representation Learning - A Review and New Perspectives

## Abstract

The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.

## Paper References

1. Deep Learning of Representations for Unsupervised and Transfer Learning
2. Unsupervised and Transfer Learning Challenge - a Deep Learning Approach
3. Sparse Feature Learning for Deep Belief Networks
4. [Extracting and composing robust features with denoising autoencoders](2008-extracting-and-composing-robust-features-with-denoising-autoencoders)
5. The Manifold Tangent Classifier
6. Why Does Unsupervised Pre-training Help Deep Learning?
7. Understanding Representations Learned in Deep Architectures
8. Large-Scale Learning of Embeddings with Reconstruction Sampling
9. A Generative Process for sampling Contractive Auto-Encoders
10. On deep generative models with applications to recognition
11. Neural Networks - Tricks of the Trade
12. Non-Local Manifold Tangent Learning
13. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations
14. Unsupervised feature learning for audio classification using convolutional deep belief networks
15. Revisiting Natural Gradient for Deep Networks
16. Deep Learning Made Easier by Linear Transformations in Perceptrons
17. Selecting Receptive Fields in Deep Networks
18. Scaling learning algorithms towards AI
19. Measuring Invariances in Deep Networks
20. Learning Deep Energy Models
21. Algorithms for manifold learning
22. Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network
23. Enhanced Gradient and Adaptive Learning Rate for Training Restricted Boltzmann Machines
24. [Learning Deep Architectures for AI](2007-learning-deep-architectures-for-ai)
25. [Multimodal learning with deep Boltzmann machines](2012-multimodal-learning-with-deep-boltzmann-machines)
26. [Stacked Denoising Autoencoders - Learning Useful Representations in a Deep Network with a Local Denoising Criterion](2010-stacked-denoising-autoencoders-learning-useful-representations-in-a-deep-network-with-a-local-denoising-criterion)
27. [A Fast Learning Algorithm for Deep Belief Nets](2006-a-fast-learning-algorithm-for-deep-belief-nets)
28. [Practical Bayesian Optimization of Machine Learning Algorithms](2012-practical-bayesian-optimization-of-machine-learning-algorithms)
29. Practical Recommendations for Gradient-Based Training of Deep Architectures
30. [Greedy Layer-Wise Training of Deep Networks](2006-greedy-layer-wise-training-of-deep-networks)
31. Classification using discriminative restricted Boltzmann machines
32. The Curse of Highly Variable Functions for Local Kernel Machines
33. Modeling pixel means and covariances using factorized third-order boltzmann machines
34. Manifold Parzen Windows
35. A Connection Between Score Matching and Denoising Autoencoders
36. DECISION TREES DO NOT GENERALIZE TO NEW VARIATIONS
37. [Learning Multiple Layers of Features from Tiny Images](2009-learning-multiple-layers-of-features-from-tiny-images)
38. Higher Order Contractive Auto-Encoder
39. Efficient Learning of Deep Boltzmann Machines
40. The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization
41. ICA with Reconstruction Cost for Efficient Overcomplete Feature Learning
42. Boosted Backpropagation Learning for Training Deep Modular Networks
43. Exploring Strategies for Training Deep Neural Networks
44. Spike-and-Slab Sparse Coding for Unsupervised Feature Discovery
45. On Contrastive Divergence Learning
46. Deep Boltzmann Machines
47. Learning to Represent Spatial Transformations with Factored Higher-Order Boltzmann Machines
48. Shift-Invariance Sparse Coding for Audio Classification
49. Inductive Principles for Learning Restricted Boltzmann Machines
50. Transforming Auto-Encoders
51. Differentiable Sparse Coding
52. What regularized auto-encoders learn from the data-generating distribution
53. Structured sparsity through convex optimization
54. Domain Adaptation for Statistical Classifiers
55. Non-Local Manifold Parzen Windows
56. Efficient Learning of Sparse Representations with an Energy-Based Model
57. Implicit Density Estimation by Local Moment Matching to Sample from Auto-Encoders
58. Connectionist Learning of Belief Networks
59. Unsupervised Learning of Sparse Features for Scalable Audio Classification
60. Algorithms for Hyper-Parameter Optimization
61. Learning a Parametric Embedding by Preserving Local Structure
62. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks)
63. Contractive Auto-Encoders - Explicit Invariance During Feature Extraction
64. Learning Sparse Topographic Representations with Products of Student-t Distributions
65. Factored 3-Way Restricted Boltzmann Machines For Modeling Natural Images
66. Learning Convolutional Feature Hierarchies for Visual Recognition
67. Learning Transformational Invariants from Natural Movies
68. The Recurrent Temporal Restricted Boltzmann Machine
69. Sparse deep belief net model for visual area V2
70. Convolutional Learning of Spatio-temporal Features
71. [Training Products of Experts by Minimizing Contrastive Divergence](2002-training-products-of-experts-by-minimizing-contrastive-divergence)
72. [Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis](2011-learning-hierarchical-invariant-spatio-temporal-features-for-action-recognition-with-independent-subspace-analysis)
73. On optimization methods for deep learning
74. Learning Recurrent Neural Networks with Hessian-Free Optimization
75. Domain Adaptation for Large-Scale Sentiment Classification - A Deep Learning Approach
76. Better Mixing via Deep Representations
77. [Reducing the Dimensionality of Data with Neural Networks](2006-reducing-the-dimensionality-of-data-with-neural-networks)
78. Convolutional Deep Belief Networks on CIFAR-10
79. Spike and Slab Variational Inference for Multi-Task and Multiple Kernel Learning
80. Deep learning via semi-supervised embedding
81. [A unified architecture for natural language processing - deep neural networks with multitask learning](2008-a-unified-architecture-for-natural-language-processing-deep-neural-networks-with-multitask-learning)
82. Closed-Form EM for Sparse Coding and Its Application to Source Separation
83. Deep learning via Hessian-free optimization
84. Deep learning from temporal coherence in video
85. Large scale image annotation - learning to rank with joint word-image embeddings
86. Nonlinear Learning using Local Coordinate Coding
87. Natural Gradient Works Efficiently in Learning
88. Improved Local Coordinate Coding using Local Tangents
89. [Acoustic Modeling Using Deep Belief Networks](2012-acoustic-modeling-using-deep-belief-networks)
90. Deep Learning using Robust Interdependent Codes
91. Learning Mixture Models of Spatial Coherence
92. Regularized estimation of image statistics by Score Matching
93. [Laplacian Eigenmaps for Dimensionality Reduction and Data Representation](2003-laplacian-eigenmaps-for-dimensionality-reduction-and-data-representation)
94. Learning long-term dependencies with gradient descent is difficult
95. Learning Fast Approximations of Sparse Coding
96. A general framework for adaptive processing of data structures
97. Learning Deep Boltzmann Machines using Adaptive MCMC
98. Training recurrent neural networks
99. A Closed-Form EM Algorithm for Sparse Coding
100. [Natural Language Processing (Almost) from Scratch](2011-natural-language-processing-almost-from-scratch)
101. Empirical Risk Minimization of Graphical Model Parameters Given Approximate Inference, Decoding, and Model Structure
102. Sample Complexity of Testing the Manifold Hypothesis
103. Ask the locals - Multi-way local pooling for image recognition
104. Learning Horizontal Connections in a Sparse Coding Model of Natural Images
105. Learning image representations from the pixel level via hierarchical sparse coding
106. Restricted Boltzmann machines for collaborative filtering
107. On Autoencoders and Score Matching for Energy Based Models
108. Marginalized Denoising Autoencoders for Domain Adaptation
109. Transforming Autoencoders
110. Learning Continuous Attractors in Recurrent Networks
111. When Does a Mixture of Products Contain a Product of Mixtures?
112. Emergence of Complex-Like Cells in a Temporal Product Network with Local Receptive Fields
113. Learning Many Related Tasks at the Same Time with Backpropagation
114. Multiple Texture Boltzmann Machines
115. Generating more realistic images using gated MRF's
116. A Theoretical Analysis of Feature Pooling in Visual Recognition
117. Inductive Principles for Restricted Boltzmann Machine Learning
118. Generative versus discriminative training of RBMs for classification of fMRI images
119. A Neural Probabilistic Language Model
120. Phone Recognition with the Mean-Covariance Restricted Boltzmann Machine
121. Unsupervised Learning of Image Manifolds by Semidefinite Programming
122. Factored conditional restricted Boltzmann Machines for modeling motion style
123. Tiled convolutional neural networks
124. Unsupervised Learning of Distributions of Binary Vectors Using 2-Layer Networks
125. Noise-contrastive estimation - A new estimation principle for unnormalized statistical models
126. Structured Variable Selection with Sparsity-Inducing Norms
127. Using fast weights to improve persistent contrastive divergence
128. [Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions](2011-semi-supervised-recursive-autoencoders-for-predicting-sentiment-distributions)
129. Learning in Markov Random Fields using Tempered Transitions
130. On Training Deep Boltzmann Machines
131. Sparse coding with an overcomplete basis set - A strategy employed by V1?
132. Justifying and Generalizing Contrastive Divergence
133. A Practical Guide to Training Restricted Boltzmann Machines
134. Slow Feature Analysis - Unsupervised Learning of Invariances
135. Fast Inference in Sparse Coding Algorithms with Applications to Object Recognition
136. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks)
137. Tempered Markov Chain Monte Carlo for training of Restricted Boltzmann Machines
138. From machine learning to machine reasoning
139. Neural Networks - Tricks of the Trade
140. Slow, Decorrelated Features for Pretraining Complex Cell-like Networks
141. [Visualizing Data using t-SNE](2008-visualizing-data-using-t-sne)
142. [Nonlinear dimensionality reduction by locally linear embedding.](2000-nonlinear-dimensionality-reduction-by-locally-linear-embedding)
143. Best practices for convolutional neural networks applied to visual document analysis
144. Emergence of simple-cell receptive field properties by learning a sparse code for natural images
145. Temporal Pooling and Multiscale Learning for Automatic Annotation and Ranking of Music Audio
146. Estimation of Non-Normalized Statistical Models by Score Matching
147. Unsupervised learning of visual invariance with temporal coherence
148. Robust Object Recognition with Cortex-Like Mechanisms
149. On the Expressive Power of Deep Architectures
150. Recursive Distributed Representations
151. On Tracking The Partition Function
152. Quickly Generating Representative Samples from an RBM-Derived Process
153. Autoencoders, Minimum Description Length and Helmholtz Free Energy
154. Some extensions of score matching
155. Unsupervised Models of Images by Spikeand-Slab RBMs
156. [Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary Speech Recognition](2012-context-dependent-pre-trained-deep-neural-networks-for-large-vocabulary-speech-recognition)
157. Stochastic Neighbor Embedding
158. Structured sparse coding via lateral inhibition
159. Deconvolutional networks
160. Learning invariant features through topographic filter maps
161. The “independent components” of natural scenes are edge filters
162. [Deep Neural Networks for Acoustic Modeling in Speech Recognition - The Shared Views of Four Research Groups](2012-deep-neural-networks-for-acoustic-modeling-in-speech-recognition-the-shared-views-of-four-research-groups)
163. [A global geometric framework for nonlinear dimensionality reduction.](2000-a-global-geometric-framework-for-nonlinear-dimensionality-reduction)
164. Self-organizing neural network that discovers surfaces in random-dot stereograms
165. Products of Experts
166. [Multi-column deep neural networks for image classification](2012-multi-column-deep-neural-networks-for-image-classification)
167. What is the best multi-stage architecture for object recognition?
168. On the Convergence Properties of Contrastive Divergence
169. [Deep Neural Networks for Acoustic Modeling in Speech Recognition](2012-deep-neural-networks-for-acoustic-modeling-in-speech-recognition)
170. Semantic hashing
171. Sequential Labeling Using Deep-Structured Conditional Random Fields
172. Parallel tempering is efficient for learning restricted Boltzmann machines
173. Learning Process in an Asymmetric Threshold Network
174. Joint Learning of Words and Meaning Representations for Open-Text Semantic Parsing
175. Optimal Approximation of Signal Priors
176. A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning
177. Natural Image Statistics - A Probabilistic Approach to Early Computational Vision
178. [Rectified Linear Units Improve Restricted Boltzmann Machines](2010-rectified-linear-units-improve-restricted-boltzmann-machines)
179. Topmoumoute Online Natural Gradient Algorithm
180. Unsupervised Discovery of Nonlinear Structure Using Contrastive Backpropagation
181. Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering
182. Training restricted Boltzmann machines using approximations to the likelihood gradient
183. [Gradient-based learning applied to document recognition](1998-gradient-based-learning-applied-to-document-recognition)
184. Slow feature analysis yields a rich repertoire of complex cell properties.
185. Should Penalized Least Squares Regression be Interpreted as Maximum A Posteriori Estimation?
186. Temporal Coherence, Natural Image Sequences, and the Visual Cortex
187. Charting a Manifold
188. Emergence of Phase- and Shift-Invariant Features by Decomposition of Natural Images into Independent Feature Subspaces
189. Hessian Eigenmaps - new locally linear embedding techniques for high-dimensional data
190. Structured Output Layer Neural Network Language Models for Speech Recognition
191. Convolutional Networks Can Learn to Generate Affinity Graphs for Image Segmentation
192. Recognition and Structure from one 2D Model View - Observations on Prototypes, Object Classes and Symmetries
193. Binary coding of speech spectrograms using a deep auto-encoder
194. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection
195. Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation
196. A Spike and Slab Restricted Boltzmann Machine
197. Probabilistic Inference Using Markov Chain Monte Carlo Methods
198. Independent component analysis, A new concept?
199. On the training of recurrent neural networks
200. Backpropagation Applied to Handwritten Zip Code Recognition
201. Probabilistic Principal Component Analysis
202. The Convergence of Contrastive Divergences
203. Modeling Temporal Dependencies in High-Dimensional Sequences - Application to Polyphonic Music Generation and Transcription
204. [Object recognition from local scale-invariant features](1999-object-recognition-from-local-scale-invariant-features)
205. Large-Scale FPGA-based Convolutional Networks
206. Statistical Analysis of Non-Lattice Data
207. Herding Dynamic Weights for Partially Observed Random Field Models
208. Deep Coding Network
209. Hessian eigenmaps - Locally linear embedding techniques for high-dimensional data
210. How are complex cell properties adapted to the statistics of natural stimuli?
211. How Does the Brain Solve Visual Object Recognition?
212. [Beyond Bags of Features - Spatial Pyramid Matching for Recognizing Natural Scene Categories](2006-beyond-bags-of-features-spatial-pyramid-matching-for-recognizing-natural-scene-categories)
213. Hierarchical models of object recognition in cortex
214. Blind separation of sources, part I - An adaptive algorithm based on neuromimetic architecture
215. Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription
216. Efficient Pattern Recognition Using a New Transformation Distance
217. Information processing in dynamical systems - foundations of harmony theory
218. [Deep Sparse Rectifier Neural Networks](2011-deep-sparse-rectifier-neural-networks)
219. Empirical Evaluation and Combination of Advanced Language Modeling Techniques
220. Projection Pursuit Regression
221. Random Search for Hyper-Parameter Optimization
222. Independent Component Analysis
223. [Neocognitron - A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position](2004-neocognitron-a-self-organizing-neural-network-model-for-a-mechanism-of-pattern-recognition-unaffected-by-shift-in-position)
224. Asymptotic Efficiency of Deterministic Estimators for Discrete Energy-Based Models - Ratio Matching and Pseudolikelihood
225. Conversational Speech Transcription Using Context-Dependent Deep Neural Networks
226. Deep, Big, Simple Neural Nets for Handwritten Digit Recognition
227. Neocognitron - A new algorithm for pattern recognition tolerant of deformations and shifts in position
228. Document image defect models
229. Group Invariant Scattering
230. Receptive fields of single neurones in the cat's striate cortex
231. Classification with scattering operators
232. Topographic Independent Component Analysis
233. On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates
234. Neural net language models
235. Almost optimal lower bounds for small depth circuits
236. [Maximum likelihood from incomplete data via the EM - algorithm plus discussions on the paper](1977-maximum-likelihood-from-incomplete-data-via-the-em-algorithm-plus-discussions-on-the-paper)
237. On the power of small-depth threshold circuits
238. A Ph.D. Thesis
239. LIII. On lines and planes of closest fit to systems of points in space
240. EM Algorithms for PCA and Sensible PCA
241. Réseaux de neurones à relaxation entraînés par critère d'autoencodeur débruitant
242. Auto-association by multilayer perceptrons and singular value decomposition
243. [Efficient BackProp](2012-efficient-backprop)
244. Analysis of a complex of statistical variables into principal components.
245. Natural Gradient Revisited
246. Learning processes in an asymmetric threshold network
247. Generalization and network design strategies
248. Learning distributed representations of concepts.
249. Bayesian and L1 Approaches to Sparse Unsupervised Learning
250. A Connectionist Approach to Speech Recognition
251. Evaluating probabilities under high-dimensional latent variable models
252. Natural Image Denoising with Convolutional Networks
253. Nonlinear Component Analysis as a Kernel Eigenvalue Problem
254. Learning the 2-D Topology of Images
255. Self-taught learning - transfer learning from unlabeled data
256. [Curriculum learning](2009-curriculum-learning)
