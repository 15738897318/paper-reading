---
title: Yin and Yang - Balancing and Answering Binary Visual Questions
authors:
- Peng Zhang
- Yash Goyal
- Douglas Summers-Stay
- Dhruv Batra
- Devi Parikh
fieldsOfStudy:
- Computer Science
meta_key: 2016-yin-and-yang-balancing-and-answering-binary-visual-questions
numCitedBy: 244
reading_status: TBD
ref_count: 51
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Yin-and-Yang:-Balancing-and-Answering-Binary-Visual-Zhang-Goyal/5fa973b8d284145bf0ced9acf2913a74674260f6?sort=total-citations
venue: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/Yin-and-Yang:-Balancing-and-Answering-Binary-Visual-Zhang-Goyal/5fa973b8d284145bf0ced9acf2913a74674260f6?sort=total-citations)

# Yin and Yang - Balancing and Answering Binary Visual Questions

## Abstract

The complex compositional structure of language makes problems at the intersection of vision and language challenging. But language also provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content. This can hinder progress in pushing state of art in the computer vision aspects of multi-modal AI. In this paper, we address binary Visual Question Answering (VQA) on abstract scenes. We formulate this problem as visual verification of concepts inquired in the questions. Specifically, we convert the question to a tuple that concisely summarizes the visual concept to be detected in the image. If the concept can be found in the image, the answer to the question is "yes", and otherwise "no". Abstract scenes play two roles (1) They allow us to focus on the highlevel semantics of the VQA task as opposed to the low-level recognition problems, and perhaps more importantly, (2) They provide us the modality to balance the dataset such that language priors are controlled, and the role of vision is essential. In particular, we collect fine-grained pairs of scenes for every question, such that the answer to the question is "yes" for one scene, and "no" for the other for the exact same question. Indeed, language priors alone do not perform better than chance on our balanced dataset. Moreover, our proposed approach matches the performance of a state-of-the-art VQA approach on the unbalanced dataset, and outperforms it on the balanced dataset.

## Paper References

1. Ask Your Neurons - A Neural-Based Approach to Answering Questions about Images
2. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
3. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator)
4. Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question
5. Don't just listen, use your imagination - Leveraging visual common sense for non-visual tasks
6. [From captions to visual concepts and back](2015-from-captions-to-visual-concepts-and-back)
7. Zero-Shot Learning via Visual Abstraction
8. Exploring Models and Data for Image Question Answering
9. VisKE - Visual knowledge extraction and question answering by visual verification of relation phrases
10. [Towards AI-Complete Question Answering - A Set of Prerequisite Toy Tasks](2016-towards-ai-complete-question-answering-a-set-of-prerequisite-toy-tasks)
11. Bringing Semantics into Focus Using Visual Abstraction
12. Learning Common Sense through Visual Abstraction
13. Mind's eye - A recurrent visual representation for image caption generation
14. [Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models](2014-unifying-visual-semantic-embeddings-with-multimodal-neural-language-models)
15. Visual Turing test for computer vision systems
16. MCTest - A Challenge Dataset for the Open-Domain Machine Comprehension of Text
17. [Deep Visual-Semantic Alignments for Generating Image Descriptions](2017-deep-visual-semantic-alignments-for-generating-image-descriptions)
18. Learning the Visual Interpretation of Sentences
19. A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input
20. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
21. Adopting Abstract Images for Semantic Scene Understanding
22. Learning to Interpret and Describe Abstract Scenes
23. Open question answering over curated and extracted knowledge bases
24. [Long-term recurrent convolutional networks for visual recognition and description](2015-long-term-recurrent-convolutional-networks-for-visual-recognition-and-description)
25. Paraphrase-Driven Learning for Open Question Answering
26. Annotator rationales for visual recognition
27. Joint Video and Text Parsing for Understanding Events and Answering Queries
28. Attributes for Classifier Feedback
29. Explain Images with Multimodal Recurrent Neural Networks
30. VizWiz - nearly real-time answers to visual questions
31. [Microsoft COCO - Common Objects in Context](2014-microsoft-coco-common-objects-in-context)
32. Simultaneous Active Learning of Classifiers &amp; Attributes via Relative Feedback
33. [Distributed Representations of Words and Phrases and their Compositionality](2013-distributed-representations-of-words-and-phrases-and-their-compositionality)
34. VizWiz - nearly real-time answers to visual questions
35. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database)
36. Predicting Object Dynamics in Scenes
37. Natural Language Processing with Python
38. Best practices for convolutional neural networks applied to visual document analysis
39. [LSTM Neural Networks for Language Modeling](2012-lstm-neural-networks-for-language-modeling)
40. [Identifying Relations for Open Information Extraction](2011-identifying-relations-for-open-information-extraction)
41. Some Improvements on Deep Convolutional Neural Network Based Image Classification
42. HunPos - an open source trigram tagger
43. Using Annotator Rationales to Improve Machine Learning for Text Categorization
44. [A Fast and Accurate Dependency Parser using Neural Networks](2014-a-fast-and-accurate-dependency-parser-using-neural-networks)
45. Exploring Nearest Neighbor Approaches for Image Captioning
