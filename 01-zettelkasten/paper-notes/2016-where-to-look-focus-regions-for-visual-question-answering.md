---
title: Where to Look - Focus Regions for Visual Question Answering
authors:
- Kevin J. Shih
- Saurabh Singh
- Derek Hoiem
fieldsOfStudy:
- Computer Science
meta_key: 2016-where-to-look-focus-regions-for-visual-question-answering
numCitedBy: 362
reading_status: TBD
ref_count: 31
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Where-to-Look:-Focus-Regions-for-Visual-Question-Shih-Singh/175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22?sort=total-citations
venue: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2016
---

[semanticscholar url](https://www.semanticscholar.org/paper/Where-to-Look:-Focus-Regions-for-Visual-Question-Shih-Singh/175e9bb50cc062c6c1742a5d90c8dfe31d2e4e22?sort=total-citations)

# Where to Look - Focus Regions for Visual Question Answering

## Abstract

We present a method that learns to answer visual questions by selecting image regions relevant to the text-based query. Our method maps textual queries and visual features from various regions into a shared space where they are compared for relevance with an inner product. Our method exhibits significant improvements in answering questions such as "what color," where it is necessary to evaluate a specific location, and "what room," where it selectively identifies informative image regions. Our model is tested on the recently released VQA [1] dataset, which features free-form human-annotated questions and answers.

## Paper References

1. Exploring Models and Data for Image Question Answering
2. Visual Madlibs - Fill in the blank Image Generation and Question Answering
3. Ask Your Neurons - A Neural-Based Approach to Answering Questions about Images
4. Simple Baseline for Visual Question Answering
5. [VQA - Visual Question Answering](2015-vqa-visual-question-answering)
6. [Show, Attend and Tell - Neural Image Caption Generation with Visual Attention](2015-show-attend-and-tell-neural-image-caption-generation-with-visual-attention)
7. [From captions to visual concepts and back](2015-from-captions-to-visual-concepts-and-back)
8. Mind's eye - A recurrent visual representation for image caption generation
9. Improving Image-Sentence Embeddings Using Large Weakly Annotated Photo Collections
10. [Deep Visual-Semantic Alignments for Generating Image Descriptions](2017-deep-visual-semantic-alignments-for-generating-image-descriptions)
11. Predicting Deep Zero-Shot Convolutional Neural Networks Using Textual Descriptions
12. [Show and tell - A neural image caption generator](2015-show-and-tell-a-neural-image-caption-generator)
13. Weakly Supervised Memory Networks
14. Explain Images with Multimodal Recurrent Neural Networks
15. [Deep Captioning with Multimodal Recurrent Neural Networks (m-RNN)](2015-deep-captioning-with-multimodal-recurrent-neural-networks-m-rnn)
16. [Long-term recurrent convolutional networks for visual recognition and description](2015-long-term-recurrent-convolutional-networks-for-visual-recognition-and-description)
17. [Edge Boxes - Locating Object Proposals from Edges](2014-edge-boxes-locating-object-proposals-from-edges)
18. [Efficient Estimation of Word Representations in Vector Space](2013-efficient-estimation-of-word-representations-in-vector-space)
19. [ImageNet Large Scale Visual Recognition Challenge](2015-imagenet-large-scale-visual-recognition-challenge)
20. [Return of the Devil in the Details - Delving Deep into Convolutional Nets](2014-return-of-the-devil-in-the-details-delving-deep-into-convolutional-nets)
21. Generating Typed Dependency Parses from Phrase Structure Parses
22. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift)
23. [Understanding the difficulty of training deep feedforward neural networks](2010-understanding-the-difficulty-of-training-deep-feedforward-neural-networks)
24. MatConvNet - Convolutional Neural Networks for MATLAB
25. [Deep Sparse Rectifier Neural Networks](2011-deep-sparse-rectifier-neural-networks)
