---
title: Designing Network Design Spaces
authors:
- Ilija Radosavovic
- Raj Prateek Kosaraju
- Ross B. Girshick
- Kaiming He
- "Piotr Doll\xE1r"
fieldsOfStudy:
- Computer Science
- Art
meta_key: 2020-designing-network-design-spaces
numCitedBy: 476
reading_status: TBD
ref_count: 41
tags:
- gen-from-ref
- other-default
- paper
urls:
- https://www.semanticscholar.org/paper/Designing-Network-Design-Spaces-Radosavovic-Kosaraju/2709167f1c3a03fa5b970a665ea48ed243aab582?sort=total-citations
venue: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
year: 2020
---

# Designing Network Design Spaces

## Abstract

In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5x faster on GPUs.

## Paper References

1. On Network Design Spaces for Visual Recognition
2. [Efficient Neural Architecture Search via Parameter Sharing](2018-efficient-neural-architecture-search-via-parameter-sharing.md)
3. [Wide Residual Networks](2016-wide-residual-networks.md)
4. EfficientNet - Rethinking Model Scaling for Convolutional Neural Networks
5. [Aggregated Residual Transformations for Deep Neural Networks](2017-aggregated-residual-transformations-for-deep-neural-networks.md)
6. [ShuffleNet V2 - Practical Guidelines for Efficient CNN Architecture Design](2018-shufflenet-v2-practical-guidelines-for-efficient-cnn-architecture-design.md)
7. [FractalNet - Ultra-Deep Neural Networks without Residuals](2017-fractalnet-ultra-deep-neural-networks-without-residuals.md)
8. [MobileNetV2 - Inverted Residuals and Linear Bottlenecks](2018-mobilenetv2-inverted-residuals-and-linear-bottlenecks.md)
9. [Densely Connected Convolutional Networks](2017-densely-connected-convolutional-networks.md)
10. [Neural Architecture Search with Reinforcement Learning](2017-neural-architecture-search-with-reinforcement-learning.md)
11. [Squeeze-and-Excitation Networks](2020-squeeze-and-excitation-networks.md)
12. [Rethinking the Inception Architecture for Computer Vision](2016-rethinking-the-inception-architecture-for-computer-vision.md)
13. [Learning Transferable Architectures for Scalable Image Recognition](2018-learning-transferable-architectures-for-scalable-image-recognition.md)
14. Searching for Activation Functions
15. [Accurate, Large Minibatch SGD - Training ImageNet in 1 Hour](2017-accurate-large-minibatch-sgd-training-imagenet-in-1-hour.md)
16. [Batch Normalization - Accelerating Deep Network Training by Reducing Internal Covariate Shift](2015-batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift.md)
17. [Deep Residual Learning for Image Recognition](2016-deep-residual-learning-for-image-recognition.md)
18. [Very Deep Convolutional Networks for Large-Scale Image Recognition](2015-very-deep-convolutional-networks-for-large-scale-image-recognition.md)
19. [Progressive Neural Architecture Search](2018-progressive-neural-architecture-search.md)
20. [DARTS - Differentiable Architecture Search](2019-darts-differentiable-architecture-search.md)
21. Swish - a Self-Gated Activation Function
22. [ImageNet - A large-scale hierarchical image database](2009-imagenet-a-large-scale-hierarchical-image-database.md)
23. [Regularized Evolution for Image Classifier Architecture Search](2019-regularized-evolution-for-image-classifier-architecture-search.md)
24. AutoAugment - Learning Augmentation Policies from Data
25. [Going deeper with convolutions](2015-going-deeper-with-convolutions.md)
26. [MobileNets - Efficient Convolutional Neural Networks for Mobile Vision Applications](2017-mobilenets-efficient-convolutional-neural-networks-for-mobile-vision-applications.md)
27. [Delving Deep into Rectifiers - Surpassing Human-Level Performance on ImageNet Classification](2015-delving-deep-into-rectifiers-surpassing-human-level-performance-on-imagenet-classification.md)
28. [Improved Regularization of Convolutional Neural Networks with Cutout](2017-improved-regularization-of-convolutional-neural-networks-with-cutout.md)
29. [Xception - Deep Learning with Depthwise Separable Convolutions](2017-xception-deep-learning-with-depthwise-separable-convolutions.md)
30. [ShuffleNet - An Extremely Efficient Convolutional Neural Network for Mobile Devices](2018-shufflenet-an-extremely-efficient-convolutional-neural-network-for-mobile-devices.md)
31. [ImageNet classification with deep convolutional neural networks](2012-imagenet-classification-with-deep-convolutional-neural-networks.md)
32. Do ImageNet Classifiers Generalize to ImageNet?
33. Backpropagation Applied to Handwritten Zip Code Recognition
34. An Introduction to the Bootstrap
35. A and V
